%!TEX program = xelatex
\documentclass[letterpaper]{article}
\usepackage{amsmath,amsfonts,amssymb,amsthm,mathrsfs,bbm}
\usepackage{booktabs,tabularx,multirow,setspace,diagbox}
\usepackage{graphicx}
\usepackage{enumerate}
\usepackage{hyperref}
\usepackage{appendix}
\usepackage[margin=1in]{geometry}
\usepackage[style=authoryear]{biblatex}
\addbibresource{sml-bib.bib}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=blue,
    citecolor=cyan,      
    urlcolor=cyan
}

\linespread{1.2}

\numberwithin{equation}{section}

\theoremstyle{definition} % default
\newtheorem{remark}{Remark}[section]
\newtheorem{example}{Example}[section]
\newtheorem{definition}{Definition}[section]
\newtheorem{proposition}{Proposition}[section]

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}


\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\sign}{\mathrm{sgn}}
\DeclareMathOperator*{\var}{\mathrm{Var}}
\DeclareMathOperator*{\minimize}{\mathrm{minimize}}
\DeclareMathOperator*{\maximize}{\mathrm{maximize}}
\DeclareMathOperator*{\tr}{\rm{tr}}
\DeclareMathOperator*{\ind}{\perp\!\!\!\!\perp}


\title{Statistical Machine Learning - Notes}  
% \author{Zhehao Li}
\date{\today} 


\begin{document}

\maketitle 

\tableofcontents


\vspace{2em}
This note \textbf{aggregates} the contents from the following books, articles and notes:
\begin{itemize}
    \item \cite{bach2021learning}
    \item \cite{mohri2018foundations}
    \item \cite{shalev2014understanding}
    \item \cite{tropp2012user}
    \item \cite{wainwright2019high}
    \item the lecture notes of Patrick Rebeschini
\end{itemize}


\section{Introduction to Supervised Learning} % (fold)
\label{sec:introduction_to_supervised_learning}

    \subsection{Decision Theory} % (fold)
    \label{sub:decision_theory}

        \textbf{Main Concern}. What is the optimal perforamce, regardless of the finiteness of the training data? In other words, if we have a perfect knowledge of the underlying probability distribution of the data, what should be done?

        \vspace{1em}
        We consider a fixed (testing) distribution $P_{X,Y}(x, y)$ on $\mathcal{X} \times \mathcal{Y}$, with marginal distribution $P_X(x)$ on $\mathcal{X}$. At this point we make no assumptions on the input space $\mathcal{X}$.

        \subsubsection{Loss Functions} % (fold)
        \label{ssub:loss_functions}

            We consdier a loss function $l: \mathcal{Y} \times \mathcal{Y} \rightarrow \mathbb{R}$, where $l(y, \hat{y})$ is the loss of predicting $\hat{y}$ while the true label is $y$. Hera are some examples:
            \begin{itemize}
                \item \textbf{Binary classification}. $\mathcal{Y} = \{0, 1\}$ or $\mathcal{Y} = \{-1, 1\}$, and we consider the $0$-$1$ loss
                    $$ l(y, \hat{y}) = \mathbbm{1}(y \neq \hat{y}) $$

                \item \textbf{Multi-category classification}. $\mathcal{Y} = \{1, \dots, k\}$ and still the $0$-$1$ loss

                \item \textbf{Regression}. $\mathcal{Y} = \mathbb{R}$ and consider the square loss
                    $$ l(y, \hat{y}) = (y - \hat{y})^2 $$
                    or the abosulte loss
                    $$ l(y, \hat{y}) = |y - \hat{y}| $$
                    which is often used for "robust" estimamtion since the penaly for lager errors is smaller.
            \end{itemize}
        
        % subsubsection loss_functions (end)

        \subsubsection{Risks} % (fold)
        \label{ssub:risks}

            What should be the performance criterion for supervised learning? The answer is, given the loss function $l: \mathcal{Y} \times \mathcal{Y} \rightarrow \mathbb{R}$, we can define the expected risk or testing error of a function $f: \mathcal{X} \rightarrow \mathcal{Y}$ as the expectation of the loss function between the output $y$ and the prediction $f(x)$.

            \begin{definition}[Expected Risk]
                Given a loss function $l: \mathcal{Y} \times \mathcal{Y} \rightarrow \mathbb{R}$, and a distribution $P(x, y)$, the expected risk of a prediction function $f: \mathcal{X} \rightarrow \mathcal{Y}$ is defined as:
                \begin{equation}
                    \mathcal{R}(f) = \mathbb{E}_{X, Y}\big[l(Y, f(X)) \big] = \int_{\mathcal{X} \times \mathcal{Y}} l(y, f(x)) d P(x, y) 
                \end{equation}
                Note that the risk depends on the joint distribution $P(x, y)$.
            \end{definition}

            \begin{definition}[Empirical Risk]
                Given a loss function $l: \mathcal{Y} \times \mathcal{Y} \rightarrow \mathbb{R}$, and data $(x_i, y_i) \in \mathcal{X} \times \mathcal{Y}, i = 1, \dots, n$, the empirical risk (training error) of a prediction function $f: \mathcal{X} \rightarrow \mathcal{Y}$ is defined as:
                \begin{equation}
                    \hat{\mathcal{R}}(f) = \frac{1}{n} \sum_{i=1}^{n} l(y_i, f(x_i))
                \end{equation}
                where here $1/n$ is the empirical distribution function $\hat{P}(x, y)$.
            \end{definition}

            \begin{itemize}
                \item \textbf{Binary classification}. $\mathcal{Y} = \{0, 1\}$ and $0$-$1$ loss $l(y, \hat{y}) = \mathbbm{1}(y \neq \hat{y}) $, we can express the risk as
                    $$ \mathcal{R}(f) = \mathbb{E}_{X, Y} \big[ \mathbbm{1}(Y \neq f(X) ) \big] = \mathbb{P}(Y \neq f(X)) $$
                    which is simply the probability of making a mistake on the testing data; while the empricial risk is the proportion of mistake on tranining data
                    $$ \hat{\mathcal{R}}(f) = \frac{1}{n} \sum_{i=1}^{n} \mathbbm{1}(y_i \neq f(x_i)) $$

                \item \textbf{Multi-category classification}. $\mathcal{Y} = \{1, \dots, k\}$ and $0$-$1$ loss, we can express the risk $\mathcal{R}(f)$ and empirical risk $\hat{\mathcal{R}}(f)$ in a similiar way.

                \item \textbf{Regression}. $\mathcal{Y} = \mathbb{R}$ and the square loss $l(y, \hat{y}) = (y - \hat{y})^2$, the risk is then
                    $$ \mathcal{R}(f) = \mathbb{E}_{X, Y}(Y - f(X))^2  $$
                    and the empirical risk
                    $$ \hat{\mathcal{R}}(f) = \frac{1}{n} \sum_{i=1}^{n} (y_i - f(x_i))^2 $$
                    Sometimes we use absolute loss $l(y, \hat{y}) = |y - \hat{y}|$ for robust optimization (since the penalty for large errors is smaller).
            \end{itemize}

        % subsubsection risks (end)

        \subsubsection{Bayes Risk} % (fold)
        \label{ssub:bayes_risk}

            After the definition of performance criterion for supervised learning, what is the best prediction function $f$ regardless of the data? The answer is, the Bayes risk and Bayes predictor.

            Using the the law of iterated expecation, we have
            $$ \mathcal{R}(f) = \mathbb{E}\big[l(Y, f(X)) \big] = \mathbb{E}\Big[\mathbb{E}\big[l(Y, f(X)) \mid X \big] \Big] = \int_{\mathcal{X}} \mathbb{E}\big[l(Y, f(X)) \mid X = x \big] d P_X(x) $$
            where 
            $$ \mathcal{R}(f; x) = \mathbb{E}[l(Y, f(X)) \mid X = x] $$
            can be viewed as conditional risk, which is a deterministic function.

            \begin{proposition}[Bayes Predictor and Bayes Risk]
                The expected risk is minimized at a Bayes predictor $f^*: \mathcal{X} \rightarrow \mathcal{Y}$ satisfying for all $x \in \mathcal{X}$,
                \begin{equation}
                    f^*(x) \in \argmin_{f(X) \in \mathcal{Y}} \mathbb{E}\big[ l(Y, f(X)) \mid X = x \big]
                \end{equation}
                The Bayes risk $\mathcal{R}^*$ is the risk of all Bayes predictors and is equal to
                \begin{equation}
                    \mathcal{R}^* = \mathbb{E}_X \Big[ \inf_{f(X) \in \mathcal{Y}} \mathbb{E}\big[l(Y, f(X)) \mid X \big] \Big]
                \end{equation}
            \end{proposition} 

            \begin{remark}
                At every point $x \in \mathcal{X}$ we have a minimizer $z^* \in \mathcal{Y}$. We unite all $(x, z^*)$ pair, we can construct a maps from $\mathcal{X}$ to $\mathcal{Z}^*$, where $\mathcal{Z}$ is the set of all minimizer $z^*$ at each point $x$. We denotes such map as $f^*: \mathcal{X} \rightarrow \mathcal{Z}^*$.
                The Bayes risk is then the probability weighted average ($P(x)$) of the conditional loss at each point $x$.
            \end{remark}

            \begin{proof}
                By definition, we have
                \begin{equation*}
                    \begin{aligned}
                        \mathcal{R}^* = \mathcal{R}(f^*) &= \mathbb{E}\big[l(Y, f^*(X)) \big] \\
                        &= \mathbb{E}\Big[ \mathbb{E}\big[l(Y, f^*(X)) \mid X \big] \Big] \\
                        &= \mathbb{E}\Big[ \inf_{f(X) \in \mathcal{Y}} \mathbb{E}\big[l(Y, f(X)) \mid X \big] \Big] \\         
                    \end{aligned}               
                \end{equation*}
            \end{proof}       
            Note that the Bayes predictor is not always unique, but that all lead to the same Bayes risk (e.g. in binary classification when $\mathbb{P}(Y = 1 \mid X) = 1/2$), and the Bayes risk is usually nonzero, unless the dependence between $X$ and $Y$ is deterministic.
            Given a supervised learning problem, the Bayes risk is the optimal performance; we define the excess risk as the deviation with respect to the optimal risk.

            \begin{proposition}[Excess Risk]
                The excess risk of a function $f: \mathcal{X} \rightarrow \mathcal{Y}$ is equal to $\mathcal{R}(f) - \mathcal{R}^*$, and it is always non-negative.
            \end{proposition}

            \begin{proof}
                We prove the propsition under the setting of binary classification with loss $l(y, \hat{y}) = \mathbbm{1}(y \neq \hat{y})$. For any fixed $X = x, x \in \mathcal{X}$, the conditional risk is
                \begin{equation*}
                    \begin{aligned}
                        \mathcal{R}(f; x) &= \mathbb{E}[\mathbbm{1}(Y \neq f(X)) \mid X = x ] \\
                        &= \mathbb{P}(Y \neq f(X) \mid X = x)\\                       
                        &= 1 - \mathbb{P}(Y = 1, f(X) = 1 \mid X = x) - \mathbb{P}(Y = -1, f(X) = -1 \mid X = x )  \\
                        &= 1 - \mathbb{P}(Y = 1 \mid X = x) \mathbb{P}(f(X) = 1 \mid X = x) - \mathbb{P}(Y = -1 \mid X = x) \mathbb{P}(f(X) = -1 \mid X = x) \\
                        &= 1 - \eta(x) \mathbb{E}\big[\mathbbm{1}(f(x) = 1) \big] - (1 - \eta(x)) \mathbb{E}\big[\mathbbm{1}(f(x) = -1) \big] \\
                    \end{aligned}
                \end{equation*}
                Hence, for $X = x, x \in \mathcal{X}$, the difference of excess conditional risk is
                \begin{equation*}
                    \begin{aligned}
                        \mathcal{R}(f; x) - \mathcal{R}(f^*; x) &= \mathbb{E}\big[\mathbbm{1}(Y \neq f(X)) \mid X = x \big] - \mathbb{E}\big[\mathbbm{1}(Y \neq f^*(X)) \mid X = x \big] \\
                        &= \mathbb{P}(Y \neq f(X) \mid X = x) - \mathbb{P}(Y \neq f^*(X) \mid X = x ) \\
                        &= \eta(x) \Big(\mathbb{E}\big[\mathbbm{1}(f^*(x) = 1) \big] - \mathbb{E}\big[\mathbbm{1}(f(x) = 1) \big] \Big) + (1 - \eta(x)) \Big(\mathbb{E}\big[\mathbbm{1}(f^*(x) = 0) \big] - \mathbb{E}\big[\mathbbm{1}(f(x) = 0) \big] \Big) \\
                        &= (2 \eta(x) - 1) \Big(\mathbb{E}\big[\mathbbm{1}(f^*(x) = 1)\big] - \mathbb{E}\big[\mathbbm{1}(f(x) = 1)\big] \Big) \\
                        &\geq 0 \\
                    \end{aligned}
                \end{equation*}
                The last inequality is due to the fact that
                \begin{itemize}
                    \item if $\eta(x) \geq 1/2$, $2 \eta(x) - 1 \geq 0$ and $\mathbb{E}[\mathbbm{1}(f^*(x) = 1)] = 1 \geq \mathbb{E}[\mathbbm{1}(f(x) = 1)] $; the product of two nonnegatives are nonnegative

                    \item if $\eta(x) \leq 1/2$, $2\eta(x) - 1 \leq 0$ and $\mathbb{E}[\mathbbm{1}(f^*(x) = 1)] = 0 \leq \mathbb{E}[\mathbbm{1}(f(x) = 1)] $; the product of two negatives are nonnegative
                \end{itemize}
                Therefore, we have shown that $\mathcal{R}(f) \geq \mathcal{R}(f^*)$ for any function $f$.
            \end{proof}

            Therefore, machine learning is "trivial": given the distribution $Y\mid X$ for any $X = x$, the optimal predictor is known. The difficulty will be that this distribution is unknown.

            \begin{itemize}
                \item \textbf{Binary classification}. the Bayes predictor for $\mathcal{Y} = \{-1, 1\}$ and $l(y, \hat{y}) = \mathbbm{1}(y \neq \hat{y})$ is 
                    \begin{equation}
                        f^{*} = \sign(2\eta(x) - 1) = \left\{
                        \begin{aligned}
                            & +1, \quad &\text{if} \ \eta(x) \geq 1/2 \\
                            & -1, \quad &\text{otherwises} \\
                        \end{aligned}
                        \right.
                    \end{equation}
                    where $\eta(x) = \mathbb{P}(Y = 1 \mid X = x)$. Note that $\eta(x) \geq 1/2$ is equivalent to 
                    $$ \frac{\mathbb{P}(Y = +1 \mid X = x) }{\mathbb{P}(Y = -1 \mid X = x) } \geq 1 $$
                    The corresponding Bayes risk is
                    $$ \mathcal{R}^* = \mathbb{P}(Y \neq f^*(X)) = \mathbb{E}\big[\min\{ \eta(x), 1 - \eta(x) \} \big] $$

                    \begin{proof}
                        The conditional risk is
                        \begin{equation*}
                            \begin{aligned}
                                \mathcal{R}(f; x) &= \mathbb{E}\big[\mathbbm{1}(Y \neq f(X)) \mid X = x \big] \\
                                &= \mathbb{P}(Y \neq f(X) \mid X = x) \\
                                &= \mathbb{P}(Y = 1, f(X) = -1 \mid X = x) + \mathbb{P}(Y = -1, f(X) = 1 \mid X = x) \\
                                &= \mathbb{P}(Y = 1 \mid X = x) \mathbb{P}(f(x) = -1) + \mathbb{P}(Y = -1 \mid X = x) \mathbb{P}(f(x) = 1) \\
                                &= \eta(x) \mathbb{P}(f(x) = -1) + (1 - \eta(x)) \mathbb{P}(f(x) = 1) \\
                            \end{aligned}
                        \end{equation*}
                        To minimize $\mathcal{R}(f; x)$, we wish the function $f: \mathcal{X} \rightarrow \{-1, 1 \}$ satisfying
                        \begin{equation*}
                            \mathcal{R}(f; x) = \left\{
                            \begin{aligned}
                                & \eta(x) \quad &\text{if} \ \eta(x) \leq 1/2 \\
                                & 1 - \eta(x) \quad &\text{if} \ \eta(x) > 1/2 \\
                            \end{aligned}
                            \right.
                        \end{equation*}
                        Therefore, we can simply let
                        \begin{equation*}
                            f^*(x) = \argmin_{f(x) \in \{-1, 1\}} \mathcal{R}(f; x) = \sign(2\eta(x) - 1) = \left\{
                            \begin{aligned}
                                & -1 \quad &\text{if} \ \eta(x) \leq 1/2 \\
                                & +1 \quad &\text{if} \ \eta(x) > 1/2 \\
                            \end{aligned}
                            \right.
                        \end{equation*}
                        as the Bayes predictor. The Bayes risk is therefore
                        \begin{equation*}
                            \begin{aligned}
                                \mathcal{R}^* &= \mathbb{E}_X \big[\mathcal{R}(f^*; X) \big] \\
                                &= \mathbb{E}_X \Big[ \eta(X) \mathbb{P}\big(\eta(X) \leq 1/2 \big) + (1 - \eta(X)) \mathbb{P}\big(\eta(X) > 1/2 \big) \Big] \\
                                &= \mathbb{E}_X\Big[\min\{\eta(X), 1 - \eta(X) \} \Big] \\
                            \end{aligned}
                        \end{equation*}

                    \end{proof}

                \item \textbf{Regression}. the Bayes predictor for $\mathcal{Y} = \mathbb{R}$ and $l(y, \hat{y}) = (y - \hat{y})^2$ is such that
                    \begin{equation}
                        f^*(x) = \mathbb{E}[Y \mid X = x]
                    \end{equation}
                    The corresponding Bayes risk is
                    $$ \mathcal{R}^{*} = \mathcal{R}(f^*) = \mathbb{E}\Big[\big(Y - \mathbb{E}[Y \mid X] \big)^2 \Big] $$

                    \begin{proof}
                        The conditional risk of given function $f: \mathcal{X} \rightarrow \mathbb{R}$ is
                        \begin{equation*}
                            \begin{aligned}
                                \mathcal{R}(f; x) &= \mathbb{E}[(Y - f(X))^2 \mid X = x] \\ 
                                &= \mathbb{E}\Big[\big(Y - \mathbb{E}[Y \mid X = x] + \mathbb{E}[Y \mid X = x] - f(X) \big)^2 \mid X = x \Big] \\
                                &= \mathbb{E}\Big[\big(Y - \mathbb{E}[Y \mid X = x] \big)^2 \mid X = x \Big] + \mathbb{E}\Big[\big(f(X) - \mathbb{E}[Y \mid X = x] \big)^2 \mid X = x \Big] \\ 
                                &\quad + 2 \mathbb{E}\Big[\big(Y - \mathbb{E}[Y \mid X = x] \big) \big(\mathbb{E}[Y \mid X = x] - f(X) \big) \mid X = x \Big] \\
                            \end{aligned}
                        \end{equation*}
                        Notice that
                        \begin{equation*}
                            \begin{aligned}
                                &\mathbb{E}\Big[\big(Y - \mathbb{E}[Y \mid X = x] \big) \big(\mathbb{E}[Y \mid X = x] - f(X) \big) \mid X = x \Big] \\
                                = &\big(\mathbb{E}[Y \mid X = x] - f(x) \big) \mathbb{E}\Big[Y - \mathbb{E}[Y \mid X = x] \mid X = x \Big] \\
                                = &\big(\mathbb{E}[Y \mid X = x] - f(x) \big) \big(\mathbb{E}[Y \mid X = x] - \mathbb{E}[Y \mid X = x] \big) \\
                                = &0 \\
                            \end{aligned}
                        \end{equation*}
                        We have
                        \begin{equation*}
                            \begin{aligned}
                                f^{*}(x) &= \argmin_{f(X) \in \mathbb{R}} \mathbb{E}[(Y - f(X))^2 \mid X = x] \\
                                &= \argmin_{f(X) \in \mathbb{R}} \mathbb{E}\Big[\big(Y - \mathbb{E}[Y \mid X = x] \big)^2 \mid X = x \Big] + \mathbb{E}\Big[\big(f(X) - \mathbb{E}[Y \mid X = x] \big)^2 \mid X = x \Big] \\
                                &= \mathbb{E}[Y \mid X = x]\\
                            \end{aligned} 
                        \end{equation*}
                        and the Bayes risk is
                        $$ \mathcal{R}^* = \mathbb{E}_X[\mathcal{R}(f^*; X) ] = \mathbb{E}\big[(Y - f^*(X))^2 \big] = \mathbb{E}\Big[\big(Y - \mathbb{E}[Y \mid X] \big)^2 \Big] $$
                    \end{proof}

            \end{itemize}

        % subsubsection bayes_risk (end)

    % subsection decision_theory (end)

    \subsection{Learning from Data} % (fold)
    \label{sub:learning_from_data}

        The decision theory framework outlined in the previous section gives a test performance criterion and optimal predictors, but it depends on the full knowledge of the \textit{test distribution} $P(x, y)$. We now briefly review how we can obtain good prediction functions from \textit{training data}, that is data sampled i.i.d. from the same distribution $P(x, y)$.
        There are two main classes prediction algorihms:
        \begin{itemize}
            \item Local Averaging Methods
                \begin{itemize}
                    \item Nearest-neighbors
                \end{itemize}

            \item Empirical Risk Minimization
                \begin{itemize}
                    \item Linear least-squares regression
                    \item Kernel methods
                    \item Sparse methods
                    \item Neural networks
                \end{itemize}
        \end{itemize}

        \subsubsection{Local Averaging Methods} % (fold)
        \label{ssub:local_averaging_methods}

            Given $n$ observations $(x_1, y_1), \dots, (x_n, y_n)$ where $\mathcal{X}$ is a metric space and $\mathcal{Y} \in \{0, 1\}$, a new point $x^{\text{test}}$ is classified by a majority vote among the $k$-nearest neighbor of $x^{\text{test}}$.
            \begin{figure}[h]
                \centering
                \begin{minipage}{0.55\textwidth}
                    \centering
                    \includegraphics[width=\textwidth]{Pics/knn.png}
                \end{minipage}
                \begin{minipage}{0.40\textwidth}
                    \centering
                    \includegraphics[width=\textwidth]{Pics/knn_curve.png}
                \end{minipage}
                \caption{$k$-nearest-neighbor}
            \end{figure}

            \begin{itemize}
                \item Pros:
                    \begin{enumerate}[(a)]
                        \item no optimization or training
                        \item often easy to implement
                        \item can get very good performance in low dimensions (in particular for non-linear dependences between $x$ and $y$)
                    \end{enumerate}

                \item Cons:
                    \begin{enumerate}[(a)]
                        \item slow at query time: must pass through all training data at each testing point (there are ways to reduce complexity)
                        \item bad for high-dimensional data (curse of dimensionality)
                        \item the choice of local \textit{distance function} is crucial
                        \item the choice of "width" parameter or $k$ has to be performed
                    \end{enumerate}
            \end{itemize}

        % subsubsection local_averaging_methods (end)

        \subsubsection{Empirical Risk Minimiation} % (fold)
        \label{ssub:empirical_risk_minimiation}
            
            Consider a parameterized family of prediction functions $f_{\theta}: \mathcal{X} \rightarrow \mathcal{Y}$ for $\theta \in \Theta$ and minimize the empirical risk with respect to $\theta \in \Theta$:
            $$ \hat{\mathcal{R}}(f_\theta) = \frac{1}{n} \sum_{i=1}^{n} l(y_i, f_\theta(x_i) ) $$
            this defines the estimator
            $$ \hat{\theta} \in \argmin_{\theta \in \Theta} \hat{\mathcal{R}}(f_{\theta}) $$
            and a function $f_{\hat{\theta} }: \mathcal{X} \rightarrow \mathcal{Y}$.
            
            \begin{itemize}
                \item Pros:
                    \begin{enumerate}[(a)]
                        \item can be relatively easy to optimize (simple derivation and numerical algebra), many algorithms avaliable (mostly based on gradient descent)
                        \item can be applied in any dimension (if a reasonable feature vector is available)
                    \end{enumerate}

                \item Cons:
                    \begin{enumerate}[(a)]
                        \item can be relatively hard to optimize (e.g. neural networks)
                        \item need a good feature vector for linear methods
                        \item dependence on parameters can be complex (e.g. neural networks)
                        \item need some capacity control to avoid overfitting
                        \item how to parameterize functions with values in $\{0, 1\}$ (convex surrogates)
                    \end{enumerate}

                \item Example: linear least-squares regression
                    $$ \frac{1}{n} \sum_{i=1}^{n} (y_i - \theta^{\top} \phi(x_i) )^2 $$
                    here $f_\theta = \theta^{\top} \phi(x_i) $ is linear in some feature vector $\phi(x) \in \mathbb{R}^d$ (no need for $\mathcal{X}$ to be a vector space). The vector $\phi(x)$ can be quite large.
            \end{itemize}

            \begin{figure}[h]
                \centering
                \includegraphics[width=.60\textwidth]{Pics/erm.png}
                \caption{empirical risk}
            \end{figure}

            \begin{itemize}
                \item \textbf{Risk decompmosition}

                    Given any $\hat{\theta} \in \Theta$, we can weite the excess risk of $f_{\hat{\theta}}$ as:
                    \begin{equation*}
                        \begin{aligned}
                            \mathcal{R}(f_{\hat{\theta}}) - \mathcal{R}^* &= \left\{\mathcal{R}(f_{\hat{\theta}} ) - \inf_{\theta \in \Theta} \mathcal{R}(f_{\theta} ) \right\} + \left\{\inf_{\theta \in \Theta} \mathcal{R}(f_{\theta} ) - \mathcal{R}^* \right\} \\
                            &= \text{estimation error} \quad + \quad \text{approximation error} \\
                        \end{aligned}
                    \end{equation*}

                    \begin{itemize}
                        \item The \textit{estimation error} is typically random, becasue the function $f_{\hat{\theta}}$ is random (depend on random training data). It is typically decreasing in $n$ (more data, less uncertainty), and usually goes up when $\Theta$ grows.

                        \item The \textit{approximation error} does not depend on the chosen of $f_{\hat{\theta}}$, and is also independent of traning size $n$.
                        It depends only on the class of functions parameterized by $\theta \in \Theta$, and hence it is always a determinsitic function. When $\Theta$ grows, the approximation error goes down, and to zero if arbitrary functions can be approximated arbitrary well by the function $f_{\theta}$. 
                    \end{itemize}
                    Typically, for any $\hat{\theta} \in \Theta$, the \textit{estimation error} is often decomposed as
                    \begin{equation*}
                        \begin{aligned}
                            \Big\{\mathcal{R}(f_{\hat{\theta}} ) - \mathcal{R}(f_{\theta^*} ) \Big\} &= \Big\{ \mathcal{R}(f_{\hat{\theta}}) - \hat{\mathcal{R}}(f_{\hat{\theta}} ) \Big\} + \Big\{ \hat{\mathcal{R}}(f_{\hat{\theta}} ) - \hat{\mathcal{R}}(f_{\theta^*} ) \Big\} + \Big\{\hat{\mathcal{R}}(f_{\theta^*} ) - \mathcal{R}(f_{\theta^*} ) \Big\} \\
                            &\leq 2 \sup_{\theta \in \Theta} \Big|\hat{\mathcal{R}}(f_\theta) - \mathcal{R}(f_{\theta}) \Big| \quad + \quad \text{empirical optimizati error}
                        \end{aligned}
                    \end{equation*}
                    where $\theta^*$ is a minimizer on $\Theta$. The uniform deviation grows with the “size” of $\Theta$, and usually decays with $n$.

                \item \textbf{Capacity control}

                    In order to avoid overfitting, we need to make sure that the set of allowed functions is not too large, by typically reducing the number of parameters, or by restricting the norm of predictors (thus by reducing the "size" of $\Theta$): this typically leads to constrained optimization, and allows for risk decompositions as done above.

                    This can be done by regularization, that is, minimizing the follow:
                    $$ \hat{\mathcal{R}}(f_\theta) + \lambda \Omega(\theta) = \frac{1}{n} \sum_{i=1}^{n} l(y_i, f_{\theta}(x_i) ) + \lambda \Omega(\theta) $$
                    where $\Omega(\theta)$ controls the complexity of $f_{\theta}$. The main example is ridge regression:
                    $$ \min_{\theta \in \mathbb{R}^d} \quad \frac{1}{n} \sum_{i=1}^{n} \big(y_i - \theta^{\top} \phi(x_i) \big)^2 + \lambda ||\theta||_2^2 $$
                    this is often easier to optimization, but harder to analyze.

                    \textbf{Note}: There is a difference between parameters (e.g., $\theta$) learned on the training data and hyperparameters (e.g., $\lambda$) learned on the validation data

            \end{itemize}

        % subsubsection empirical_risk_minimiation (end)

    % subsection learning_from_data (end)

    \subsection{Statistical Learning Theory} % (fold)
    \label{sub:statistical_learning_theory}

        The goal of learning theory is to provide some guarantess of performance on unseen data. A common assumption is that the samples $S_n(P) = \{(X_1, Y_1), \dots, (X_n, Y_n) \}$ are obtained as i.i.d. observations from some unknown distribution $P$ from a family $\mathcal{P}$.

        An algorithm $\mathcal{A}$ is a mapping from $S_n(P)$ (could be any size $n$) to a function from $f: \mathcal{X} \mapsto \mathcal{Y}$. The risk depends on the probability $P \in \mathcal{P}$, as $\mathcal{R}_P(f)$. The goal is to find $\mathcal{A}$ such that the risk
        \begin{equation*}
            \mathcal{R}_{P}\big(\mathcal{A}(S_n(P)) \big) - \mathcal{R}_P^* 
        \end{equation*}
        is small enough, where $\mathcal{R}_{P}^{*}$ is the Bayes risk with respect to joint distribution $P(X, Y)$. Here we assume that $S_n(P)$ is sampled from $P$, where $P \in \mathcal{P}$ is unknown. Moreover, the risk is random because $S_n$ is random.
    
        \subsubsection{Measures of Performance} % (fold)
        \label{ssub:measures_of_performance}

            There are several ways of dealing with the randomness of the risk in order to obtain a criterion.

            \begin{itemize}
                \item \textbf{Expected Error}: we measure the performance of algorithm as                  
                    \begin{equation*}
                        \mathbb{E}\Big[\mathcal{R}_{P}\big(\mathcal{A}(S_n(P) ) \big) \Big]
                    \end{equation*}
                    where the expectation is with respect to the training data. An algorithm $\mathcal{A}$ is called \textit{consistent in expectation} for the distribution $P$, if 
                        $$ \mathbb{E}\Big[\mathcal{R}_{P}\big(\mathcal{A}(S_n(P)) \big) \Big] - \mathcal{R}_{P}^{*} \rightarrow 0 $$
                    when $n$ tends to infinity.
                    
                \item \textbf{Probably Approximately Correct (PAC) Learning}: for a given $\delta \in (0, 1)$ and $\varepsilon > 0$
                    \begin{equation*}
                        \mathbb{P}\Big( \mathcal{R}_P\big(\mathcal{A}(S_n(P) ) \big) - \mathcal{R}_{P}^* \leq \varepsilon \Big) \geq 1 - \delta
                    \end{equation*}
                    The crux is to find $\varepsilon$ which is as small as possible (typically as a function of $\delta$). An algorithm $\mathcal{A}$ is called consistent in PAC for the distribution $P$, if for any $\varepsilon > 0$, there exists $\delta_n \in (0, 1)$, such that
                        $$ \mathbb{P}\Big( \mathcal{R}_P\big(\mathcal{A}(S_n(P) ) \big) - \mathcal{R}_{P}^* \leq \varepsilon \Big) \geq 1 - \delta_n $$
                    and the sequence $\delta_n$ goes to zero as $n \rightarrow \infty$.

            \end{itemize}
        
        % subsubsection measures_of_performance (end)

        \subsubsection{Some Notions in Learning Problems} % (fold)
        \label{ssub:some_notions_in_learning_problems}

            \begin{definition}[Uniform Consistency]
                An algorithm is called universally consistent (in expectation) if for all distributions $P$ on $(X, Y)$, the algorithm $\mathcal{A}$ is consistent in expectation with respect to distribution $P$.
            \end{definition}

            Most often, we want to study uniform consistency within a class $\mathcal{P}$ of distributions satisfying some regularities. We thus aim at finding an algorithm $\mathcal{A}$ such that
            \begin{equation*}
                \sup_{P \in \mathcal{P}} \ \mathbb{E}\Big\{ \mathcal{R}_P \big(\mathcal{A}(S_n(P) ) \big) - \mathcal{R}_{P}^{*} \Big\}
            \end{equation*}
            is as small as possible. We will see the details in Section \ref{sec:empirical_risk_minimization} and \ref{sec:pac_learn_and_uniform_convergence}.

            \begin{definition}[Minimax Risk]
                The minimax risk is defined as
                \begin{equation}
                    \inf_{\mathcal{A}} \sup_{P \in \mathcal{P}} \ \mathbb{E}\Big\{ \mathcal{R}_P \big(\mathcal{A}(S_n(P) ) \big) - \mathcal{R}_{P}^{*} \Big\}
                \end{equation}
            \end{definition}
            The minimax risk is typically a function of the sample size $n$, the properties of $\mathcal{X}, \mathcal{Y}$ and the distribution space $\mathcal{P}$. In order to compute the estimates of minimax risk, several techniques exist:
            \begin{itemize}
                \item Upper-bounding: one given algorithm with a convergence proof provides an upper-bound on the optimal performance

                \item Lower-bounding: in some setups, it is possible to show that the infimum over all algorithms is greater than a certian quantity.
            \end{itemize}
            The ML researcher are happy when upper-bounds and lower-bounds match (up to constant factors).

            \begin{itemize}
                \item \textbf{Non-asymptotic Analysis}

                    The analysis can be “non-asymptotic”, with an upper-bound with explicit dependence on all quantities; the bound is then valid for all $n$, even if sometimes vacuous.


                \item \textbf{Asymptotic Analysis}

                    The analysis can also be “asymptotic”, where for examples $n$ goes to infinity and limits are taken (alternatively, several quantities can be made to grow simultaneously).

            \end{itemize}
            What (arguably) matters most here is the dependence of these \textbf{rates} on the problem, not the choice of “in expectation” vs. in “high probability”, or “asymptotic” vs. “non-asymptotic”, as long as the problem parameters explicitly appear.

        % subsubsection some_notions_in_learning_problems (end)

        \subsubsection{No Free Lunch Theorems} % (fold)
        \label{ssub:no_free_lunch_theorems}

            Although it may be tempting to define the optimal learning algorithm that works optimally for all distributions, this is impossible. \textbf{In other words, learning is not possible without assumptions}.

            The following theorems shows that for any algorithm, for a fixed $n$, there is a data distribution $P$ that makes the algorithm useless

            \begin{theorem}[No Free Lunch - Fixed n]
                Consider the binary classification with $0$-$1$ loss, with $\mathcal{X}$ infinite. Let $\mathcal{P}$ denote the set of all probability distributions on $\mathcal{X} \times \{0, 1\}$. For any $n > 0$ and learning algorithm $\mathcal{A}$,
                \begin{equation*}
                    \sup_{P \in \mathcal{P}} \ \left\{ \mathbb{E}_{S_n}\Big[\mathcal{R}_{P}\big(\mathcal{A}(S_n(P) ) \big) \Big] - \mathcal{R}_{P}^* \right\} \geq 1/2
                \end{equation*}
            \end{theorem}
            The proof see \cite{bach2021learning}. The folliwng theroem is much stronger, as it more convincingly shows that learning can be arbitrarily slow without assumption.

            \begin{theorem}[No Free Lunch - Sequence of Errors]
                Consider the binary classification with $0$-$1$ loss, with $\mathcal{X}$ infinite. Let $\mathcal{P}$ denote the set of all probability distributions on $\mathcal{X} \times \{0, 1\}$. For any decreasing sequence $a_n$ tending to zero and such that $a_1 \leq 1/16$, for any learning algorithm $\mathcal{A}$, there exists $P \in \mathcal{P}$, such that for all $n \geq 1$,
                \begin{equation*}
                     \mathbb{E}_{S_n}\Big[\mathcal{R}_{P} \big(\mathcal{A}(S_n(P) ) \big) \Big] - \mathcal{R}_{P}^* \geq a_n
                \end{equation*}
            \end{theorem}
        
        % subsubsection no_free_lunch_theorems (end)

    % subsection statistical_learning_theory (end)

% section introduction_to_supervised_learning (end)


\section{Convexification of the Risk} % (fold)
\label{sec:convexification_of_the_risk}

    Before looking at the necessary probabilistic tools, we will first show how problems where the output space is not a vector space, such as binary classification with $y = \{-1, 1\}$, can be reformulated with so-called convex surrogates of loss functions.

    \begin{remark}[Motivation of Risk Convexificatoin]
        As our goal is to estimate a binary-valued function, the first idea that comes into mind is to minimize the empirical risk over a hypothesis space of binary-valued functions. 
        However, this approach leads to a combinatorial problem which can be computationally intractable and moreover, it is not clear how to control the capacity for these type of hypothesis spaces. Learning a real-valued function instead through the problem the framework of convex surrogates simplifies and overcomes this problem as it convexifies the problem and classical penalty-based regularization techniques can be used for theoretical analysis and for algorithms.
    \end{remark}

    Instead of learning $f: \mathcal{X} \mapsto \{-1, 1\}$, we will thus learn a function $g: \mathcal{X} \mapsto \mathbbm{R}$ and define $f(x) = \sign(g(x) )$ where
    \begin{equation*}
        \sign(a) = 
        \left\{ 
        \begin{aligned}
            & +1 \quad &\text{if} \quad a \geq 0 \\
            & -1 \quad &\text{if} \quad a < 0 \\
        \end{aligned}
        \right.
    \end{equation*}
    The risk of the function $f = \sign \circ g$, still denoted $\mathcal{R}(g)$, is then equal to
    $$ \mathcal{R}(g) = \mathbb{P}\big(\sign(g(x) \neq y) \big) = \mathbb{E}\big[\mathbbm{1}(g(x) \neq y) \big] = \mathbb{E}\big[\mathbbm{1}(y g(x) < 0) \big] = \mathbb{E}\big[\phi_{0-1}(y g(x)) \big] $$
    where $\phi_{0-1}: \mathbb{R} \mapsto \mathbb{R}$, with $\phi_{0-1}(u) = \mathbbm{1}(u < 0) $ is called the "margin-based" $0-1$ loss function. For empirical risk minimization, we then miniize the empirical risk
    $$ \hat{\mathcal{R}}(g) = \frac{1}{n} \sum_{i=1}^{n} \phi_{0-1}(y_i g(x_i) ) $$
    with respect to $g: \mathcal{X} \mapsto \mathbb{R}$. However, the function $\phi_{0-1}$ is not continuous (and thus also non-convex) and leads to difficult optimization problems.

    \subsection{Convex Surrogates} % (fold)
    \label{sub:convex_surrogates}

        A key concept in machine learning is the use of convex surrogates, where we replace $\phi_{0-1}$ by another function $\phi$ with better numerical properties (all will be convex). Instead of minimizing the classical risk $\mathcal{R}(g)$ or its empirical version $\hat{\mathcal{R}}(g)$, one then minimizes the $\phi$-risk (and its empirical version) defiend as
        $$ \mathcal{R}_{\phi}(g) = \mathbb{E}\big[\phi(y g(x) ) \big] $$
        and 
        $$ \hat{\mathcal{R}}_{\phi}(g) = \frac{1}{n} \sum_{i=1}^{n} \phi(y g(x) ) $$

        \begin{figure}[htbp]
            \centering
            \includegraphics[width=.6\textwidth]{Pics/connvex_surrogates.png}
        \end{figure}

        \begin{itemize}
            \item \textbf{Quadratic loss}. $\phi(u) = (u - 1)^2$ leading to
                \begin{equation*}
                    \begin{aligned}
                        \phi(y g(x)) &= (yg(x) - 1)^2 = (yg(x) - y^2)^2 = y^2(g(x) - y)^2 \\
                        &= (y - g(x))^2 = (g(x) - y)^2 \\
                    \end{aligned} 
                \end{equation*}
                with the notice that $y^2 = 1$. Hence, we get leas-squares.

            \item \textbf{Logistic loss}. $\phi(u) = \log(1 + e^{-u})$, leading to
                $$ \phi(y g(x)) = \log(1 + e^{- y g(x)}) = - \log \left(\frac{1}{1 + e^{-y g(x)}} \right) = -\log \sigma(y g(x) ) $$
                where $\sigma(\nu) = 1 / (1 + e^{-\nu})$ is the sigmoid function. Note the link with maximum likelihood estimation, where we define the model through
                $$ \mathbb{P}(y = 1 \mid x) = \sigma(f(x)) \quad \text{and} \quad \mathbb{P}(y = -1 \mid x) = \sigma(- f(x) ) = 1 - \sigma(f(x)) $$

            \item \textbf{Hinge loss}. $\phi(u) = \max(1 - u, 0)$, with linaer predictors, this leads to the support vector machine, and $yf(x)$ is often called the "margin" in this context. This loss has a geometric interpretation.

            \item \textbf{Squared Hinge loss}. $\phi(u) = \max(1 - u, 0)^2$, this is a smooth counterpart to the regular hinge loss.
        \end{itemize}
    
    % subsection convex_surrogates (end)

    \subsection{Geometric Interpretation of the Support Vector Machine} % (fold)
    \label{sub:geometric_interpretation_of_the_support_vector_machine}

        We consider $n$ observations $(x_i, y_i) \in \mathbb{R}^d \times \{-1, 1\}$ for $i = 1, \dots, n$.

        \vspace{1em}
        \noindent
        \textbf{Separable data}. We first assume that the data are separable by an affine hyperpalne, that is, there exist $\omega \in \mathbb{R}^d$ and $b \in \mathbb{R}$ such that for all $i \in \{1, \dots, n\}$, 
        $$ y_i (\omega^{\top} x_i + b) > 0 $$
        Among the infinitely many separating hyperplane, we aim at selecting the one for which closet point from the dataset is farthest.

        \begin{figure}[htbp]
            \centering
            \includegraphics[width=.6\textwidth]{Pics/separating_hyperplane.png}
        \end{figure}

        The distance from $x_i$ to the hyperplane $\{x \in \mathbb{R}^d, \omega^{\top} x + b = 0\}$ is equal to $\frac{|\omega^{\top} x_i + b|}{||\omega||_2}$, and thus, this minimal distance is
        $$ \minimize_{x_i} \quad \frac{y_i(\omega^{\top} x_i + b) }{||\omega||_2} $$
        and we thus aim at maximizing this quantity over $\omega$ and $b$. Because of the invariance by rescaling $\omega$ and $b$, that is, we can rescale $(\omega, b)$ pair such that
        $$ \minimize_{x_i} \quad \frac{y_i(\omega^{\top} x_i + b) }{||\omega||_2} = \frac{1}{||\omega||_2} $$
        Then this problem is equivalent to 
        \begin{equation}
            \begin{aligned}
                \minimize_{\omega \in \mathbb{R}^d, \ b \in \mathbb{R}} \quad &\frac{1}{2} ||\omega||_2^2 \\
                \text{subject to} \quad &y_i (\omega^{\top} x_i + b) \geq 1, \quad \forall \ i \in \{ 1, \dots, n\} \\
            \end{aligned}
        \end{equation}

        \vspace{1em}
        \noindent
        \textbf{General data}. When data may not separated by an hyperplane, then we can introduce so-called "slack variables" $\xi_i \geq 0, i = 1, \dots, n$, allowing the constraint $y_i (\omega^{\top} x_i + b) \geq 1$ to be not satisfied, by introducing instead the constraint 
        $$ y_i (\omega^{\top} x_i + b) \geq 1 - \xi_i $$
        The overall amount of slack is then minimized, leading tot he following problem (with $C > 0$)
        \begin{equation}
            \begin{aligned}
                \minimize_{\omega \in \mathbb{R}^d, b \in \mathbb{R}, \xi \in \mathbb{R}^n} \quad & \frac{1}{2} ||\omega||_2^2 + C \sum_{i=1}^{n} \xi_i \\
                \text{subject to} \quad & y_i(\omega^{\top} x_i + b) \geq 1 - \xi_i \\
                & \xi_i \geq 0 \quad \forall \ i \in \{1, \dots, n\} \\
            \end{aligned}
            \label{eq:svc-problem}
        \end{equation}
        With $\lambda = \frac{1}{n C}$, the problem above is equivalent to
        $$ \minimize_{\omega \in \mathbb{R}^d, b \in \mathbb{R}} \quad \frac{1}{n} \sum_{i=1}^{n} \big(1 - y_i(\omega^{\top} x_i + b) \big)^{+} + \frac{\lambda}{2} ||\omega||_2^2 $$
        which is exactly an $l_2$-regularized empirical risk minimization with the hinge loss, for the prediction function $f(x) = \omega^{\top} x + b$.

        \vspace{1em}
        \noindent
        \textbf{Lagrange dual}. The problem in Eq.\ref{eq:svc-problem} is a linearly constrained convex optimmization problem, and can be analyzed using Lagrangian duality. We consdier non-negative Lagrange multipliers $\alpha_i$ and $\beta_i$, $i \in \{1, \dots, n\}$ and the following Lagrangian
        $$ \mathcal{L}(\omega, b, \xi, \alpha, \beta) = \frac{1}{2} ||\omega||_2^2 + C \sum_{i=1}^{n} \xi_i - \sum_{i=1}^{n} \alpha_i\big(y_i(\omega^{\top} x_i + b) - 1 + \xi_i \big) - \sum_{i=1}^{n} \beta_i \xi_i $$
        Minimizing with respect to $\xi \in \mathbb{R}^n$ leads to the constraints
        $$ \alpha_i + \beta_i = C \quad \forall \ i \in \{1, \dots, n \} $$
        while minimizing with respect to $b$ leads to the constraints
        $$ \sum_{i=1}^{n} \alpha_i y_i = 0 $$
        and finally minimizing with respect to $\omega$ tells us
        $$ \omega = \sum_{i=1}^{n} \alpha_i y_i x_i $$
        Substitute these constraints back to Lagrangian we get the dual function and dual optimizaton problem
        \begin{equation}
            \begin{aligned}
                \maximize_{\alpha \in \mathbb{R}^n} \quad &\sum_{i=1}^{n} \alpha_i - \frac{1}{2} \sum_{i,j = 1}^{n} \alpha_i \alpha_j y_i y_j x_i^{\top} x_j \\
                \text{subject to} \quad &\sum_{i=1}^{n} \alpha_i y_i = 0 \\
                & \alpha_i \in [0, C] \quad \forall \ i \in \{1, \dots, n\} \\
            \end{aligned}
        \end{equation}
        As we will show in the future that all $l_2$-regularized learning problems with linear predictors, the optimization problem only depends on the dot-products
        $$ x_i^{\top} x_j \quad \forall \ i,j = 1, \dots, n $$
        and the optimal predictor can be wriiten as a linear combination of input data points $x_i, i = 1, \dots, n$.

        \vspace{1em}
        \noindent
        \textbf{Support vector}. For optimal primal and dual variables, the "complementary slackness" conditions for linear inequality constraints lead to
        $$ \alpha_i \big(y_i (\omega^{\top} x_i + b) - 1 + \xi_i \big) = 0 $$
        and
        $$ \beta_i \xi_i = (C -\alpha_i) \xi_i = 0 $$
        This implies that $\alpha_i = 0$ as soon as $y_i (\omega^{\top} x_i + b) < 1$, and thus many of the $\alpha_i$ are equal to zero, and the optimal predictor is a linear combination of only a few of the data point $x_i$'s which are then called "support vectors".
    
    % subsection geometric_interpretation_of_the_support_vector_machine (end)

    \subsection{Conditional Surrogate Risk and Classification Calibration} % (fold)
    \label{sub:conditional_surrogate_risk_and_classification_calibration}

        Most of the convex surrogates $\Phi$ are upper-bounds on the $0-1$ loss, and all can be made so with rescaling. Using this as the sole justification of the good performance of a convex surrogate is misleading justification, with the exception of problems with almost surely zero loss for the Bayes predictor (which is only possible when the Bayes risk is zero).

        If we denote $\eta(X) = \mathbb{P}(Y = 1 \mid X) \in [0, 1]$, then we have, $\mathbb{E}[Y \mid X] = 2 \eta(x) - 1$ and as in Section \ref{sub:decision_theory},
        \begin{equation*}
             \mathcal{R}(g) = \mathbb{E}\Big[\Phi_{0-1}(Y g(X) ) \Big] = \mathbb{E}\Big[\mathbb{E}\big[\mathbbm{1}(Y \neq g(X) ) \mid X \big] \Big] \geq \mathbb{E}\Big[\min \big\{\eta(X), 1 - \eta(X) \big\} \Big] = \mathcal{R}^*  
        \end{equation*}
        and one of the best classifier is 
        $$ f^{*}(X) = \sign(2\eta(X) - 1) $$
        Note that there are many potential other functions $g(x)$ than $2\eta(X) - 1$ so that $f^*(X) = \sign(g(X))$ is optimal. The first (minor) reason is the arbitrary choice of prediction for the tie $\eta(X) = 1/2$. The other reason is that $g(X)$ simply has to have the same sign as $2\eta(X) - 1$, which leads to many possibilities beyond $2\eta(X) - 1$.

        In order to study the impact of using the $\Phi$-risk, we first look at the conditional risk for a given $X$ (as for the $0$-$1$ loss, the function that $g$ that will minimize the $\Phi$-risk can be determined by looking at each $x$ separately).

        \begin{definition}[Conditional $\Phi$-risk]
            Let $g: \mathcal{X} \rightarrow \mathbb{R}$, we define the conditional $\Phi$-risk as
            \begin{equation}
                \mathbb{E}\big[\Phi(Y g(X) ) \mid X \big] = \eta(X) \Phi(g(X)) + \big(1 - \eta(X)\big) \Phi(- g(X)) := C_{\eta(X)}\big(g(X) \big)
            \end{equation}
            with 
            $$ C_{\eta}(\alpha) = \eta \Phi(\alpha) + (1 - \eta)\Phi(- \alpha) $$
        \end{definition}

        The least we can expect from a convex surrogate is that in the population case, where all $X$'s decouple, the optimal $g(X)$ obtained by minimizing the conditional $\Phi$-risk exactly leads to the same prediction as the Bayes predictor (at least when this prediction is unique). In other words, since the prediction is $\sign(g(X))$, we want that for any $\eta \in [0, 1]$:
        \begin{equation}
            \begin{aligned}
                (\text{positive optimal prediction}), \quad \eta > 1/2 \quad &\Leftrightarrow \quad \argmin_{\alpha \in \mathbb{R}} \ C_{\eta}(\alpha) \subset \mathbb{R}_{+} \\
                (\text{negative optimal prediction}), \quad \eta < 1/2 \quad &\Leftrightarrow \quad \argmin_{\alpha \in \mathbb{R}} \ C_{\eta}(\alpha) \subset \mathbb{R}_{-} \\
            \end{aligned}
            \label{eq:classification-calibrated}
        \end{equation}
        A function $\Phi$ that satisfies these two statement is said \textit{classification-calibrated}, or simply \textit{calibrated}. It turns out that when $\Phi$ is convex, a simple sufficient and necessary condition is avaiable:

        \begin{proposition}[\cite{bartlett2006convexity}]
            Let $\Phi: \mathbb{R} \rightarrow \mathbb{R}$ convex. $\Phi$ calibrated $\Leftrightarrow$ $\Phi$ is differentiable at $0$ and $\Phi'(0) < 0$.
        \end{proposition}

        \begin{figure}[htb]
            \centering
            \includegraphics[width=.7\linewidth]{Pics/classification_calibrated.png}
            \caption{Classification calibration}
            \label{fig:classfication-calibration}
        \end{figure}

        \begin{proof}
            Since $\Phi$ is convex, so is $C_{\eta}$ for any $\eta \in [0, 1]$, and thus we simply consider left and right derivatives at zero too obatin conditions acbout location of minimizers, with the two possibilities below (minimizer in $\mathbb{R}_{+}$ if and only if the right derivative at zero strictly negative, and minimizre in $\mathbb{R}_{-}$ if and only if the left derivative at zero is strictly positive):
            \begin{equation}
                \begin{aligned}
                    \argmin_{\alpha \in \mathbb{R}} \ C_{\eta}(\alpha) \subset \mathbb{R}_{+} \quad &\Leftrightarrow \quad (C_{\eta})_{+}(0)' = \eta \Phi_{+}'(0) - (1 - \eta) \Phi_{-}'(0) < 0 \\
                    \argmin_{\alpha \in \mathbb{R}} \ C_{\eta}(\alpha) \subset \mathbb{R}_{-} \quad &\Leftrightarrow \quad (C_{\eta})_{-}(0)' = \eta \Phi_{-}'(0) - (1 - \eta) \Phi_{+}'(0) < 0 \\
                \end{aligned}
                \label{eq:calibrated-differentiate}
            \end{equation}
            \begin{enumerate}[(a)]
                \item Assume $\Phi$ is calibrated.

                    By letting $\eta$ tend to $(1/2)+$ in Eq.(\ref{eq:calibrated-differentiate}), we have
                        $$ (C_{1/2})_{+}(0)' = \frac{1}{2} [\Phi_{+}'(0) - \Phi_{-}'(0)] \leq 0 $$
                        Since $\Phi$ is convex, we always have $\Phi_{+}'(0) - \Phi_{-}'(0) \geq 0$. Thus the left and right derivatives are equal, which implies that $\Phi$ is differentiable at $0$. Then 
                        $ C_{\eta}'(0) = (2 \eta - 1) \Phi'(0) $
                        and from the first rows of Eq.(\ref{eq:classification-calibrated}) and Eq.(\ref{eq:calibrated-differentiate}), we need to have
                        $\Phi'(0) < 0 $.

                \item Assume $\Phi$ is differentiable at $0$ and $\Phi'(0) < 0$, then $C_{\eta}'(0) = (2 \eta - 1) \Phi'(0)$; Eq.(\ref{eq:classification-calibrated}) are then direct consequnces of Eq.(\ref{eq:calibrated-differentiate}) by noticing the Fig. \ref{fig:classfication-calibration}.
            \end{enumerate}
        \end{proof}
        Note that the proposition above excludes the convex surrogate $u \mapsto (-u)^{+} = \max\{-u, 0\}$, which is not differentiable at zero.
        From now on, we assume that $\Phi$ is calibrated and convex, that is, $\Phi$ convex, $\Phi$ differentiable in $0$, and $\Phi(0) < 0$. We should also notice that if $\Phi(\alpha)$ is symmetric with respect to origin point $0$, we have
        \begin{equation}
            C_{\eta(X)} = (2\eta - 1) \Phi(g(X))
        \end{equation}
    
    % subsection conditional_surrogate_risk_and_classification_calibration (end)

    \subsection{Relationship between Risk and Surrogate Risk} % (fold)
    \label{sub:relationship_between_risk_and_surrogate_risk}
    
        Now that we know that for any $x \in \mathcal{X}$, minimizing $C_{\eta(X)}(g(X))$ with respect to $g(X)$ leads to the optimal prediction through $\sign(g(X))$, we would like to make sure that an explicit control of the excess $\Phi$-risk leads to an explicit control of the original excess risk. In otherwords, we are looking for a monotonic function $H: \mathbb{R}_{+} \rightarrow \mathbb{R}_{+}$ such that 
        $$ \mathcal{R}(g) - \mathcal{R}^{*} \leq H[\mathcal{R}_{\Phi}(g) - \mathcal{R}_{\Phi}^{*}] $$
        where $\mathcal{R}_{\Phi}^{*}$ is the minimum possible $\Phi$-risk. The function $H$ is often called the \textbf{calibration function}.

        We first start with a simple lemma expressing the excess risk, as well as an upper bound, that we need for comparison inequalities below.

        \begin{lemma}
            For any function $g: \mathcal{X} \rightarrow \mathbb{R}$, and for a Bayes predictor $g^{*}$:
            $$ \mathcal{R}(g) - \mathcal{R}^* = \mathbb{E}[|2 \eta(X) - 1| \cdot \mathbbm{1}_{g(X) \cdot g^{*}(X) < 0}] $$
            Moreover, we have $\mathcal{R}(g) - \mathcal{R}(g^*) \leq \mathbb{E}|2\eta(X) - 1 - g(X)|$, and as a matter of fact, for any function $b: \mathbb{R} \rightarrow \mathbb{R}$ that preserves the sign (that is $b(\mathbb{R}_{+}) \subset \mathbb{R}_{+}$ and $b(\mathbb{R}_{-}) \subset \mathbb{R}_{-}$ ), we have
            $$ \mathcal{R}(g) - \mathcal{R}(g^*) \leq \mathbb{E}[|2 \eta(X) - 1 - b(g(X))] $$
            \label{lm:excess-risk-bound}
        \end{lemma}

        \begin{proof}
            Recall that $\eta(X) = \mathbb{P}(Y = 1\mid X)$. 
            We express the excess risk as:
            \begin{equation*}
                \begin{aligned}
                    \mathcal{R}(g) - \mathcal{R}(g^{*}) &= \mathbb{E}\Big[\mathbb{E}\big[\mathbbm{1}_{\sign(g(X)) \neq Y} - \mathbbm{1}_{\sign(g^*(X)) \neq Y} \mid X \big] \Big] \\
                    &= \mathbb{E}\Big[\mathbb{E}\big[\mathbbm{1}_{\sign(g(X)) \neq 1} - \mathbbm{1}_{\sign(g^*(X)) \neq 1} \mid X, Y = 1 \big] \eta(X) + \big[\mathbbm{1}_{\sign(g(X)) \neq 0} - \mathbbm{1}_{\sign(g^*(X)) \neq 0} \mid X, Y = 0 \big] (1 - \eta(X)) \Big] \\
                \end{aligned}
            \end{equation*}
            by definition of the $0 - 1$ loss. For any given $X \in \mathcal{X}$, we can look at the two possible case for the signs of $\eta(X) - 1/2$ and $g(X)$ that lead to different predictions for $g$ and $g^{*}$, namely
            \begin{enumerate}[(a)]
                \item for $\eta(X) > 1/2$ and $g(X) < 0$, the expectation is $\eta(X) - (1 - \eta(X)) = 2 \eta(X) - 1 > 0$; and

                \item for $\eta(X) < 1/2$ and $g(X) > 0$, we get $1 - 2 \eta(X) > 0$
            \end{enumerate}
            By combining these two cases into the condition $g(X) \cdot g^*(X) < 0$ and the condition expectation $|2 \eta{X} - 1|$, we get
            $$ \mathcal{R}(g) - \mathcal{R}(g^*) = \mathbb{E}\Big[|2\eta(X) - 1| \cdot \mathbbm{1}_{\sign(g(X)) \cdot \sign(g^*(X)) < 0} \Big] $$
            which is just the first result.

            For the seoncd result, we simply use the fact that if $g(X) \cdot g^{*}(X) < 0$, then, by splitting the cases in two (the first one being $\eta(X) > 1/2$ and $g(X) < 0$, the second one being $\eta(X) < 1/2$ and $g(X) > 0$), we get 
            $$|2 \eta(X) - 1| \leq |2 \eta(X) - 1 - g(X) |$$
            As long as the funcition $b$ presever the sign of $g(X)$, we obtain the last result.
        \end{proof}

        We see that the excess risk is the expectation of a quantity $|2 \eta(X) - 1| \cdot \mathbbm{1}_{g(X) \cdot g^{*}(X) < 0}$, which is equal to $0$ if the classification is the same as the Bayes predictor and equal to $|2 \eta(X) - 1|$ otherwise. 
        On the other hand, the excess conditional $\Phi$-risk is the quantity
        $$ \eta(X) \Phi(g(X)) + (1 - \eta(X)) \Phi(- g(X)) - \inf_{\alpha}\{\eta(X) \Phi(\alpha) + (1 - \eta(X)) \Phi(- \alpha) \} $$
        which, as a function of $g(X)$, is the deviation between a convex function of $g(X)$ and its minimum value. We simply need to relate it to the quantity $|2 \eta(X) - 1| \cdot \mathbbm{1}_{g(X) \cdot g^{*}(X) < 0}$ above for any $x \in \mathcal{X}$ and take expectations.

        \vspace{1em}
        \cite{bartlett2006convexity} proposes a general framework. Here we will only consider the hinge loss and smooth losses for simplicity.

        \begin{itemize}
            \item \textbf{Hinge Loss}. For the hinge loss $\Phi(\alpha) = (1 - \alpha)^{+} = \max\{1 - \alpha, 0\}$, we can easily compute the minimizer of the conditional $\Phi$-risk (which leads to the minimizer of the $\Phi$-risk). Indeed, we need to minimizer $\eta(X) (1 - \alpha)^{+} + (1 - \eta(X)) (1 + \alpha)^{+}$, which is a piecewise affine function with kinks at $-1$ and $1$, with a minimizer attained at $u = 1$ for $\eta(X) > 1/2$, and symmetrically at $u = -1$ for $\eta(X) < 1/2$, with a minimum conditional $\Phi$-risk equal to $2 \min \{1 - \eta(X), \eta(X)\}$.

            The two excess risks are plotted below for the hinge loss and the $0$-$1$ loss, for $\eta(X) > 1/2$, showing pictorially that the conditional excess $\Phi$-risk is greater than the excess risk.

            \begin{figure}[htb]
                \centering
                \includegraphics[width=.8\linewidth]{Pics/excess-risk-hinge-loss.png}
                \caption{The excess risk of hinge loss and $0$-$1$ loss}
                \label{fig:excess-risk-hinge-loss}
            \end{figure}

            This leads to the calibration function $H(\sigma) = \sigma$ for the hinge loss.

            Note that when the Bayes risk is zero, that is, $\eta(X) \in \{0, 1\}$ almost surely, then using the fact that the hinge loss is an upper-bound on the $0$-$1$ loss is enough to show that the excess risk is less than the excess $\Phi$-risk (indeed, the two optimal risk $\mathcal{R}^*$ and $\mathcal{R}_{\Phi}^{*}$ are equal to zero).

            \item \textbf{Smooth Loss}. We consider smooth losses of the form (up to additive and multiplicative constants) $\Phi(v) - a(v) - v$, where $a(v) = \frac{1}{2} v^2$ for the quadratic loss, $a(v) = 2\log(e^{v/2} + e^{-v/2}) $ for the logistic loss. We assume that $a$ is even ($a(-v) = a(v)$), $a(0) = 0$, $a$ is $\beta$-smooth (that is, $a''(v) \leq \beta$ for all $v \in \mathbb{R}$). This implies that for all $v \in \mathbb{R}$, 
            $$ a(v) - \alpha v - \inf_{w \in \mathbb{R}}\{a(w) - \alpha w \} \geq \frac{1}{2 \beta}|\alpha - a'(v)|^2 $$ 
            leading to
            \begin{equation*}
                \begin{aligned}
                    \mathcal{R}_{\Phi}(g) - \mathcal{R}_{\Phi}^{*} &= \mathbb{E}[a(g(X)) - (2\eta(X) - 1)g(X) - \inf_{w \in \mathbb{R}}{a(w) - (2 \eta(X) - 1)w } ] \\
                    &\geq \frac{1}{2\beta} \mathbb{E} |2 \eta(X) - 1 - a'(g(X)) |^2 \quad \text{by property above} \\
                    &\geq \frac{1}{2\beta} (\mathbb{E}|2 \eta(X - 1 - a'(g(X))| )^2 \quad \text{by Jensens's inequality} \\
                    &\geq \frac{1}{2\beta} \quad \text{by Lemma \ref{lm:excess-risk-bound}}
                \end{aligned}
            \end{equation*}
            This leads to the calibration function $H(\sigma) = \sqrt{\sigma}$ for the square loss and $H(\sigma) = \sqrt{2\sigma}$ for the logistic loss.
        \end{itemize}
            
        \begin{remark}
            Show that the function $a^*$ satisfies $a^* \big(\mathcal{R}(g) - \mathcal{R}^{*}\big) \leq \mathcal{R}_{\Phi}(g) - \mathcal{R}_{\Phi}^{*}$ for any function $g: \mathcal{X} \rightarrow \mathbb{R}$.
        \end{remark}

        We can make the following observations:
        \begin{itemize}
            \item For the (non-smooth) hinge loss, the calibration function is identity, so if the excess $\Phi$-risk goes to zero at a certain rate, the excess risk is goes to zero at the same rate; whereas for smooth losses, the upper-bound only ensures a (worse) rate with a square root. Therefore, when going from the excess $\Phi$-risk to the excess risk, that is, after thresholding the function $g$ at zero, the observed rates may be worse.

            \item Note that the noiseless case when $\eta(X) \in \{0, 1\}$ (zero Bayes risk) leads to stronger calibration function, as well as a series of intermediate "low-noise" conditions.
        \end{itemize}

    % subsection relationship_between_risk_and_surrogate_risk (end)

    \subsection{Impact on Approximation Errors} % (fold)
    \label{sub:impact_on_approximation_errors}

        For the same classification problem, several convex surrogates can be used. While the Bayes classifier is always the same, that is,
        $$ f^{*}(X) = \sign(2 \eta(X) - 1) $$
        the minimmizer of the testing $\Phi$-risk will be different. For example, for the hinge loss, the minimizer $g(X)$ is exactly $\sign(2 \eta(X) - 1)$, while for losses of the form like above $\Phi(v) = a(v) - v$, we have $a'(g(X)) = 2 \eta(X) - 1$, and thus for the square loss $g(X) = 2 \eta(X) - 1$, while for the logistic loss, one can check that $g(X) = \text{atanh}(2 \eta(X) - 1)$ (hyperbolic arc tangent). See example below, with $\mathcal{X} = \mathbb{R}$ and Gaussian class conditional densities.
    
    % subsection impact_on_approximation_errors (end)

% subsection convexification_of_the_risk (end)


\section{Empirical Risk Minimization} % (fold)
\label{sec:empirical_risk_minimization}

    \textbf{Main Concern}. Given a joint distribution $P(X, Y)$ (which is unknown), and $n$ independent and identically distributed observations from $P(X, Y)$, our goal is to learn a function $f: \mathcal{X} \rightarrow \mathcal{Y}$ with minimum risk: 
    \begin{equation*}
        \mathcal{R}_P(f) = \mathbb{E}_{P}[l(Y, f(X))]
    \end{equation*}
    or equivalently minimum excss risk:
    \begin{equation*}
        \mathcal{R}_P(f) - \mathcal{R}_P^* = \mathcal{R}_P(f) - \inf_{g} \mathcal{R}_P(g)
    \end{equation*}
    where $g$ is a measurable function. In this section, we introduce the way called empirical risk minimization, and for simplicity we will omit the subscript $P$ in the expected risk $\mathcal{R}$.

    \subsection{Risk Minimization Decomposition} % (fold)
    \label{sub:risk_minimization_decomposition}

        We consider a family $\mathcal{F}$ of prediction functions $f: \mathcal{X} \rightarrow \mathcal{Y}$. Empirical risk mnimization aims at finding
        $$ \hat{f} \in \argmin_{f \in \mathcal{F}} \ \hat{\mathcal{R}}(f) = \frac{1}{n} \sum_{i=1}^{n} l(Y_i, f(X_i) ) $$
        where the empirical minimize $\hat{f}$ depends on training sample $S_n = \{(X_1, Y_1), \dots, (X_n, Y_n) \}$ and thus is random. In fact, We can decompose the risk with respect to $f$ as follows into two terms:
        \begin{equation}
            \begin{aligned}
                \mathcal{R}(\hat{f}) - \mathcal{R}^* &= \Big\{\mathcal{R}(\hat{f}) - \inf_{f' \in \mathcal{F} } \mathcal{R}(f') \Big\} + \Big\{ \inf_{f' \in \mathcal{F}} \mathcal{R}(f') - \mathcal{R}^* \Big\} \\
                &= \text{estimation error} \quad + \quad \text{approximation error} \\
            \end{aligned} 
        \end{equation}
        A classical example is the situation where the family of functions is parameterized by a subset of $\mathbb{R}^d$, that is, $\mathcal{F} = \{f_\theta, \theta \in \Theta \}$ for $\Theta \subset \mathbb{R}^d$. This includes neural networks and the simplest case of linear model of the form $f_{\theta}(x) = \theta^T \varphi(x) $, for a certain feature vector $\varphi(x)$.

    % subsection risk_minimization_decomposition (end)
    
    \subsection{Approximation Error} % (fold)
    \label{sub:approximation_error}

        Bounding the approximation error corresponds to bounding $\inf_{f \in \mathcal{F}} \mathcal{R}(f) - \mathcal{R}^*$ and requires assumptions on the Bayes predictor $f^*$ to achieve non-trival learning rates.

        Here we will focus on $\mathcal{F} = \{f_{\theta}, \theta \in \Theta \}$ for $\Theta \subset \mathbb{R}^d$ and a convex Lipschitz-continuous losses. By assuming that $\theta_{*}$ is the minimizer of $\mathcal{R}(f_{\theta})$ over $\theta \in \mathbb{R}^d$ (typically, it does not need to belong to $\Theta$), the approximation error decomposes into
        \begin{equation}
            \inf_{\theta \in \Theta} \mathcal{R}(f_{\theta}) - \mathcal{R}^* = \left(\inf_{\theta \in \Theta} \mathcal{R}(f_{\theta}) - \inf_{\theta \in \mathbb{R}^d} \mathcal{R}(f_{\theta}) \right) + \left(\inf_{\theta \in \mathbb{R}^d} \mathcal{R}(f_{\theta}) - \mathcal{R}^* \right)
        \end{equation}
        The second term $\inf_{\theta \in \mathbb{R}^d} \mathcal{R}(f_\theta) - \mathcal{R}^*$ is an incompressible error coming from the chosen of hypothesis space $\mathcal{F}$. While the first term $ \theta \mapsto \mathcal{R}(f_{\theta}) - \inf_{\theta \in \mathbb{R}^d} \mathcal{R}(f_{\theta}) $ is positive on $\mathbb{R}^d$, which can be typically upperbounded by a certain norm $\Omega(\theta - \theta_{*})$. Hence it represents a distance between minimizer $\theta_{*}$ and set $\Theta$ on $\mathbb{R}$

        \begin{figure}[htb]
            \centering
            \includegraphics[width=.5\textwidth]{Pics/approximation_error.png}
            \caption{The distance between minimizer $\theta_{*}$ and set $\Theta$ on $\mathbb{R}$}
        \end{figure}
        For example, if the loss $l(y, \hat{y})$ which is considered as $G$-Lipschitz-continuous with respect to the second variable $\hat{y}$ (possible for regression or convex surrogate for binary classification), we have
        \begin{equation*}
            \mathcal{R}(f_{\theta}) - \mathcal{R}(f_{\theta'}) = \mathbb{E}\Big[l \big(Y, f_{\theta}(X) \big) - l\big(Y, f_{\theta'}(X) \big) \Big] \leq G \cdot \mathbb{E}\big[|f_{\theta}(X) - f_{\theta'}(X)| \big] 
        \end{equation*}
        and hence the first term is upper bounded by $G$ times the smallest distance between $f_{\theta_{*}}$ and $\mathcal{F} = \{f_{\theta}, \theta \in \Theta \}$.
        A classical example will be $f_{\theta}(x) = \theta^T \varphi(x)$, and $\Theta = \{\theta \in \mathbb{R}^d, ||\theta||_2 \leq D \}$, leading to the upper bound 
        $$ \inf_{\theta \in \Theta} \mathcal{R}(f_{\theta}) - \inf_{\theta \in \mathbb{R}^d} \mathcal{R}(f_{\theta} ) \leq G \cdot \mathbb{E}\big[||\varphi(x)||_2 \big] \big(||\theta_{*}||_2 - D \big)^{+} $$
        which is equalt zero if $||\theta_{*}||_2 \leq D$.

    % subsection approximation_error (end)

    \subsection{Estimation Error} % (fold)
    \label{sub:estimation_error}

        The estimation error is often decomposed using the minimizer of the expected risk for our class of models $\mathcal{F}$, $ g \in \argmin_{g \in \mathcal{F}} \mathcal{R}(g) $; and the minimizer of the empirical risk, $ \hat{f} \in \argmin_{f \in \mathcal{F}} \hat{\mathcal{R}}(f)$.
        That is
        \begin{equation}
            \begin{aligned}
                \mathcal{R}(\hat{f}) - \inf_{f \in \mathcal{F}} \mathcal{R}(f) = \mathcal{R}(\hat{f}) - \mathcal{R}(g) &= \left\{ \mathcal{R}(\hat{f}) - \hat{\mathcal{R}}(\hat{f}) \right\} + \left\{\hat{\mathcal{R}}(\hat{f}) - \hat{\mathcal{R}}(g) \right\} + \left\{\hat{\mathcal{R}}(g) - \mathcal{R}(g) \right\} \\
                &\leq \sup_{f \in \mathcal{F}} \left\{\mathcal{R}(f) - \hat{\mathcal{R}}(f) \right\} + \left\{\hat{\mathcal{R}}(\hat{f}) - \hat{\mathcal{R}}(g) \right\} + \sup_{f \in \mathcal{F}} \left\{\hat{\mathcal{R}}(f) - \mathcal{R}(f) \right\} \\
                &\leq \sup_{f \in \mathcal{F}} \left\{\mathcal{R}(f) - \hat{\mathcal{R}}(f) \right\} + 0 + \sup_{f \in \mathcal{F}} \left\{\hat{\mathcal{R}}(f) - \mathcal{R}(f) \right\} \\
                &\leq 2 \sup_{f \in \mathcal{F}} \left|\hat{\mathcal{R}}(f) - \mathcal{R}(f) \right| \\
            \end{aligned}
            \label{eq:estimation-error}
        \end{equation}
        The inequality in the third row holds because we assume $\hat{f}$ is the minimizer of empirical risk, so we have $\hat{\mathcal{R}}(\hat{f}) - \hat{\mathcal{R}} \leq 0 $. When $\hat{f}$ is not the global minimizer of $\hat{\mathcal{R}}$ but simply satisfies $\hat{\mathcal{R}}(\hat{f}) \leq \inf_{f \in \mathcal{F}} \hat{\mathcal{R}}(f) + \varepsilon$, then the \textit{optimization error} $\varepsilon$ has to be added to the bound above

        In general, there are two ways to bound the supremum of empirical process $\sup_{f \in \mathcal{F}} \left|\hat{\mathcal{R}}(f) - \mathcal{R}(f) \right|$. 
        \begin{itemize}
            \item Directly bound $\sup_{f \in \mathcal{F}} |\hat{\mathcal{R}}(f) - \mathcal{R}(f)|$ with high probability (note that $\hat{\mathcal{R}}(f)$ here is a random variable, so we can bound it with high probability)

           \item Bound the uniform deviaiton of $\sup_{f \in \mathcal{F}} |\hat{\mathcal{R}}(f) - \mathcal{R}(f)|$ from its expectation; and then bound the expectation $\mathbb{E}[\sup_{f \in \mathcal{F}} |\hat{\mathcal{R}}(f) - \mathcal{R}(f)|]$
        \end{itemize}
        Before we go deep into the theory of uniform convergence, we see some simple examples.

        \subsubsection{Uniform Deviation from Expectation} % (fold)
        \label{ssub:uniform_deviation_from_expectation}
        
            Let 
            $$ H(Z_1, \dots, Z_n) = \sup_{f \in \mathcal{F}} \left\{\hat{\mathcal{R}}(f) - \mathcal{R}(f) \right\} $$
            where the random variables $z_i = (x_i, y_i)$ are independent and identically distributed, and $\hat{\mathcal{R}}(f) = \frac{1}{n} \sum_{i=1}^{n} l(Y_i, f(X_i))$. We let $l_{\infty}$ be the maximal absolute value of the loss functions for all $(X, Y)$ in the support of the data generating distribution and $f \in \mathcal{F}$, that is
            $l_{\infty} = \max_i |l(Y_i, f(X_i))|$.
            
            When changing a single $Z_i \in \mathcal{X} \times \mathcal{Y}$ into $Z_i' \in \mathcal{X} \times \mathcal{Y}$, the bounded difference of $H$ is almost surely at most $\frac{2}{n} l_\infty$, that is because
            \begin{equation*}
                \begin{aligned}
                    \big| H(Z_1, \dots, Z_i, \dots, Z_n) - H(Z_1, \dots, Z_i', \dots Z_n) \big|
                    &= \left|\sup_{f \in \mathcal{F}} \left\{\hat{\mathcal{R}}(f) - \mathcal{R}(f) \right\} - \sup_{f \in \mathcal{F}} \left\{\hat{\mathcal{R}}'(f) - \mathcal{R}'(f) \right\} \right| \\ 
                    &\leq \left|\sup_{f \in \mathcal{F}} \hat{\mathcal{R}}(f) - \sup_{f \in \mathcal{F}} \hat{\mathcal{R}}'(f) \right| \\
                    &\leq \left| \sup_{f \in \mathcal{F}} \left\{ \hat{\mathcal{R}}(f) - \hat{\mathcal{R}}'(f) \right\} \right| \\
                    &= \left| \sup_{f \in \mathcal{F}} \left\{ \frac{1}{n} (l(Z_i) - l(Z_i')) \right\} \right| \leq \frac{2}{n} l_{\infty} \\
                \end{aligned}
            \end{equation*}
             where the third inequality holds beacue in general, $\sup_h A(f) - \sup_f B(f) \leq \sup_f [A(f) - B(f)]$. Now, we can apply the MacDiarmid inequality, 
            \begin{equation*}
                \mathbb{P}\Big(H(Z_1, \dots, Z_n) - \mathbb{E}[H(Z_1, \dots, Z_n)] \geq t \Big) \leq \exp \left(- \frac{2 t^2}{\sum_{i=1}^{n} (\frac{2}{n} l_{\infty})^2 } \right) = \exp \left(- \frac{n t^2}{2 l_{\infty}^2 } \right) 
            \end{equation*} 
            By setting $\delta = \exp \left(- n t^2 / 2 l_{\infty}^2 \right)$, which leads to $ t = l_\infty \sqrt{\frac{2\log(1 / \delta) }{n} } $, with probability greater than $1 -\delta$, we have
            \begin{equation*}
                H(Z_1, \dots, Z_n) - \mathbb{E}[H(Z_1, \dots, Z_n)] \leq l_\infty \sqrt{\frac{2\log(1 / \delta) }{n} }
            \end{equation*}
            Therefore, recall that $H(Z_1, \dots, Z_n) = \sup_{f \in \mathcal{F}} \left\{\hat{\mathcal{R}}(f) - \mathcal{R}(f) \right\}$, we have
            \begin{equation}
                \sup_{f \in \mathcal{F}} \left\{\hat{\mathcal{R}}(f) - \mathcal{R}(f) \right\} \leq  l_\infty \sqrt{\frac{2\log(1 / \delta) }{n} } + \mathbb{E}_{S_n}\left[\sup_{f \in \mathcal{F}} \left\{\hat{\mathcal{R}}(f) - \mathcal{R}(f) \right\} \right]
                \label{eq:expected-unifrom-deviation}
            \end{equation}
            where $\hat{\mathcal{R}}(f)$ is a random variable relying on training samples $S_n$. We thus only need to bound the expectation of $\sup_{f \in \mathcal{F}} \left\{\hat{\mathcal{R}}(f) - \mathcal{R}(f) \right\}$, and add on top of the above result.

        % subsubsection uniform_deviation_from_expectation (end)

        \subsubsection{Linear Hypothesis Space} % (fold)
        \label{ssub:linear_hypothesis_space}
        
            In this case, we consider the case when the hypothesis function space $\mathcal{F} = \{\theta^{\top} \varphi(x) \mid ||\theta||_2 \leq D \}$ is linear  with $l_2$-ball constraint ($l_2$-norm bounded by $D$), and the loss function is quadratic, that is
            $$ l(Y, f(X)) = (Y - \theta^{\top} \varphi(X))^2 $$
            From these we get
            \begin{equation*}
                \begin{aligned}
                    \hat{\mathcal{R}}(f) - \mathcal{R}(f) &= \theta^{\top} \left(\frac{1}{n} \sum_{i=1}^{n} \varphi(X_i) \varphi(X_i)^{\top} - \mathbb{E}[\varphi(X) \varphi(X)^{\top} ] \right) \theta \\
                    & - 2 \theta^{\top} \left(\frac{1}{n} \sum_{i=1}^{n} Y_i \varphi(X_i) - \mathbb{E}[Y \varphi(X) ] \right) + \left(\frac{1}{n} \sum_{i=1}^{n} Y_i^2 - \mathbb{E}[Y^2] \right) \\
                \end{aligned}
            \end{equation*}
            Hence, the supremum can be upper bounded in closed from as
            \begin{equation*}
                \begin{aligned}
                    \sup_{||\theta||_2 \leq D} |\hat{\mathcal{R}}(f) - \mathcal{R}(f)| &\leq D^2 \left| \left|\frac{1}{n} \sum_{i=1}^{n} \varphi(X_i) \varphi(X_i)^{\top} - \mathbb{E}[\varphi(X) \varphi(X)^{\top} ] \right|\right|_{op} \\
                    &+ 2D \left|\left|\frac{1}{n} \sum_{i=1}^{n} Y_i \varphi(X_i) - \mathbb{E}[Y \varphi(X) ] \right|\right|_2 + \left|\frac{1}{n} \sum_{i=1}^{n} Y_i^2 - \mathbb{E}[Y^2] \right| \\
                \end{aligned}
            \end{equation*}
            where $||M||_{op}$ is the operator norm of the matrix $M$ defined as $||M||_{op} = \sup_{||u||_2 = 1} ||Mu||_2 $.

            \begin{itemize}
                \item Bounding the Matrix

                    Suppose $\varphi(\cdot)$ is a $d$-dimensional function of $X$. Let 
                    $$ M_i = \varphi(X_i) \varphi(X_i)^{\top} - \mathbb{E}[\varphi(X) \varphi(X)^{\top} ] $$
                    Then $M_i$ is a $d \times d$ symmetric matrix with $\mathbb{E}[M_i = 0]$. Given a sequence of $n$ i.i.d symmetric matricies $\{M_i, \ i = 1, \dots, n \}$, we can apply matrix Hoeffding's inequality and get
                    \begin{equation*}
                        \mathbb{P}\left(\lambda_{\max}\left(\frac{1}{n} \sum_{i=1}^{n} M_i \right) \geq t \right) \leq d \cdot \exp \left(- \frac{n t^2}{8 \sigma^2} \right)
                    \end{equation*}
                    where $\sigma^2 = \lambda_{\max}(\bar{M})$. With probability $1 - \delta$, we have $t = \sigma \sqrt{\frac{8 \log(d / \delta) }{n} } $ and
                    \begin{equation*}
                        \lambda_{\max}\left(\frac{1}{n} \sum_{i=1}^{n} M_i \right) \leq \sigma \sqrt{\frac{8 \log(d / \delta) }{n} }
                    \end{equation*}
                    Notice that $\bar{M} = (\frac{1}{n} \sum_{i=1}^{n} M_i)$ is also a symmetric matrix, for any vector $\theta$, we have
                    \begin{equation*}
                        \theta^T \left(\frac{1}{n} \sum_{i=1}^{n} M_i \right) \theta \leq \lambda_{\max}\left(\frac{1}{n} \sum_{i=1}^{n} M_i \right) \theta^T \theta \leq D^2 \sigma \sqrt{\frac{8 \log(d / \delta) }{n} } 
                    \end{equation*}

                \item Bounding the Vector

                    Suppose $\varphi(X)$ is a $d$-dimensional vector, then we're going to find a uniform bound for its $l_2$-norm.
                    \begin{equation*}
                        \begin{aligned}
                            \mathbb{P} \left(\left|\left| \frac{1}{n} \sum_{i=1}^{n} Y_i \varphi(X_i) - \mathbb{E}[Y_i \varphi(X_i)] \right|\right|_2 \geq t \right) &= \mathbb{P} \left( \left[\sum_{j=1}^{d} \left|\frac{1}{n} \sum_{i=1}^{n} Y_i \varphi_j(X_i) - \mathbb{E}[Y_i \varphi_j(X_i)] \right|^2 \right]^{1/2} \geq t \right) \\
                            &= \mathbb{P} \left( \sum_{j=1}^{d} \left|\frac{1}{n} \sum_{i=1}^{n} Y_i \varphi_j(X_i) - \mathbb{E}[Y_i \varphi_j(X_i)] \right|^2 \geq t^2 \right) \\
                            &\leq \sum_{j=1}^{d} \mathbb{P}\left(\left|\frac{1}{n} \sum_{i=1}^{n} Y_i \varphi_j(X_i) - \mathbb{E}[Y_i \varphi_j(X_i)] \right|^2 \geq \frac{t^2}{d} \right) \quad (\text{union bound}) \\
                            &= \sum_{j=1}^{d} \mathbb{P}\left(\left|\frac{1}{n} \sum_{i=1}^{n} Y_i \varphi_j(X_i) - \mathbb{E}[Y_i \varphi_j(X_i)] \right| \geq \frac{t}{\sqrt{d}} \right)  \\
                        \end{aligned}
                    \end{equation*}
                    Now, if we assume $|Y \varphi_j(X)|$ are uniformly bounded by constant $c$ for any $j \in \{1, \dots, d \}$, we can apply Hoeffding's inequality and get
                    \begin{equation*}
                        \mathbb{P}\left(\left|\frac{1}{n} \sum_{i=1}^{n} Y_i \varphi_j(X_i) - \mathbb{E}[Y_i \varphi_j(X_i)] \right| \geq t \right) \leq 2 \exp \left(- \frac{2 n t^2}{d c^2} \right)
                    \end{equation*}
                    which leads to the fact that
                    \begin{equation}
                        \mathbb{P} \left(\left|\left| \frac{1}{n} \sum_{i=1}^{n} Y_i \varphi(X_i) - \mathbb{E}[Y_i \varphi(X_i)] \right|\right|_2 \geq t \right) \leq \sum_{j=1}^{d} 2 \exp \left(- \frac{2 n t^2}{d c^2} \right) = 2d \exp \left(- \frac{2 n t^2}{d c^2} \right)
                    \end{equation}
                    Finally, with probability $1 - \delta$, we have
                    \begin{equation*}
                        \left|\left| \frac{1}{n} \sum_{i=1}^{n} Y_i \varphi(X_i) - \mathbb{E}[Y_i \varphi(X_i)] \right|\right|_2 \leq c \sqrt{\frac{d \log (2d / \delta)}{2n}}
                    \end{equation*}

                \item Bouding the Scalar

                    Similarily, suppose $Z = Y^2$ is a bounded variable with support $[a, b]$, then applying the Hoeffding's bound, we have with probability $1 - \delta$
                    \begin{equation*}
                        \left|\frac{1}{n} \sum_{i=1}^{n} Y_i^2 - \mathbb{E}[Y^2] \right| \leq (b - a) \sqrt{\frac{\log (2 / \delta)}{2n}}
                    \end{equation*}

            \end{itemize}
            Finally, by letting $\delta' = \delta/3$ in each of the three bounds above and applying union bound again, we can upper-bond the empirical process with probability $1 - \delta$,
            \begin{equation*}
                \begin{aligned}
                    \sup_{||\theta||_2 \leq D} |\hat{\mathcal{R}}(f) - \mathcal{R}(f)| 
                    &\leq D^2 \sigma \sqrt{\frac{8 \log(3 d / \delta) }{n} } + 2Dc \sqrt{\frac{\log (6d / \delta)}{2n}} + (b - a) \sqrt{\frac{\log (6 / \delta)}{2n}} \\
                    &\approx (4D^2 \sigma + 2Dc + b - a ) \sqrt{\frac{\log (6 / \delta)}{2n}} = \mathcal{O}\left(\frac{1}{n} \right) \\
                \end{aligned}
            \end{equation*}

        % subsubsection linear_hypothesis_space (end)

        \subsubsection{Finite Hypothesis Space} % (fold)
        \label{ssub:finite_hypothesis_space}
        
            We assume in this section that the loss functions $l(Y, f(X))$ are bounded between $-l_{\infty}$ and $l_{\infty}$.

            \vspace{1em}
            \noindent
            \textbf{Direct Bounding Approach}.
            Using the upper-bound $2 \sup_{f \in \mathcal{F}} |\hat{\mathcal{R}}(f) - \mathcal{R}(f)| $ on the estimation error, we have the union bound:
            \begin{equation*}
                \mathbb{P} \left(\mathcal{R}(\hat{f}) - \inf_{f \in \mathcal{F}} \mathcal{R}(f) \geq t \right) \leq \mathbb{P} \left(2 \sup_{f \in \mathcal{F}} |\hat{\mathcal{R}}(f) - \mathcal{R}(f) | \geq t \right) \leq \sum_{f \in \mathcal{F}} \mathbb{P} \left(2 \left|\hat{\mathcal{R}}(f) - \mathcal{R}(f) \right| \geq t \right)    
            \end{equation*}
            We have, for $f \in \mathcal{F}$ fixed, $\hat{\mathcal{R}}(f) = \frac{1}{n}\sum_{i=1}^{n} l(Y_i, f(X_i) ) $ and we can apply Hoeffding's inequality to bound each $\mathbb{P} \left(2 |\hat{\mathcal{R}}(f) - \mathcal{R}(f)| \geq t \right)$, leading to
            \begin{equation*}
                \mathbb{P}\left(\mathcal{R}(\hat{f}) - \inf_{f \in \mathcal{F}} \mathcal{R}(f) \geq t \right) \leq \sum_{f \in \mathcal{F}} 2 \exp \left(- \frac{n t^2}{2 l_{\infty}^2} \right) = 2 |\mathcal{F}| \exp \left(- \frac{n t^2}{2 l_{\infty}^2} \right)
            \end{equation*}
            Thus, by setting $\delta = 2 |\mathcal{F}| \exp \left(- nt^2 / 2 l_{\infty}^2 \right) $, and finding the corresponding $t$, with probability greater than $1 - \delta$,
            \begin{equation}
                \mathcal{R}(\hat{f}) - \inf_{f \in \mathcal{F}} \mathcal{R}(f) \leq 2 l_{\infty} \sqrt{ \frac{\log(2|\mathcal{F}| / \delta) }{n} }
            \end{equation}

            \vspace{1em}
            \noindent
            \textbf{Bounding the Expectation}.   
            In terms of expectation, we get (using the proof of the expectation of the maximum, which apply both bounded and sub-Gaussian random variables)
            \begin{equation}
                \mathbb{E}\left[\mathcal{R}(\hat{f}) - \inf_{f \in \mathcal{F}} \mathcal{R}(f) \right] \leq 2 \mathbb{E}\left[\sup_{f \in \mathcal{F}} \left|\hat{R}(f) - \mathcal{R}(f) \right| \right] \leq 2 l_{\infty} \sqrt{\frac{2 \log |\mathcal{F}| }{n} }
                \label{eq:finite-expectation-bounds}
            \end{equation}
            Here is the proof, when function family $\mathcal{F}$ is finite, we have
            \begin{equation*}
                \begin{aligned}
                    \mathbb{E}\left[\sup_{f \in \mathcal{F}} |\hat{\mathcal{R}}(f) - \mathcal{R}(f) | \right] &= \mathbb{E}\left[\max \left\{\hat{R}(f_1) - \mathcal{R}(f), \dots, \hat{\mathcal{R}}(f_{|\mathcal{F}|}) - \hat{\mathcal{R}}(f_{|\mathcal{F}|}) \right\} \right] \\
                    &= \mathbb{E}\left[\frac{1}{n} \log e^{t \max \left\{\hat{R}(f_1) - \mathcal{R}(f), \dots, \hat{\mathcal{R}}(f_{|\mathcal{F}|}) - \hat{\mathcal{R}}(f_{|\mathcal{F}|}) \right\} } \right] \\
                    &\leq \frac{1}{t} \log \mathbb{E}\left[e^{t \max \left\{\hat{R}(f_1) - \mathcal{R}(f), \dots, \hat{\mathcal{R}}(f_{|\mathcal{F}|}) - \hat{\mathcal{R}}(f_{|\mathcal{F}|}) \right\} } \right] \quad (\text{Jensen's Inequality}) \\
                    &= \frac{1}{t} \log \mathbb{E}\left[\max \left\{e^{t(\hat{\mathcal{R}}(f_1) - \mathcal{R}(f_1) ) } + \cdots + e^{t(\hat{\mathcal{R}}(f_{|\mathcal{F}| }) - \mathcal{R}(f_{|\mathcal{F}| }) ) }  \right\} \right] \\
                    &\leq \frac{1}{t} \log \mathbb{E}\left[e^{t(\hat{\mathcal{R}}(f_1) - \mathcal{R}(f_1) ) } + \cdots + e^{t(\hat{\mathcal{R}}(f_{|\mathcal{F}| }) - \mathcal{R}(f_{|\mathcal{F}| }) )} \right] \quad (\text{bounding the max by the sum}) \\
                \end{aligned}
            \end{equation*}
            Since the Chernoff bound of bounded loss $l(Y, f(X))$ is
            \begin{equation*}
                \begin{aligned}
                    \mathbb{E}\left[e^{t(\hat{\mathcal{R} }(f_k) - \mathcal{R}(f_k)) } \right] &= \prod_{i=1}^{n} \mathbb{E} \left[e^{\frac{t}{n} \left( l(Y_i, f_k(X_i) ) - \mathbb{E}[l(Y_i, f_k(X_i) ) ] \right) } \right] \\
                    &\leq \prod_{i=1}^{n} \exp \left(\frac{ l_{\infty}^2 t^2}{2 n^2} \right) = \exp \left(\frac{ l_{\infty}^2 t^2}{2 n} \right)
                \end{aligned}
            \end{equation*}
            Substitute the result back to the expectation of estimation error, we get
            \begin{equation*}
                \begin{aligned}
                    \mathbb{E}\left[\sup_{f \in \mathcal{F}} |\hat{\mathcal{R}}(f) - \mathcal{R}(f) | \right] &\leq \frac{1}{t} \log \mathbb{E}\left[e^{t(\hat{\mathcal{R}}(f_1) - \mathcal{R}(f_1) ) } + \cdots + e^{t(\hat{\mathcal{R}}(f_{|\mathcal{F}| }) - \mathcal{R}(f_{|\mathcal{F}| }) )} \right] \\
                    &\leq \frac{1}{t} \log \left(|\mathcal{F}| \exp \left(\frac{ l_{\infty}^2 t^2}{2 n} \right) \right) \\
                    &= \frac{\log |\mathcal{F}|}{t} + l_{\infty}^2 \frac{t}{2n} \\
                \end{aligned}
            \end{equation*}
            Minimizer over $t$, we get $t = \frac{\sqrt{2 n \log |\mathcal{F}|} }{l_{\infty}} $, and therefore
            \begin{equation*}
                 \mathbb{E}\left[\sup_{f \in \mathcal{F}} |\hat{\mathcal{R}}(f) - \mathcal{R}(f) | \right] \leq l_{\infty} \sqrt{\frac{2 \log |\mathcal{F}|}{n}}
            \end{equation*} 

            Finaly, plugging the above result into the equation \ref{eq:expected-unifrom-deviation}, we have with probabability $1 - \delta$
            \begin{equation}
                \mathcal{R}(\hat{f}) - \inf_{f \in \mathcal{F}} \mathcal{R}(f) \leq 2 \sup_{f \in \mathcal{F}} \left\{\hat{\mathcal{R}}(f) - \mathcal{R}(f) \right\} \leq 2 l_{\infty} \left( \sqrt{\frac{2 \log(1/\delta)}{n}} + \sqrt{\frac{2 \log |\mathcal{F}|}{n}} \right)
            \end{equation}

        % subsection finite_hypothesis_space (end)

        \subsubsection{Beyond the Finite Hypothesis Space} % (fold)
        \label{ssub:beyond_the_finite_hypothesis_space}
        
            The simple idea behind covering numbers is to deal with function spaces (with infinitely many elements by approximating them through a finite numner of elements. This is often refered to as an “$\varepsilon$-net argument”.

            \begin{figure}[h]
                \centering
                \begin{minipage}{.45\textwidth}
                    \includegraphics[width=2in]{Pics/covering_number_1.png}
                \end{minipage}
                \begin{minipage}{.45\textwidth}
                    \includegraphics[width=2in]{Pics/covering_number_2.png}
                \end{minipage}
                \caption{The left picture is an example in two dimensions of a covering with Euclidean balls; The right is an example of $l_{\infty}$-balls}
            \end{figure}

            \begin{definition}[Covering Numbers]
                We assume there exists $m = m(\varepsilon)$ elements $f_1, \dots, f_m$ such that for any $f \in \mathcal{F}$, there exists $i \in \{1, \dots, n\}$ such that $d(f, f_i) \leq \varepsilon$. The minimal possible number $m(\varepsilon)$ is the covering number of $\mathcal{F}$ at precision $\varepsilon$.
            \end{definition}

            We first need to assume that the risks $\mathcal{R}$ and $\hat{\mathcal{R}}$ are regular, for example, they are $G$-Lipschitz-continuous with respect to some distance $d$ on $\mathcal{F}$.
            Now, given a cover of $\mathcal{F}$, for all $f \in \mathcal{F}$, and with $(f_i)_{i \in \{1, \dots, m_{\varepsilon}\} }$ the associated cover elements
            \begin{equation*}
                \begin{aligned}
                    |\hat{\mathcal{R}}(f) - \mathcal{R}(f)| & \leq |\hat{\mathcal{R}}(f) - \hat{\mathcal{R}}(f_i)| + |\hat{\mathcal{R}}(f_i) - \mathcal{R}(f_i)| + |\mathcal{R}(f_i) - \mathcal{R}(f) | \\
                    &\leq 2 G \varepsilon + \sup_{i \in \{1, \dots, m(\varepsilon)\}} |\hat{\mathcal{R}}(f_i) - \mathcal{R}(f_i) | \\
                \end{aligned}
            \end{equation*}
            Using bounds \ref{eq:finite-expectation-bounds} on the expectation of the maximum (bounded random variables are sub-Gaussian), we have
            \begin{equation}
                \begin{aligned}
                    \mathbb{E}\left[\sup_{f \in \mathcal{F}} \left|\hat{\mathcal{R}}(f) - \mathcal{R}(f) \right| \right] &\leq 2 G \varepsilon + \mathbb{E}\left[\sup_{i \in \{1, \dots, m(\varepsilon) \} } \left|\hat{\mathcal{R}}(f_i) - \mathcal{R}(f_i) \right| \right] \\ 
                    &\leq 2 G \varepsilon + l_{\infty} \sqrt{\frac{2 \log m(\varepsilon) }{n} } \\
                \end{aligned}
            \end{equation}
            The first term of the bound capture the estmation biased controlled by $\varepsilon$, while the second tem characterize the complexity of the covering. 

            \begin{itemize}
                \item Therefore, if $m(\varepsilon) \sim \varepsilon^{-d}$, ignoring constants, we need to balance $\varepsilon + \sqrt{\frac{d \log(1 / \varepsilon)}{n} } $, which leads to, with a choice of $\varepsilon$ proportional to $1 / \sqrt{n}$, to a rate proportional $\sqrt{\frac{d \log n}{n}}$

                \item Unfortunately, this often leads to a non-optimal dependence on sample size $n$ (because of the existence of $\log n$), as the rate is essentially proportional to $\sqrt{d /n}$

                \item One very powerful tool that avoids these undesired dependences on dimension is Rademacher complexities or Gaussian complexities
            \end{itemize}
        
        % subsubsection beyond_the_finite_hypothesis_space (end)

    % subsection estimation_error (end)  

% section empirical_risk_minimization (end)


\section{PAC Learning and Uniform Convergence} % (fold)
\label{sec:pac_learn_and_uniform_convergence}

    \subsection{PAC Learning} % (fold)
    \label{sub:pac_learning}

        In the previous section, we have shown that for a finite hypothesis space, if the ERM rule with respect to that class is applied on a sufficiently large training sample (whose size is independent of the underlying distribution or labeling function) then the output hypothesis will be probably approximately correct. More generally, we now define \textit{Probably Approximately Correct} (PAC) learning.

        \begin{definition}[PAC Learnability]
            A hypothesis class $\mathcal{F}$ is PAC learnable if there exist a function 
            $ m_{\mathcal{F}}: (0, 1)^2 \mapsto \mathbb{N} $
            and a learning algorithm with the following property: for every $\varepsilon, \delta \in (0, 1)$ and for every probability distribution $P$ over $\mathcal{X}$, and for every labeling function $g: \mathcal{X} \mapsto \{0, 1\}$, if the realizable assumption holds with respect to $\mathcal{F}, P, g$, then when running the learning algorithm on $m \geq m_{\mathcal{F}}(\varepsilon, \delta) $ i.i.d examples generated by $P$ and labeled by $g$, the algorithms returns a hypothesis $f$, such that, with probability of  at least $1 - \delta$, 
            $$ \mathcal{R}_{P, g}(f) \leq \varepsilon $$
            where $\mathcal{R}_{P, g}(f) = \mathbb{E}_{P}[l(g(X), f(X))]$ and $l(y, \hat{y})$ is a loss function.
        \end{definition}

        \noindent
        The definition of Probably Approximately Correct learnability contains two approximation parameters:
        \begin{itemize}
            \item the accuracy parameter $\varepsilon$ determines how far the output classifier can be from the optimal one (approximately correct)

            \item and the confidence parameter $\delta$ indicating how likely the classifier is to meet the accuracy requriement (probably)
        \end{itemize}

        \noindent
        \textbf{Sample Complexity}.
        The function $m_{\mathcal{F}}: (0, 1)^2 \mapsto \mathbb{N}$ determines the \textit{sample complexity} of learning $\mathcal{F}$: that is, how many examples are required to guarantee a probably approximately correct solution.
        The sample complexity $m_{\mathcal{F}}$ is a function of accuracy ($\varepsilon$) and confidence ($\delta$) parameters. It also depends on properties of the hypothesis class $\mathcal{F}$ -- for example, for a finite class we showed that the sample comolexity depends on log the size of $\mathcal{F}$ (see Section \ref{ssub:finite_hypothesis_space}).

        Note that if $\mathcal{F}$ is PAC learnable, there are many functions $m_{\mathcal{F}}$ that satisfy the requirement given in the definition of PAC learnability. Therefore, to be precise, we will define the sample complexity of learning $\mathcal{F}$ to be the "minimal function", in the sense that for any $(\varepsilon, \delta)$, $m_{\mathcal{F}}(\varepsilon, \delta)$ is the minimal integer that satisfies the requirements of PAC learning.

        \begin{corollary}
            Every finite hypothesis class is PAC learnable with sample complexity
            $$ m_{\mathcal{F}}(\varepsilon, \delta) \leq \left\lceil \frac{\log(|\mathcal{F}|/\delta) }{\varepsilon} \right\rceil $$
        \end{corollary}
        There are infinite classes that are learnable as well. Later on we will show that what determines the PAC learnability of a class is not its finiteness but rather a combinatorial measure called the \textit{VC dimension}.
    
    % subsection pac_learning (end)

    \subsection{Agnostic PAC Learning} % (fold)
    \label{sub:agnostic_pac_learning}

        The model we have just described can be readily generalized, so that it can be made relevant to a wide scope of learning tasks. We consider generalizations in two aspects:
        \begin{itemize}
            \item Relaxing the realizability assumption

            \item Learning problems beyond binary classification
        \end{itemize}

        \begin{definition}[Agnostic PAC Learnability]
            A hypothesis class $\mathcal{F}$ is agnostic PAC learnable if there exist a function $m_{\mathcal{F}}: (0, 1)^3 \mapsto \mathbb{N}$ and a learning algorithm with the following property: for every $\varepsilon, \delta \in (0, 1)$ and for every distribution $S_n$ over $\mathcal{X} \times \mathcal{Y}$, when running the learning algorithm on $m \geq m_{\mathcal{F}}(\varepsilon, \delta)$ i.i.d. examples generated by probability distribution $P$, the algorithm returns a hypothesis $h$ such that, with probability of at least $1 - \delta$ (over the choice of the $m$ training examples),
            $$ \mathcal{R}(f) - \inf_{f' \in \mathcal{F}} \mathcal{R}(f') \leq \varepsilon $$
            where here we simply denote $\mathcal{R} = \mathcal{R}_P$ as the expected risk.
        \end{definition}
    
    % subsection agnostic_pac_learning (end)

    \subsection{Uniform Convergence} % (fold)
    \label{sub:uniform_convergence}
        
        In this section, we will show that uniform convergence is sufficient for learnability. 
        The idea behind the learning condition discussed here is very simple. Recall that, given a hypothesis class, $\mathcal{F}$, the empirical risk minimization (ERM) learning a paradigm works as follows:
        Uppon recieving a training sample $S$, the learner evaluates the risk of each $f$ in $\mathcal{F}$ on the given sample and outputs a member of $\mathcal{F}$ that minimizes this empirical risk.

        The hope is that an $f$ minimizes the empirical risk with respect to $S$ is a risk minimizer (or has risk close to the minimum) with respect to the true data probability distribution $P$ as well. 
        Recall that we have shown previously that
        \begin{equation}
            \mathcal{R}(\hat{f}) - \inf_{f \in \mathcal{F}} \mathcal{R}(f) \leq 2 \sup_{f \in \mathcal{F}} \left|\hat{\mathcal{R}}(f) - \mathcal{R}(f) \right|
            \label{eq:estimation-error-bound}
        \end{equation}
        Hence, for that, it suffices to ensure that the empirical risks of all members of $\mathcal{F}$ are good approximations of their true risk. Put another way, we need that uniformly over all hypotheses in the hypothesis class, the empirical risk will be close to the true risk, as formalized in the following.


        \begin{definition}[$\varepsilon$-representative sample]
            A training set $S$ is called $\varepsilon$-representative (w.r.t. domain $\mathcal{X}$, distribution $P$, hypothesis class $\mathcal{F}$ and loss function $l$) if
            $$ \forall \ f \in \mathcal{F}, \ \left|\hat{\mathcal{R}}(f) - \mathcal{R}(f)\right| \leq \varepsilon $$
            where here we dentoe $\hat{\mathcal{R}} = \mathcal{R}_S$ as the empirical risk with respect to sample $S$ and $\mathcal{R} = \mathcal{R}_P$ the expected risk.
        \end{definition}

        The next simple lemma states that whenever the sample is $\varepsilon/2$-representative, the ERM leanring rule is guaranteed to return a good hypothesis.

        \begin{lemma}
            Assume that a training set $S$ is $\varepsilon/2$-representative (w.r.t. domain $\mathcal{X}$, distribution $P$, hypothesis class $\mathcal{F}$ and loss function $l$). Then, any output of $\text{ERM}_{\mathcal{R}}(S)$, namely, any $\hat{f} \in \argmin_{f \in \mathcal{F}} \hat{R}(f)$, satisifies
            $$ \mathcal{R}(\hat{f}) - \inf_{f \in \mathcal{F}} \mathcal{R}(f) \leq \varepsilon $$
        \end{lemma}

        \begin{proof}
            From Eq.(\ref{eq:estimation-error-bound}), we know that
            $$ \mathcal{R}(\hat{f}) - \inf_{f \in \mathcal{F}} \mathcal{R}(f) \leq 2 \sup_{f \in \mathcal{F}} \left|\hat{\mathcal{R}}(f) - \mathcal{R}(f) \right| $$
            Since the training sample $S$ is $\varepsilon/2$-representative, namely, $|\hat{\mathcal{R}}(f) - \mathcal{R}(f)| \leq \varepsilon / 2 $, we have
            $$ \mathcal{R}(\hat{f}) - \inf_{f \in \mathcal{F}} \mathcal{R}(f) \leq \varepsilon $$
        \end{proof}

        The preceding lemma implies that to ensure that the \textbf{ERM is an agnostic PAC learner}, it suffices to show that with probability of at least $1 - \delta$ over the random choice of a training set, it will be an $\varepsilon$-representative training set. The uniform convergence condition formalizes this requirement.

        \begin{definition}[Uniform Convergence]
            We say that a hypothesis class $\mathcal{F}$ has the uniform convergence property (w.r.t. a domain $\mathcal{X}$ and a loss function $l$) if there exists a function
            $$ m_{\mathcal{F}}^{\text{UC}}: (0, 1)^2 \mapsto \mathbb{N} $$
            such that for every $\varepsilon, \delta \in (0, 1)$ and for every probability distribution $P$ over $\mathcal{X}$, if $\mathcal{S}$ is a sample of $m \geq m_{\mathcal{F}}^{\text{UC}}(\varepsilon, \delta)$ examples drawn i.i.d from $P$, then, with probability of at least $1 - \delta$, sample $\mathcal{S}$ is $\varepsilon$-represnetative.
        \end{definition}

        Similar to the definition of sample complexity for PAC learning, the function $m_{\mathcal{F}}^{\text{UC}}$ measures the (minimal) sample complexity of obtaining the uniform convergence property, namely, how many examples we need to ensure that with probability of at least $1 - \delta$ the sample would be $\varepsilon$-representative.

        \begin{remark}
            The term \textit{uniform} here refers to having a fixed sample size that works for all members of $\mathcal{F}$ and for all possible probability distributions $P$ over the domain and some loss function.
        \end{remark}

        \begin{corollary}
            If a class $\mathcal{F}$ has the uniform convergence property with a function $m_{\mathcal{F}}^{UC}$ then the class is agnostically PAC learnable with the sample complexity $m_{\mathcal{F}}(\varepsilon, \delta) \leq m_{\mathcal{F}}^{UC}(\varepsilon/2, \delta)$. Furthermore, in that case, the $\text{ERM}_{\mathcal{F}}$ paradigm is a successful agnostic PAC learner for $\mathcal{F}$.
        \end{corollary}

    % subsection uniform_convergence (end)

% section pac_learning_and_uniform_convergence (end)


\section{Rademacher Complexity} % (fold)
\label{sec:rademacher_complexity}

    In Section \ref{sec:pac_learn_and_uniform_convergence} we have shown that uniform convergence is a sufficient condition for learnability. In this section, we study the Rademacher complexity, which measures the rate of uniform convergence. We will provide generailization bounds based on this measure. To begin with, let's recall the definition of an $\varepsilon$-representative sample.

    \subsection{Motivation for Rademacher Complexity} % (fold)
    \label{sub:motivation_for_rademacher_complexity}
    
        \begin{definition}[$\varepsilon$-representative sample]
            A training set $S$ is called $\varepsilon$-representative (w.r.t. domain $\mathcal{X}$, distribution $P$, hypothesis class $\mathcal{F}$ and loss function $l$) if
            $$ \sup_{f \in \mathcal{F} } \left|\hat{\mathcal{R}}(f) - \mathcal{R}(f)\right| \leq \varepsilon $$
        \end{definition}

        From Eq.(\ref{eq:estimation-error-bound}) we know that if $S$ is an $\varepsilon/2$-representative sample set, then the empirical risk minimization (ERM) rule is $\varepsilon$-consistent, namely, $\mathcal{R}(\hat{f}) - \inf_{f \in \mathcal{F}} \mathcal{R}(f) \leq \varepsilon $, where $\hat{f} = \text{ERM}_{\mathcal{F}}(S)$. 
        For simplicity, we define a new variable $Z$ over domain $\mathcal{Z} = \mathcal{X} \times \mathcal{Y}$, and a function $h$ from hypothesis space $\mathcal{H} = \{h: (Y, X) \mapsto l(Y, f(X) ) \mid f \in \mathcal{F} \}$. The expected risk (w.r.t. probability distribution $P$) and the empirical risk (w.r.t. some sample set) are defined as follows:
        \begin{equation*}
            \mathcal{R}(h) = \mathbb{E}[h(Z)] \quad \text{and} \quad \hat{\mathcal{R}}(h) = \frac{1}{n} \sum_{S} h(Z)
        \end{equation*}
        We define the \textit{representativeness} of $S$ with respect to $\mathcal{H}$ as the largest gap between the expected risk of a function $h$ and its empirical risk, that is,
        \begin{equation}
            \text{Rep}_{P}(\mathcal{H}, S) := \sup_{h \in \mathcal{H}} \hat{R}(h) - \mathcal{R}(h) 
        \end{equation}

        Now suppose we would like to estimate the representativeness of $S$ using the sample $S$ only. One simple idea is to split $S$ into two disjionit sets, $S = S_1 \cup S_2$; refer $S_1$ as a training set and to $S_2$ as a validation set. We can then estimate the representativeness of $S$ by
        \begin{equation}
            \sup_{h \in \mathcal{H}} \ \hat{\mathcal{R}}_{S_1}(h) - \hat{\mathcal{R}}_{S_2}(h)
        \end{equation}
        This can be written more compactly by defining $\sigma = (\sigma_1, \dots, \sigma) \in \{\pm 1\}^n$ to be a vector such that $S_1 = \{Z_i: \sigma_i = 1 \}$ and $S_2 = \{Z_i: \sigma_i = -1 \}$. Then, if we further assume that $|S_1| = |S_2|$, then the equation above can be rewritten as
        \begin{equation}
            \sup_{h \in \mathcal{H}} \ \frac{2}{n} \sum_{i=1}^n \sigma_i h(Z_i)
        \end{equation}
        The Rademacher complexity measure this idea by considering the expectation of the above with respect to a random choice of Rademacher variable $\sigma$. We will see the formal definition in next section.

    % subsection motivation_for_rademacher_complexity (end)

    \subsection{Rademacher Complexity} % (fold)
    \label{sub:rademacher_complexity}

        Rememeber that our goal here is to provide an upper-bound on the empirical process $\sup_{f \in \mathcal{F}} \{\hat{\mathcal{R}}(f) - \mathcal{R}(f) \}$ introduced in Section \ref{sub:estimation_error}, which happens to be equal to
        \begin{equation*}
            \sup_{h \in \mathcal{H}} \left\{\frac{1}{n} \sum_{i=1}^{n} h(Z_i) - \mathbb{E}[h(Z)] \right\}
        \end{equation*}
        Later we will show that the empirical process is upper-bounded by two times the Rademacher complexity. Now, let's see the definition of empirical Rademacher complexity and then the expected Rademacher complexity.

        \begin{definition}[Empirical Rademacher Complexity]
            Let $\mathcal{H}$ be a family of functions mapping from $\mathcal{Z}$ to $\mathbb{R}$, and $S_n = (Z_1, \dots, Z_n)$ a fixed sample of size $n$ with elements in $\mathcal{Z}$. Then the empirical Rademacher compelxity of $\mathcal{H}$ with respect to the sample $S_n$ is defined as
            \begin{equation}
                \hat{R}_{S_n}(\mathcal{H}) = \mathbb{E}_{\sigma} \left[\sup_{h \in \mathcal{H}} \frac{1}{n} \sum_{i=1}^{n} \sigma_i h(Z_i) \right]
                \label{eq:empirical-Rademacher-complexity} 
            \end{equation}
            $$  $$
            where $\sigma = (\sigma_1, \dots, \sigma_n)^{\top}$, with $\sigma_i$'s independent uniform random variables taking values in $\{\pm 1\}$. The random variables $\sigma_i$ are called Rademacher variables.
        \end{definition}
        Let $h_S$ denote the vector of values taken by function $h$ over the sample $S$, namely, $h_S = (h(Z_1), \dots, h(Z_n))^{\top}$. Then the empirical Rademacher complexity can be rewritten as
        $$ \hat{R}_{S_n}(\mathcal{H}) = \mathbb{E}_{\sigma} \left[\sup_{h \in \mathcal{H}} \frac{\langle \sigma, h_S \rangle }{n} \right] $$
        Thus, the empirical Rademacher complexity measures on average how well the function class $\mathcal{H}$ correlates with random noise on $S$. This describes the richness of the family $\mathcal{H}$: richer or more complex families $\mathcal{H}$ can generate more vectors $h_S$ and thus better correlate with random noise, on average.

        \begin{definition}[Rademacher Complexity]
            Let $P$ denote the probability distribution according to which samples are drawn. For any integer $n \geq 1$, the Rademacher complexity is the expectation of the empirical Rademahcer complexity over all samples of size $n$ drawn i.i.d. with respect to $P$.
            \begin{equation}
                R_n(\mathcal{H}) = \mathbb{E}_{P}\left[\hat{R}_{S_n}(\mathcal{H}) \right] = \mathbb{E}_{\sigma, P} \left[\sup_{h \in \mathcal{H}} \frac{1}{n} \sum_{i=1}^{n} \sigma_i h(Z_i) \right]
                \label{eq:Rademacher-complexity}
            \end{equation}
        \end{definition}

        Now, we show that, through a general "symmetrization" property, the Rademacher complexity $R_n(\mathcal{H})$ directly controls the expectation of empirical process, that is $\mathbb{E}[\sup_{f \in \mathcal{F}} (\hat{\mathcal{R}}(f) - \mathcal{R}(f)) ]$.

        \begin{theorem}[Symmetrization]
            Given the Rademacher complexity of $\mathcal{H}$ defined in equaiton \ref{eq:Rademacher-complexity}, we have
            \begin{equation}
                \mathbb{E}\left[\sup_{h \in \mathcal{H}} \left(\frac{1}{n} \sum_{i=1}^{n} h(Z_i) - \mathbb{E}[h(Z)] \right) \right] \leq 2 R_n(\mathcal{H})
            \end{equation}
            and 
            \begin{equation}
                \mathbb{E}\left[\sup_{h \in \mathcal{H}} \left(\mathbb{E}[h(Z)] - \frac{1}{n} \sum_{i=1}^{n} h(Z_i) \right) \right] \leq 2 R_n(\mathcal{H})
            \end{equation}
            \label{thm:symmetrization-Rademacher-complexity}
        \end{theorem}

        \begin{proof}
            Let $S_n' = \{Z_1', \dots, Z_n'\}$ be an independent copy of the data $S_n = \{Z_1, \dots, Z_n\}$. Let $(\sigma_i)_{i \in \{1, \dots, n\}}$ be i.i.d. Rademacher random variables, which are also independent of $S_n$ and $S_n'$. Using that for all $i \in \{1, \dots, n\}$, $\mathbb{E}[h(Z_i') \mid S_n ] = \mathbb{E}[h(Z)]$, we have
            \begin{equation*}
                \begin{aligned}
                    \mathbb{E}\left[\sup_{h \in \mathcal{H}} \left(\mathbb{E}[h(Z)] - \frac{1}{n} \sum_{i=1}^{n}h(Z_i) \right) \right] &= \mathbb{E}\left[\sup_{h \in \mathcal{H}} \left(\frac{1}{n} \sum_{i=1}^{n} \mathbb{E}[h(Z_i') \mid S_n] - \frac{1}{n} \sum_{i=1}^{n} h(Z_i) \right) \right] \\
                    &= \mathbb{E}\left[\sup_{h \in \mathcal{H}} \left(\frac{1}{n} \sum_{i=1}^{n} \mathbb{E}[h(Z_i') - h(Z_i) \mid S_n ]  \right) \right] \\
                \end{aligned}
            \end{equation*}
            by definition of the independent copy $S_n'$. Then using that the supremum of the expectation is less than expectation of the supremum, 
            \begin{equation*}
                \begin{aligned}
                    \mathbb{E}\left[\sup_{h \in \mathcal{H}} \left(\mathbb{E}[h(Z)] - \frac{1}{n} \sum_{i=1}^{n}h(Z_i) \right) \right] &\leq \mathbb{E}\left[\mathbb{E}\left( \sup_{h \in \mathcal{H}} \left(\frac{1}{n} \sum_{i=1}^{n}[h(Z_i') - h(Z_i) ] \right) \mid S_n \right) \right] \\
                    &= \mathbb{E}\left[\sup_{h \in \mathcal{H}} \left(\frac{1}{n} \sum_{i=1}^{n}[h(Z_i') - h(Z_i) ] \right) \right] \\
                    &= \mathbb{E}\left[\sup_{h \in \mathcal{H}} \left(\frac{1}{n} \sum_{i=1}^{n} \sigma_i [h(Z_i') - h(Z_i) ] \right) \right] \quad (\text{symmetrization}) \\
                    &\leq \mathbb{E}\left[\sup_{h \in \mathcal{H}} \left(\frac{1}{n} \sum_{i=1}^{n} \sigma_i h(Z_i) \right) \right] + \mathbb{E}\left[\sup_{h \in \mathcal{H}} \left(\frac{1}{n} \sum_{i=1}^{n} -\sigma_i h(Z_i) \right) \right] \\
                    &= 2 \mathbb{E}\left[\sup_{h \in \mathcal{H}} \left(\frac{1}{n} \sum_{i=1}^{n} \sigma_i h(Z_i) \right) \right] = 2 R_n(\mathcal{H})
                \end{aligned}
            \end{equation*}
            The reasoning is essentially identical for $\mathbb{E}\left[\sup_{h \in \mathcal{H}} \left(\mathbb{E}[h(Z)] - \frac{1}{n} \sum_{i=1}^{n} h(Z_i) \right) \right] \leq 2 R_n(\mathcal{H})$.

        \end{proof}

        \begin{theorem}[Generalization bonud via Rademacher Complexity]
            Suppose for all $h \in \mathcal{H}$, $0 \leq h(Z) \leq 1$. Then, with probability at least $1 - \delta$,
            \begin{equation}
                \begin{aligned}
                    \sup_{h \in \mathcal{H}} \left[\frac{1}{n} \sum_{i=1}^{n}h(Z_i) - \mathbb{E}[h(Z)] \right] &\leq 2 R_{n}(\mathcal{H}) + \sqrt{\frac{\log (2 / \delta) }{2n}} \\
                    \sup_{h \in \mathcal{H}} \left[\frac{1}{n} \sum_{i=1}^{n}h(Z_i) - \mathbb{E}[h(Z)] \right] &\leq 2 \hat{R}_{S}(\mathcal{H}) + 3 \sqrt{\frac{\log (2 / \delta) }{2n}} \\
                \end{aligned}
                \label{eq:generalization-bound-Rademacher-complexity}
            \end{equation}
            \label{thm:generalization-bound-Radeamcher-complexity}
        \end{theorem}

        \begin{proof}
            For conciseness, define
            $$ H(Z_1, \dots, Z_n) := \sup_{h \in \mathcal{H}} \left[\frac{1}{n} \sum_{i=1}^{n} h(Z_i) - \mathbb{E}[h(Z_i) ] \right] $$
            and we prove the theorem for four steps.

            \begin{itemize}
                \item Step 1. We bound $H$ using McDiarmid's inequality. To use McDiarmid's inequality, we firstly check that the bounded difference condition holds:
                    \begin{equation*}
                        \begin{aligned}
                            H(Z_1, \dots, Z_i, \dots, Z_n) - H(Z_1, \dots, Z_i', \dots, Z_n) &\leq \sup_{h \in \mathcal{H}} \left\{\frac{1}{n} \sum_{j=1}^{n} h(Z_j) \right\} - \sup_{h \in \mathcal{H}} \left\{\frac{1}{n} \sum_{j \neq i}^{n} h(Z_j) + \frac{1}{n} h(Z_i') \right\} \\
                            &\leq \sup_{h \in \mathcal{H}} \left\{\frac{1}{n} \sum_{j = 1}^n h(Z_j) - \frac{1}{n} \sum_{j \neq i}^{n} h(Z_j) - \frac{h(Z_i') }{n} \right\} \\ 
                            &= \sup_{h \in \mathcal{H}} \left\{\frac{1}{n}(h(Z_i) - H(Z_i') )  \right\} \\
                            &\leq \frac{1}{n} \qquad (\text{given } h(Z) \leq 1) \\
                        \end{aligned}
                    \end{equation*}
                    where the second inequality holds beacue in general, $\sup_h A(f) - \sup_f B(f) \leq \sup_f [A(f) - B(f)]$. We can thus apply McDiarmid's inequality with parameters $c_1 = \cdots = c_n = 1/n$,
                    \begin{equation*}
                        \mathbb{P}\Big(H(Z_1, \dots, Z_n) - \mathbb{E}[H(Z_1, \dots, Z_n)] \geq t \Big) \leq \exp \left(- \frac{2 t^2}{\sum_{i=1}^{n}c_i^2 } \right) = \exp \left(- 2 n t^2 \right) 
                    \end{equation*}
                    that is, with probability $1 - \delta$
                    \begin{equation*}
                        H(Z_1, \dots, Z_n) \leq \mathbb{E}[H(Z_1, \dots, Z_n)] + \sqrt{\frac{\log (2/\delta)}{2n} }
                    \end{equation*} 
                    where here we set $\exp(-2 n t^2) = \delta/2$.

                \item Step 2. We apply the Theorem \ref{thm:symmetrization-Rademacher-complexity} to get the upper bound of the expectation of the empirical process 
                \begin{equation*}
                    \mathbb{E} [H(Z_1, \dots, Z_n)] \leq 2 R_n(\mathcal{H})
                \end{equation*}
                which implies
                $$ \sup_{h \in \mathcal{H}} \left[\frac{1}{n} \sum_{i=1}^{n}h(Z_i) - \mathbb{E}[h(Z)] \right] \leq 2 R_{n}(\mathcal{H}) + \sqrt{\frac{\log (2 / \delta) }{2n}} $$

                \item Step 3. Bound expected Rademacher complexity through empirical Rademacher complexity and McDiarmid inequality. To begin with, define
                    $$ \tilde{H} = \hat{R}_{S_n}(\mathcal{H}) := \mathbb{E}_{\sigma} \left[\sup_{h \in \mathcal{H}} \frac{1}{n} \sum_{i=1}^{n} \sigma_i h(Z_i) \right] $$
                    Using a similar argument in Step 1, we find that $\tilde{H}$ also satisfies the bounded difference condition:
                    \begin{equation*}
                        \begin{aligned}
                            \tilde{H}(Z_1, \dots, Z_i, \dots, Z_n) - \tilde{H}(Z_1, \dots, Z_i', \dots, Z_n) &\leq \mathbb{E}\left[ \sup_{h \in \mathcal{H}} \left\{\frac{1}{n} \sum_{j=1}^{n} h(Z_j) \right\} - \sup_{h \in \mathcal{H}} \left\{\frac{1}{n} \sum_{j \neq i}^{n} h(Z_j) + \frac{1}{n} h(Z_i') \right\} \right] \\
                            &\leq \mathbb{E}\left[ \sup_{f \in \mathcal{F}} \left\{\frac{1}{n} \sum_{j = 1}^n h(Z_j) - \frac{1}{n} \sum_{j \neq i}^{n} h(Z_j) - \frac{h(Z_i') }{n} \right\} \right] \\ 
                            &= \mathbb{E} \left[ \sup_{h \in \mathcal{H}} \left\{\frac{1}{n}(h(Z_i) - h(Z_i') ) \right\} \right] \\
                            &\leq \frac{1}{n} \\
                        \end{aligned}
                    \end{equation*}
                    because the term inside the sup is always upper-bounded by $1$. We can therefore apply McDiarmid's inequality again with parameter $c_1 = \cdots c_n = 1/n$ and get
                    \begin{equation*}
                        \mathbb{P}\Big( \tilde{H}(Z_1, \dots, Z_n) - \mathbb{E}[\tilde{H}(Z_1, \dots, Z_n)] \geq t \Big) \leq \exp \left(- \frac{2 t^2}{\sum_{i=1}^{n}c_i^2 } \right) = \exp \left(- 2 n t^2 \right) 
                    \end{equation*}
                    and 
                    \begin{equation*}
                        \mathbb{P}\Big( \tilde{H}(Z_1, \dots, Z_n) - \mathbb{E}[\tilde{H}(Z_1, \dots, Z_n)] \leq -t \Big) \leq \exp \left(- \frac{2 t^2}{\sum_{i=1}^{n}c_i^2 } \right) = \exp \left(- 2 n t^2 \right) 
                    \end{equation*}
                    that is, with probability $1 - \delta$
                    \begin{equation*}
                        \mathbb{E}[ \tilde{H}(Z_1, \dots, Z_n)] \leq \tilde{H}(Z_1, \dots, Z_n) + \sqrt{\frac{\log (2/\delta)}{2n} }
                    \end{equation*} 
                    where here we set $\exp(-2 n t^2) = \delta/2 $. 

                \item Step 4. Putting all things together by noticing that
                    $$ \mathbb{E}[\tilde{H}] = \mathbb{E}_{P}[\hat{R}_{S_n}(\mathcal{H})] = R_n(\mathcal{H}) $$
                    we have with probability $1 - \delta$,
                    \begin{equation*}
                        \begin{aligned}
                            \sup_{h \in \mathcal{H}} \left[\frac{1}{n} \sum_{i=1}^{n}h(Z_i) - \mathbb{E}[h(Z)] \right] = H(Z_1, \dots, Z_n) &\leq \mathbb{E}[H(Z_1, \dots, Z_n)] + \sqrt{\frac{\log (2/\delta)}{2n} } \qquad (\text{Step 1.}) \\
                            &\leq 2 R_n(\mathcal{H}) + \sqrt{\frac{\log (2/\delta)}{2n}} \qquad (\text{Step 2.}) \\
                            &\leq 2 \left(\hat{R}_{S}(\mathcal{H}) + \sqrt{\frac{\log (2/\delta)}{2n} } \right) + \sqrt{\frac{\log (2/\delta)}{2n} } \qquad (\text{Step 3.}) \\
                            &= 2 \hat{R}_{S}(\mathcal{H}) + 3 \sqrt{\frac{\log (2/\delta)}{2n} } 
                        \end{aligned}
                    \end{equation*}
            \end{itemize}
        \end{proof}

        A useful fact is that both empirical Rademacher complexity and expected Rademacher complexity are translation invariant.

        \begin{proposition}
            Let $\mathcal{H}$ be a family of functions mapping $\mathcal{Z} \mapsto \mathbb{R}$ and define $\mathcal{H}' = \{h'(Z) = h(Z) + c_0 \mid h \in \mathcal{H} \}$ for some $c_0 \in \mathbb{R}$. Then we have 
            $$ \hat{R}_{S}(\mathcal{H}) = \hat{R}_{S}(\mathcal{H}') \quad \text{and} \quad R_{n}(\mathcal{H}) = R_{n}(\mathcal{H}') $$
        \end{proposition}

        \begin{proof}[Hint]
            The property of Rademacher random variables.
        \end{proof}
    
    % subsection rademacher_complexity (end)

    \subsection{Lipschitz-continuous Losses} % (fold)
    \label{sub:lipschitz_continuous_losses}

        A particularly appealing property in our context is the following property, sometimes called the “contraction principle”.

        \begin{remark}
            For a compact intevral, 
            continuously differentiable $\subseteq$ Lipschitz continuous $\subseteq$ absolutely continuous $\subseteq$ bounded variation $\subseteq$ differentiable almost everywhere
        \end{remark}

        \begin{proposition}[Contraction Principle - Lipschitz-continuous Functions]
            Given any functions $b$, $a_i : \Theta \mapsto \mathbb{R}$ (no assumption on $\Theta$) and $\varphi_i: \mathbb{R} \mapsto \mathbb{R}$ any $1$-Lipschitz-functions, for $i = 1, \dots, n$, we have, for $\sigma \in \mathbb{R}^n$ a vector of independent Rademacher random variables,
            \begin{equation}
                \mathbb{E}_{\sigma}\left[\sup_{\theta \in \Theta} \ b(\theta) + \sum_{i=1}^{n} \sigma_i \varphi_i(a_i(\theta) ) \right] \leq \mathbb{E}_{\sigma}\left[\sup_{\theta \in \Theta} \ b(\theta) + \sum_{i=1}^{n} \sigma_i a_i(\theta) \right]
            \end{equation}
        \end{proposition}

        \begin{proof}
            We consider a proof by induction on $n$. The case $n = 0$ is trivial, and we show how to go from $n \geq 0$ to $n + 1$. We thus consider $\mathbb{E}_{\sigma_1, \dots, \sigma_{n+1}} \left[\sup_{\theta \in \Theta} b(\theta) + \sum_{i=1}^{n+1} \sigma_i \varphi_i(a_i(\theta) ) \right]$ and compute the expectation with respect to $\sigma_{n+1}$ explicitly, by considering the two potential values with probability $1/2$,
            \begin{equation*}
                \begin{aligned}
                    &\mathbb{E}_{\sigma_1, \dots, \sigma_{n+1}} \left[\sup_{\theta \in \Theta} b(\theta) + \sum_{i=1}^{n+1} \sigma_i \varphi_i(a_i(\theta) ) \right] \\
                    = & \frac{1}{2} \mathbb{E}_{\sigma_1, \dots, \sigma_n} \left[\sup_{\theta \in \Theta} b(\theta) + \sum_{i=1}^{n} \sigma_i \varphi_i(a_i(\theta) ) + \varphi_{n+1}(a_{n+1}(\theta) ) \right] + \frac{1}{2} \mathbb{E}_{\sigma_1, \dots, \sigma_n} \left[\sup_{\theta \in \Theta} b(\theta) + \sum_{i=1}^{n} \sigma_i \varphi_i(a_i(\theta) ) - \varphi_{n+1}(a_{n+1}(\theta) ) \right] \\
                    = & \mathbb{E}_{\sigma_1, \dots, \sigma_n} \left[\sup_{\theta, \theta' \in \Theta} \frac{b(\theta) + b(\theta')}{2} + \sum_{i=1}^{n} \sigma_i \frac{\varphi_i(a_i(\theta)) + \varphi_i(a_i(\theta')) }{2} + \frac{\varphi_{n+1}(a_{n+1}(\theta) ) - \varphi_{n+1}(a_{n+1}(\theta') ) }{2} \right] \\
                \end{aligned}
            \end{equation*}
            By taking the supremum over $(\theta, \theta')$ and $(\theta', \theta)$ and using Lipschitz-continuity, we get
            \begin{equation*}
                \begin{aligned}
                    & \mathbb{E}_{\sigma_1, \dots, \sigma_n} \left[\sup_{\theta, \theta' \in \Theta} \frac{b(\theta) + b(\theta')}{2} + \sum_{i=1}^{n} \sigma_i \frac{\varphi_i(a_i(\theta)) + \varphi_i(a_i(\theta')) }{2} + \frac{\varphi_{n+1}(a_{n+1}(\theta) ) - \varphi_{n+1}(a_{n+1}(\theta') ) }{2} \right] \\
                    = &\mathbb{E}_{\sigma_1, \dots, \sigma_n} \left[\sup_{\theta, \theta' \in \Theta} \frac{b(\theta) + b(\theta')}{2} + \sum_{i=1}^{n} \sigma_i \frac{\varphi_i(a_i(\theta)) + \varphi_i(a_i(\theta')) }{2} + \frac{ \big| \varphi_{n+1}(a_{n+1}(\theta) ) - \varphi_{n+1}(a_{n+1}(\theta') ) \big| }{2} \right] \\
                    \leq &\mathbb{E}_{\sigma_1, \dots, \sigma_n} \left[\sup_{\theta, \theta' \in \Theta} \frac{b(\theta) + b(\theta')}{2} + \sum_{i=1}^{n} \sigma_i \frac{\varphi_i(a_i(\theta)) + \varphi_i(a_i(\theta')) }{2} + \frac{ \big| a_{n+1}(\theta)  - a_{n+1}(\theta') \big| }{2} \right] \\
                    = &\mathbb{E}_{\sigma_1, \dots, \sigma_n} \left[\sup_{\theta, \theta' \in \Theta} \frac{b(\theta) + b(\theta')}{2} + \sum_{i=1}^{n} \sigma_i \frac{\varphi_i(a_i(\theta)) + \varphi_i(a_i(\theta')) }{2} + \frac{a_{n+1}(\theta)  - a_{n+1}(\theta') }{2} \right] \\
                \end{aligned}
            \end{equation*}
            The first and last equalities hold becasue of the fact that $\sup_{\theta, \theta' \in \Theta} a_{n+1}(\theta) - a_{n+1}(\theta') $ at least equal to zero.
            Now, we can redo the exact same sequence of equalities with $\varphi_{n+1}$ being the identity, to obtain that the last expression above is equal to
            \begin{equation*}
                \begin{aligned}
                    &\mathbb{E}_{\sigma_1, \dots, \sigma_{n}} \mathbb{E}_{\sigma_{n+1}} \left[\sup_{\theta \in \Theta} b(\theta) + \sum_{i=1}^{n} \sigma_i \varphi_i(a_i(\theta) ) + \sigma_{n+1} a_{n+1}(\theta) \right] \\
                    \leq &\mathbb{E}_{\sigma_1, \dots, \sigma_{n}} \mathbb{E}_{\sigma_{n+1}} \left[\sup_{\theta \in \Theta} b(\theta) + \sum_{i=1}^{n} \sigma_i a_i(\theta) + \sigma_{n+1} a_{n+1}(\theta) \right] \quad \text{by the induction hypothesis}
                \end{aligned}
            \end{equation*}
            which leads to the desired result.

        \end{proof}

        We can apply the contraction principle above to supervised learning situations where $u_i \mapsto l(y_i, u_i)$ is $G$-Lipschitz-continuous for all $i$ almost surely (possible for regression or when using a convex surrogate for binary classification), leading to
        \begin{equation}
            \mathbb{E}_{\sigma} \left[\sup_{f \in \mathcal{F}} \frac{1}{n} \sum_{i=1}^{n} \sigma_i l(Y_i, f(X_i) ) \mid S_n \right] \leq G \cdot \mathbb{E}_{\sigma} \left[\sup_{f \in \mathcal{F}} \frac{1}{n} \sum_{i=1}^{n} \sigma_i f(X_i) \mid S_n \right] 
        \end{equation} 
        by the contraction principle, which leads to
        \begin{equation}
            \frac{1}{2} \mathbb{E}\left[\sup_{h \in \mathcal{H}} \left(\frac{1}{n} \sum_{i=1}^{n} \sum_{i=1}^{n} h(Z_i) - \mathbb{E}[h(Z)] \right) \right] \leq R_n(\mathcal{H}) \leq G \cdot R_n(\mathcal{F})
            \label{eq:Rademacher-complexity-of-hypothesis-class}
        \end{equation}
        Thus, the Rademacher complexity of the class of prediction functions $R_n(\mathcal{F})$ controls the expectation of empirical process.
    
    % subsection lipschitz_continuous_losses (end)

    \subsection{Ball-constrained Linear Predictions} % (fold)
    \label{sub:ball_constrained_linear_predictions}

        We now assume that $\mathcal{F} = \{f_{\theta}(X) = \theta^{\top} \varphi(X) \mid \Omega(\theta) \leq D \}$ where $\Omega$ is norm on $\mathbb{R}^d$. We denote by $\Phi \in \mathbb{R}^{n \times d}$ the design matrix, that is,
        \begin{equation*}
            \Phi = 
            \begin{bmatrix}
                \varphi_{1}(X_1) & \varphi_{2}(X_1) & \cdots & \varphi_d(X_1) \\
                \varphi_{1}(X_2) & \varphi_{2}(X_2) & \cdots & \varphi_d(X_2) \\
                \vdots & \vdots  &  & \vdots \\
                \varphi_{1}(X_n) & \varphi_{2}(X_n) & \cdots & \varphi_d(X_n) \\
            \end{bmatrix}
        \end{equation*}
        We have
        \begin{equation}
            \mathcal{R}_n(\mathcal{F}) = \mathbb{E}\left[\sup_{\Omega(\theta) \leq D} \left(\frac{1}{n} \sum_{i=1}^{n} \sigma_i \theta^{\top} \varphi(X_i) \right) \right] = \mathbb{E}\left[\sup_{\Omega(\theta) \leq D}\left(\frac{1}{n} \sigma^{\top} \Phi \theta \right) \right] = \frac{D}{n} \mathbb{E}\Big[\Omega^*(\Phi^{\top} \sigma) \Big]
        \end{equation}
        where $\Omega^*(u) = \sup \{u^{\top} \theta \mid \Omega(\theta) \leq 1 \}$ is the dual norm of $\Omega$.  
        \begin{itemize}
            \item when $\Omega$ is the $l_p$-norm with $p \in [1, \infty]$, then $\Omega^*$ is the $l_q$-norm, with conjugate realtion $\frac{1}{p} + \frac{1}{q} = 1$

            \item $||\cdot||_1^{*} = ||\cdot||_{\infty}$ and $||\cdot ||_{\infty}^{*} = ||\cdot ||_1$ and $||\cdot||_2^{*} = ||\cdot||_2$.
        \end{itemize}
        Thus, computing Rademacher complexities is equivalent to computing expectation of norms. When $\Omega = ||\cdot||_2$, we get:
        \begin{equation}
            \begin{aligned}
                \mathcal{R}_n(\mathcal{F}) &= \frac{D}{n} \mathbb{E}\big[||\Phi^{\top} \sigma||_2 \big] \\
                &\leq \frac{D}{n} \sqrt{\mathbb{E}[||\Phi^{\top} \sigma||_2^2 ] } \quad (\text{Jensens' inequality apply on } f(x) = x^2) \\
                &= \frac{D}{n} \sqrt{\mathbb{E}[\tr(\Phi^{\top} \sigma \sigma^{\top} \Phi )] } \quad (\text{holds for any vector}) \\
                &= \frac{D}{n} \sqrt{\mathbb{E}[\tr(\Phi^{\top} \Phi)] } \quad (\text{using that IID such that } \mathbb{E}[\sigma \sigma^{\top}] = I) \\
                &= \frac{D}{n} \sqrt{\sum_{i=1}^{n} \mathbb{E}[\varphi(X_i)^{\top} \varphi(X_i) ] } = \frac{D}{n} \sqrt{\sum_{i=1}^{n} \mathbb{E}||\varphi(X_i)||_2^{2} } = \frac{D}{\sqrt{n}} \sqrt{\mathbb{E}||\varphi(X)||_2^2} \\
            \end{aligned}
            \label{eq:Rademacher-complexity-of-function-class}
        \end{equation}
        We thus obtain a \textit{dimension-independent} Rademacher complexity that we can use in the summary below.

        \begin{example}
            Upper-bound the Rademacher complexity for $\Omega = ||\cdot||_1$.
        \end{example}
    
    % subsection ball_constrained_linear_predictions (end)

    \subsection{Putting Things Together (Linear Predictions)} % (fold)
    \label{sub:putting_things_together_}

        With all the elements above (section \ref{sub:lipschitz_continuous_losses} and \ref{sub:ball_constrained_linear_predictions}), we can now propose the following general result (where no convexity of the loss function is assumed).

        \begin{proposition}[Estimation Error]
            Assume a $G$-Lipschitz-continuous loss function, linear prediction functions with $\mathcal{F} = \{f_{\theta}(X) = \theta^{\top} \varphi(X) \mid ||\theta||_2 \leq D \}$, where $\mathbb{E}||\varphi(X)||_{2}^{2} \leq R^2$. Let $\hat{f} = f_{\hat{\theta}} \in \mathcal{F}$ be the minimizer of the empirical risk, then:
            \begin{equation*}
                \mathbb{E}\big[\mathcal{R}(f_{\hat{\theta}}) \big] - \inf_{||\theta||_2 \leq D} \ \mathcal{R}(f_{\theta}) \leq \frac{2GRD}{\sqrt{n} }
            \end{equation*}
            It is essential to know that $f_{\hat{\theta}}$ here is a random variable.
        \end{proposition}

        \begin{proof}
            Using Proposition \ref{prop:symmetrization}, equation \ref{eq:Rademacher-complexity-of-hypothesis-class} and \ref{eq:Rademacher-complexity-of-function-class}, we get the desire result.
        \end{proof}

        \begin{proposition}[Approximation Error]
            If we assume that there exists a minimizer $\theta_{*}$ of $\mathcal{R}(f_{\theta})$ over $\mathbb{R}^d$, the approximation error is upper-bounded by
            \begin{equation*}
                \begin{aligned}
                    \inf_{||\theta||_2 \leq D} \ \mathcal{R}(f_{\theta}) - \mathcal{R}(f_{\theta_*}) &\leq G \inf_{||\theta||_2 \leq D} \mathbb{E}[|f_{\theta}(X) - f_{\theta_*}(X) ] \quad (G\text{-Lipschitz-continuous loss function}) \\
                    &= G \inf_{||\theta||_2 \leq D} \mathbb{E}[|\varphi(X)^{\top}(\theta - \theta_{*}) ] \\
                    &\leq G \inf_{||\theta||_2 \leq D} ||\theta - \theta_{*}||_2 \cdot \mathbb{E}[||\varphi(X)||_2^2] \leq GR \inf_{||\theta||_2 \leq D} ||\theta - \theta_{*}||_2
                \end{aligned}
            \end{equation*}
        \end{proposition}
        This leads to empirical risk minimization error is uppber-bounded by
        \begin{equation}
            \mathbb{E}[\mathcal{R}(f_{\hat{\theta}}) ] - \mathcal{R}(f_{\theta_*}) \leq GR \inf_{||\theta||_2 \leq D} ||\theta - \theta_{*}||_2 + \frac{2GRD}{\sqrt{n}} = GR(||\theta_{*}||_2 - D)^{+} + \frac{2GRD}{\sqrt{n} }
        \end{equation}
        We see that for $D = ||\theta_{*}||_2$, we obtain the bound $\frac{2GR||\theta_*||_2}{\sqrt{n}}$, but this setting requires to know $||\theta_{*}||_2$ which is not possible in practice. 
        \begin{itemize}
            \item if $D$ is too large, the estimation error gets larger, leading to overfitting;

            \item while if $D$ is too small, the approximation error can quickly kick in (with a value that does not go to zero when $n$ tends to infinity), leading to underfitting.
        \end{itemize}
    
    % subsection putting_things_together_ (end)

    \subsection{From Constrained to Regularized Estimation} % (fold)
    \label{sub:from_constrained_to_regularized_estimation}

        In practice, it is preferable to penalize by the norm $\Omega(\theta) = ||\theta||_2$ instead of constraining (the main reasons being that the hyperparameter is easier to find and the optimization is easier).

        For simplicity, we only consider the $l_2$-norm here. We now denote $\hat{\theta}_{\lambda}$ the minimizer of 
        \begin{equation}
            \hat{\mathcal{R}}(f_{\theta}) + \frac{\lambda}{2} ||\theta||_2^2
            \label{eq:regularized-empirical-risk}
        \end{equation}
        If the loss is always positive, then
        $$ \frac{\lambda}{2} ||\hat{\theta}_{\lambda}||_2^2 \leq \hat{\mathcal{R}}(f_{\hat{\theta}_{\lambda}}) + \frac{\lambda}{2} ||\hat{\theta}_{\lambda}||_2^2 \leq \hat{\mathcal{R}}(f_0) $$
        leading to a bound $||\hat{\theta}_{\lambda}||_2 = \mathcal{O}(1 / \sqrt{\lambda} )$. Thus, with $D = \mathcal{O}(1 / \sqrt{\lambda})$ in the bound above, this lead to a deviation of $\mathcal{O}(1 / \sqrt{\lambda n})$, which is not optimal.
        
        We now cite \cite{sridharan2008fast} without proof an interesting stronger result using the strong convexity of the squared $l_2$-norm.

        \begin{proposition}[Fast Rates for Regularized Objectives]
            Assume a $G$-Lipschitz-continuous \textbf{convex} loss function, linear prediction functions with $\mathcal{F} = \{f_{\theta}(X) = \theta^{\top} \varphi(X) \mid ||\theta||_2 \leq D \}$, where $\mathbb{E}||\varphi(X)||_2^2 \leq R^2$. Let $\hat{\theta}_{\lambda} \in \mathbb{R}^d$ be the minimizer of the regularized empirical risk in equation \ref{eq:regularized-empirical-risk}, then
            \begin{equation*}
                \mathbb{E}[\mathcal{R}(f_{\hat{\theta}_{\lambda}})] \leq \inf_{\theta \in \mathbb{R}^d} \left\{\mathcal{R}(f_{\theta}) + \frac{\lambda}{2} ||\theta||_2^2 \right\} + \frac{32 G^2 R^2}{\lambda n}     
            \end{equation*} 
        \end{proposition}
        Note that we obtain a "fast rate" in $\mathcal{O}(R^2 / (\lambda n) )$, which has a better dependence in $n$, but depends on $\lambda$, which can be very small in practice. One classical choice of $\lambda$ if $\lambda \propto GR / (\sqrt{n} ||\theta_{*}||) $, leading to the slow rate
        \begin{equation*}
            \mathbb{E}[\mathcal{R}(f_{\theta_*} )] \leq \mathcal{R}(f_{\theta_*}) + \mathcal{O}\left(\frac{GR}{\sqrt{n}} ||\theta_*||_2 \right)
        \end{equation*}
    
    % subsection from_constrained_to_regularized_estimation (end)

% section rademacher_complexity (end)


\section{Growth Function and VC-Dimension} % (fold)
\label{sec:growth_function_and_vc_dimension}

    \subsection{Growth Function} % (fold)
    \label{sub:growth_function}

        Here we will show how the Rademacher complexity can be bounded in terms of the growth function in binary classification problem.
        To begin with, recall that the empirical Rademacher complexity with respect to sample $S$ with size $n$ is defined as
        $$ \hat{R}_S(\mathcal{H}) = \mathbb{E}_{\sigma} \left[\sup_{h \in \mathcal{H}} \frac{1}{n} \sum_{i=1}^{n} \sigma_i h(X_i) \right] $$

        \begin{definition}[Growth Function]
            The growth function $\Pi_{\mathcal{H}}: \mathbb{N} \mapsto \mathbb{N}$ for a hypothesis set $\mathcal{H}$ is defined by
            \begin{equation}
                \forall \ n \in \mathbb{N}_{+}, \ \Pi_{\mathcal{H}}(n) = \max \left| \big\{(h(X_1), \dots, h(X_n) ) \mid h \in \mathcal{H}, \ X_1, \dots, X_n \in \mathcal{X} \big\} \right|
                \label{eq:growth-function}
            \end{equation}
            where $|\cdot|$ compute the cardinality of a set.
        \end{definition}

        Thus, $\Pi_{\mathcal{H}}(n)$ is the maximum number of distinct ways in which $n$ points can be classified using hypotheses in $\mathcal{H}$. This provides another measure of the richness of the hypothesis set $\mathcal{H}$. However, unlike the Rademacher complexity, this measure does not depend on the distribution, it is purely \textbf{combinatorial}.
        To relate the Rademacher complexity to the growth function, we will use Massart’s lemma.

        \begin{theorem}[Massart's Lemma]
            Let $A \subset \mathbbm{R}^n$ be a finite set, with $r = \sup_{X \in A} ||X||_2$, then the following holds:
            \begin{equation}
                \mathbb{E}_{\sigma} \left[\sup_{X \in \mathcal{A}} \frac{1}{n} \sum_{i=1}^{n} \sigma_i X_i \right] \leq \frac{r \sqrt{2 \log |A|} }{n}
            \end{equation}
            where $\sigma_i$'s are independent Rademacher variables taking values in $\{-1, +1\}$ and $X_1, \dots, X_n$ are the components of vector $X$.
            \label{thm:Massart-lemma}
        \end{theorem}

        \begin{proof}
            For any $t > 0$, using Jensen's inequality, rearranging terms, and bounding the supremum by a sum, we obtain:
            \begin{equation*}
                \begin{aligned}
                    \exp \left(t \cdot \mathbb{E}_{\sigma}\left[\sup_{X \in A} \sum_{i=1}^{n} \sigma_i X_i \right] \right) &\leq \mathbb{E}_{\sigma} \left[\exp\left(t \sup_{X \in A}\sum_{i=1}^{n} \sigma_i X_i \right) \right] \\
                    &= \mathbb{E}_{\sigma} \left[\sup_{X \in A} \exp \left(t \sum_{i=1}^{n} \sigma_i X_i \right) \right] \\ 
                    &\leq \sum_{X \in A} \mathbb{E}_{\sigma} \left[\exp \left(t \sum_{i=1}^n \sigma_i X_i \right) \right]
                \end{aligned}
            \end{equation*}
            We next use the independence of the $\sigma_i$'s, then apply the bound in Eq.(\ref{eq:Hoeffding-lemma}), and the definition of $r$ to write:
            \begin{equation*}
                \begin{aligned}
                    \exp \left(t \cdot \mathbb{E}_{\sigma}\left[\sup_{X \in A} \sum_{i=1}^{n} \sigma_i X_i \right] \right) &\leq \sum_{X \in A} \prod_{i=1}^{n} \mathbb{E}[\exp(t \sigma_i X_i) ] \\
                    &\leq \sum_{X \in A} \prod_{i=1}^{n} \exp \left(\frac{t^2 (2 X_i)^2}{8} \right) \\
                    &= \sum_{X \in A} \exp \left(\frac{t^2}{2} \sum_{i=1}^{n} X_i^2 \right) \\
                    &\leq \sum_{X \in A} \exp \left(\frac{t^2 r^2}{2} \right) = |A| \cdot \exp \left(\frac{t^2 r^2}{2} \right)
                \end{aligned}
            \end{equation*}
            The last inequality holds by applying he definition of $r$, which is 
            $$r := \sup_{X \in \mathcal{A}} ||X||_2 = \sup_{X \in \mathcal{A}} \sqrt{\sum_{i=1}^{n} X_i^2} $$
            Taking the logarithm on both sides and dividing by $t$ yields:
            \begin{equation}
                \mathbb{E}_{\sigma} \left[\sup_{X \in A} \sum_{i=1}^{n} \sigma_i X_i \right] \leq \frac{\log |A|}{t} + \frac{t r^2}{2}
            \end{equation}
            Since such inequality holds for every $t$, we can minimize over $t$ and get $t = \frac{\sqrt{2 \log |A|}}{r}$ and get
            $$ \mathbb{E}_{\sigma} \left[\sup_{X \in A} \sum_{i=1}^{n} \sigma_i X_i \right] \leq r \sqrt{2 \log |A|} $$
            Dividing both sides by $n$ leads to the desired result.

        \end{proof}
        Using this result, we can bound the Rademacher complexity $R_{n}(\mathcal{H})$ in terms of the growth function $\pi_{\mathcal{H}}$.

        \begin{corollary}
            Let $\mathcal{H}$ be a family of functions taking values in $\{-1, +1\}$. Then the following holds:
            \begin{equation}
                R_{n}(\mathcal{H}) \leq \sqrt{\frac{2 \log \Pi_{\mathcal{H}}(n)}{n}}
            \end{equation}
        \end{corollary}

        \begin{proof}
            For a fixed sample $S_n = (X_1, \dots, X_n) \sim P$, we denote by $H_{S_n}$ the set of vectors of function values $(h(X_1), \dots, h(X_n))^{\top}$ where $h$ is in $\mathcal{H}$. Singce $h \in \mathcal{H}$ take values in $\{-1, +1\}$, the norm of these vectors is bounded by $\sqrt{n}$. We can then apply Massart's lemma as follows:
            $$ R_n(\mathcal{H}) = \mathbb{E}_{P} \left[\mathbb{E}_{\sigma} \left[\sup_{u \in H_{S_n}} \frac{1}{n} \sum_{i=1}^{n} \sigma_i u_i \right] \right] \leq \mathbb{E}_{P}\left[\frac{\sqrt{n} \sqrt{2 \log |H_{S_n}|} }{n} \right] $$
            By definition, $|H_{S_n}|$ is bounded by the growth function $\Pi_{\mathcal{H}}(n)$, thus,
            $$ R_n(\mathcal{H}) \leq \mathbb{E}_{P} \left[\frac{\sqrt{n} \sqrt{2 \log \Pi_{\mathcal{H}}(n) } }{n} \right] = \sqrt{\frac{2 \log \Pi_{\mathcal{H}}(n)}{n}} $$
            which concludes the proof.

        \end{proof}
    
        Combining the generalization bound via Rademacher complexity in Thm. \ref{thm:generalization-bound-Radeamcher-complexity} with the corollary above yields immediately the following generalization bound in terms of the growth function.

        \begin{theorem}[Generalization Bound via Growth Function]
            Let $\mathcal{H}$ be a family of functions taking values in $\{-1, +1\}$. Then, for any $\delta > 0$, with probability at least $1 - \delta$, we have
            \begin{equation}
                \mathcal{R}(h) \leq \hat{\mathcal{R}}(h) + 2 \sqrt{\frac{2 \log \Pi_{\mathcal{H}}(n)}{n}} + \sqrt{\frac{\log (2 / \delta) }{2n}}
                \label{eq:generalization-bound-growth-function}
            \end{equation}     
            $h \in \mathcal{H}$. Again, recall that $\mathcal{R}(h) = \mathbb{E}[h(Z)]$ and $\hat{\mathcal{R}}(h) = \frac{1}{n} \sum_{i=1}^{n} h(Z_i) $.
            \label{thm:generalization-bound-growth-function}
        \end{theorem}

        Growth function bounds can also be derived directly (withougt using Rademacher complexity bounds first). The resulting bound is：
        \begin{equation}
            \mathbb{P}\left(\left|\mathcal{R}(h) - \hat{\mathcal{R}}(h) \right| > \varepsilon \right) \leq 4 \Pi_{\mathcal{H}}(2n) \exp \left(- \frac{n \varepsilon}{8} \right)
            \label{eq:direct-generalization-bound-growth-function}
        \end{equation}

        The computation of the growth function may not be always convenient since, by definition, it requires computing $\Pi_{\mathcal{H}}(n)$ for all $n \geq 1$. The next section introduces an alternative measure of the complexity of a hypothesis set H that is based instead on a single scalar, which will turn out to be in fact deeply related to the behavior of the growth function.

    % subsection growth_function (end)

    \subsection{VC-dimension} % (fold)
    \label{sub:vc_dimension}

        Here, we introduce the notion of VC-dimension (Vapnik-Chervonenkis dimension). The VC-dimension is also a purely combinatorial notion but it is often easier to compute than the growth function (or the Rademacher Complexity). As we shall see, the VC-dimension is a key quantity in learning and is directly related to the growth function.

        To define the VC-dimension of a hypothesis class $\mathcal{H}$, we first introduce the concepts of \textit{dichotomy} and that of \textit{shattering}. Given a hypothesis set $\mathcal{H}$, a dichotomy of a set $S$ is one of the possible ways of labeling the points of $S$ using a hypothesis in $\mathcal{H}$. A set $S$ of $n \geq 1$ is said to be shattered by a hypothesis class $\mathcal{H}$ when $\mathcal{H}$ realizes all possible dichotomies of $S$, that is when $\Pi_{\mathcal{H}}(n) = 2^n$.

        \begin{definition}[VC-dimension]
            The VC-dimension of a hypothesis class $\mathcal{H}$ is the size of the largest sample set that can be fully shattered by $\mathcal{H}$:
            \begin{equation}
                \text{VCdim}(H) = \max\{n: \Pi_{\mathcal{H}}(n) = 2^n \}
                \label{eq:VC-dimension}
            \end{equation}
        \end{definition}

        \begin{remark}
            Note that, by definition, if $\text{VCdim}(\mathcal{H}) = d$, there exists a sample set of size $d$ that can be fully shattered. But, this does not imply that all sets of size $d$ or less are fully shattered, in fact, this is typically not the case.
        \end{remark}

        In general, to compute the VC-dimension, we will typically show a lower bound for its value and then a matching upperbouond. To given a lower bound $d$ for $\text{VCdim}(\mathcal{H})$, it suffices toshow that a set $S$ of cardinality $d$ can be shattered by $\mathcal{H}$. To give an upper bound, we need to prove that no set $S$ of cardinality $d + 1$ can be shattered by $\mathcal{H}$, which is typically more difficult.
        The followings are some examples of classifers and their VC dimension.
        \begin{itemize}
            \item \textbf{Interval Classifier on real line}. Consider a hypothesis class $\mathcal{H} = \{h_{[a, b]} \mid \forall \ a < b \}$ where
                \begin{equation*}
                    h_{[a, b]}(X) = \left\{
                    \begin{aligned}
                        & +1, \ x \in [a, b] \\
                        & -1, \ x \notin [a, b] \\
                    \end{aligned}
                    \right.
                \end{equation*}
                It can be shown that $\text{VCdim}(\mathcal{H}) = 2$ in this case. Suppose there is a sample set $S$ with two random variables $(X_1, X_2) = (x_1, x_2)$. Without loss of generality, we suppose $x_1 < x_2$. Then, all four possible dichotomies $(+1, +1)$, $(-1, +1)$, $(+1, -1)$ and $(-1, -1)$ can be realized by some classifer $h_{[a, b]}$. In contrast, by the definition of intervals, no set of three point can be shattered since the case $(+1, -1, +1)$ labeling cannot be realized by any $h_{[a, b]}$. Hence,
                $$ \text{VCdim}(\text{intervals in } \mathbb{R}) = 2 $$

            \item \textbf{Hyperplane Classifier}.
                $$ \text{VCdim}(\text{hyperpalnes in } \mathbb{R}^d) = d + 1 $$
                To begin with, we derive a lower bound by starting with a set of $d + 1$ points in $\mathbb{R}^d$, setting $X_0$ to be the origin and defining $X_i$, for $i \in \{1, \dots, d \}$, as the point whose $i$th coordinate is $1$ and other points are $0$, that is,
                $$ X_{ii} = 1 \quad \text{and} \quad X_{ij} = 0 \ \forall j \neq i $$
                for all $d + 1$ points. Let $Y_0, Y_1, \dots, Y_d \in \{-1, +1 \}$ be an arbitrary set of labels for $X_0, X_1, \dots, X_d$. Let $w$ be the vector whose $i$th coordinate is $Y_i$. Then the classifier defined by the hyperplane $\{x \mid w^{\top} X + Y_0 / 2 = 0 \}$ shatters $X_0, X_1, \dots, X_d$ since for any $i \in \{0,1, \dots, d\}$,
                \begin{equation}
                    \sign\left(w^{\top} X_i + \frac{Y_0}{2} \right) = \sign \left(Y_i + \frac{Y_0}{2} \right) = Y_i    
                \end{equation} 
                To obtain an upper bound, it suffices to show that no set of $d + 2$ points can be shattered by halfspaces. Concretely, let $S$ be a set of $d + 2$ points. By Radon's Theorem \ref{thm:Radon-thm}, it can be partitioned into two sets $X_1$ and $X_2$ such that their convex hulls intersect. 
                Observe that when two sets of points $S_1$ and $S_2$ are separaetd by a hyperplane, and each of their convex hulls are also separated by that hyperplane. Thus, $S_1$ and $S_2$ cannot be separated by a hyperplane and $S$ is not shattered. 

                Combining our lower and upper bounds, we have proven that VC-dimension is $d + 1$ in this case.

            \item \textbf{Axis-ligned Rectangles}.
                $$ \text{VCdim}(\mathcal{H}) = 4 $$
                We first show that the VC-dimension is at least four, by considering four points in a diamond pattern. Then it is clear that all $16$ dichotomies can be realized.

            \item \textbf{Convex Polygons}.

            \item \textbf{Sine Functions}.

                The previous examples could suggest that the VC-dimension of $\mathcal{H}$ coincides with the number of free parameters defining $\mathcal{H}$. For example, the number of parameters defining hyperplanes matches their VC-dimension. However, this does not hold in general.

                \begin{figure}[h]
                    \centering
                    \includegraphics[width=.6\textwidth]{Pics/sine_function.png}
                    \caption{An example of a sine function used for classification}
                    \label{fig:sine-function}
                \end{figure}

                Here is a striking example. Consider the following of sine functions: 
                $$ \mathcal{H} = \{t \mapsto \sin(\omega t) \mid \omega \in \mathbb{R} \}$$
                These singe function can be used to classifiy the points on the real line: a point is labeled positively if it is above the curve, negatively otherwise. Altough this family of sine function is defined via a single parameter, $\omega$, it can be shown that $\text{VCdim}(\text{sine functions}) = + \infty$.

        \end{itemize}

        \begin{theorem}[Radon's theorem]
            Any set $S$ of $d + 2$ points in $\mathbb{R}^d$ can be partitioned into two subsets $S_1$ and $S_2$ such that the convex hulls of $S_1$ and $S_2$ intersect.  
            \label{thm:Radon-thm}
        \end{theorem}

        \begin{proof}
            Let $S = \{X_1, \dots, X_{d+2}\} \subset \mathbb{R}^d$. The following is a system of $d + 1$ linear equations in $\alpha_1, \dots, \alpha_{d+2}$,
            $$ \sum_{i=1}^{d+2} \alpha_i X_i = 0 \quad \text{and} \quad \sum_{i=1}^{d+2} \alpha_i = 0 $$
            This is because the first equality leads to $d$ equations, one for each component of point. The number of unknown, $d + 2$, is large than the number of equations, $d + 1$, and therefore the system admits a non-zero solution $\beta_1, \dots, \beta_{d+2}$.

            Since $\sum_{i=1}^{d+2} \beta_i = 0$, both $I_1 = \{i \in [1, d + 2] \mid \beta_i > 0\}$ and $I_2 = \{i \in [1, d + 2] \mid \beta_i < 0 \}$ are non-empty sets. Then $S_1 = \{X_i \mid i \in I_1 \}$ and $S_2 = \{X_i \mid i \in I_2 \}$ form a partition of $S$. By the last equation above, we have
            $$ \sum_{i \in I_1} \beta_i = - \sum_{i \in I_2} \beta_i $$
            Let $\beta = \sum_{i \in I_1} \beta_i $, then the first part $\sum_{i=1}^{d+2} \alpha_i X_i = 0$ implies that
            $$ \sum_{i \in I_1} \frac{\beta_i}{\beta} X_i = \sum_{i \in I_2} \frac{- \beta_i}{\beta} X_i $$
            with $\sum_{i \in I_1} \beta_i / \beta = 1 = \sum_{i \in I_2} -\beta_i / \beta$, and $\beta_i / \beta \geq 0$ for $i \in I_1$ and $\frac{- \beta_i}{\beta} \geq 0$ for $i \in I_2$. By definition of the convex hulls, this implies that $\sum_{i \in I_1} \frac{\beta_i}{\beta} X_i $ belongs both to the convex hull of $S_1$ and to that of $S_2$.

        \end{proof}
        
    % subsection vc_dimension (end)

    \subsection{Link Growth Function and VC-dimension} % (fold)
    \label{sub:link_growth_function_and_vc_dimension}

        We have shown that the VC-dimension of many other hypothesis sets can be determined or uppder bounded in a similar way. In particular, the VC-dimension of any vector space of dimension $r < \infty$ can be shown to be at most $r$. The next result known as Sauer's lemma clarifies the connection between the notions of growth function and VC-dimension.

        \begin{theorem}[Sauer's lemma]
            Let $\mathcal{H}$ be a hypothesis set with $\text{VCdim}(\mathcal{H}) = d$. Then, for all $m \in \mathbb{N}$, the following inequality holds:
            \begin{equation}
                \Pi_{\mathcal{H}}(n) \leq \sum_{i=0}^{d} \binom{n}{i}
            \end{equation}
        \end{theorem}

        \begin{proof}
            The proof is by induction on $n + d$. The statement clearly holds for $n = 1$ and $d = 0$ or $d = 1$. Now, assume that it holds for $(n - 1, d - 1)$ and $(n - 1, d)$. Fix a sample set $S = \{x_1, \dots, x_n \}$ with $\Pi_{\mathcal{H}}(n)$ dichotomies and let $G = \mathcal{H}_S$ be the set of concepts $\mathcal{H}$ induces by restriction to $S$.

            Now consider the following families over $S' = \{x_1, \dots, x_{n-1}\}$. We define $G_1 = G_{S}$ as the set of concepts $\mathcal{H}$ includes by restriction to $S'$. Next, ...

        \end{proof}

        \begin{corollary}
            Let $\mathcal{H}$ be a hypothesis set with $\text{VCdim}(\mathcal{H}) = d$. Then for all $n \geq d$,
            \begin{equation}
                \Pi_{\mathcal{H}}(n) \leq \left(\frac{e n}{d} \right)^d = \mathcal{O}(n^d)
            \end{equation}
        \end{corollary}

        \begin{proof}
            The proof begins by using Sauer's lemma. The first inequality multiplies each summand by a factor that is greater than or equal to one since $n \geq d$, while the second inequality adds non-negative summands to the summation.
            \begin{equation*}
                \begin{aligned}
                    \Pi_{\mathcal{H}}(n) &\leq \sum_{i=0}^{d} \binom{n}{i} \\
                    &\leq \sum_{i=0}^{d} \binom{n}{i} \left(\frac{n}{d}\right)^{d - i} \\
                    &\leq \sum_{i=0}^{n} \binom{n}{i} \left(\frac{n}{d}\right)^{d - i} \\
                    &= \left(\frac{n}{d} \right)^{d} \sum_{i=0}^{d} \binom{n}{i} \left(\frac{d}{n} \right)^{i} \\
                    &= \left(\frac{n}{d} \right)^{d} \left(1 + \frac{d}{n} \right)^{n} \leq \left(\frac{n}{d} \right)^d e^d
                \end{aligned}
            \end{equation*}
            After simplifying the expression using the binomial theorem, the final inequality follows using the general identity $(1 + x) \leq e^{x}$.

        \end{proof}

        The explicit relationship just formulated between VC-dimension and the growth function combined with corollary above leads immediately to the following generalization bounds based on the VC-dimension.

        \begin{theorem}[Generalization Bounds via VC-dimension]
            Let $\mathcal{H}$ be a family of functions taking values in $\{-1, +1\}$ with VC-dimension $d$. Then, for any $\delta > 0$, with probability at least $1 - \delta$, the following holds for all $h \in \mathcal{H}$:
            \begin{equation}
                \mathcal{R}(h) \leq \hat{\mathcal{R}}(h) + \sqrt{\frac{2 d \log(en / d)}{n}} + \sqrt{\frac{\log (1 / \delta)}{2 n}}
                \label{eq:generalization-bound-VC-dimension}
            \end{equation}
            \label{thm:generalization-bound-VC-dimension}
        \end{theorem}

        Thus, the form of this generalization bound is
        \begin{equation}
            \mathcal{R}(h) \leq \hat{\mathcal{R}}(h) + \mathcal{O}\left(\sqrt{\frac{\log (n / d)}{(n / d)} } \right)
        \end{equation}
        which emphasizes the importance of the ratio $m/d$ for generalization. The theorem provides another instance of Occam's razor principle where simplicity is measured in terms of smaller VC-dimension.
        
        VC-dimension bounds can be derived directly without using an intermediate Rademacher complexity bound, as shown in (\ref{eq:direct-generalization-bound-growth-function}). Combining Sauer's lemma with (\ref{eq:direct-generalization-bound-growth-function}) leads to the following high-probability bound
        \begin{equation}
            \mathcal{R}(h) \leq \hat{\mathcal{R}}(h) + \sqrt{\frac{8 d \log(2em / d) + 8 \log(4 / \delta) }{n} }
        \end{equation}
        which has the general form we derive above. The log factor plays only a minor role in these bounds. A finer analysis can be used in fact to eliminate that factor.
    
    % subsection link_growth_function_and_vc_dimension (end)

    \subsection{Lower Bounds} % (fold)
    \label{sub:lower_bounds}
    
        See the details in \cite{mohri2018foundations}.

    % subsection lower_bounds (end)

% section growth_function_and_vc_dimension (end)


\section{Covering Number and Chaining} % (fold)
\label{sec:covering_number_and_chaining}

    In the case of (binary) classification, we established that only a finite number of elements in the hypothesis class $\mathcal{F}$ really matter, as far as establishing a notion of complexity of $\mathcal{F}$ that can be used to bound uniform deviations in expectation ($\hat{\mathcal{R}}(f) - \mathcal{R}(f)$): only the classifiers yielding different labelings matter. We did so using combinatorial arguments, leading to the notion of complexity given by the growth function, which measures the maximal size of $\mathcal{F}$ when restricted to a given number of points. This quantity, in turn, can be upper-bounded in terms of the VC dimension.

    We will now apply the same idea in the setting of regression, where we consider real-valued predictors. We will isolate a few (finitely many) predictors of interest, bound the Rademacher complexity of the set of restrictions to samples in terms of the Rademacher complexity of these representative predictors, and control the error that we commit by only considering a subset of $\mathcal{F}$

    Our goal is to find a finite set that explains "most of" the deviation in expectation, up to a certain precision parameter $\varepsilon$. To do so, we will use metric arguments and the notion of covering numbers. This analysis, in fact, will yield improvements also in the setting of binary classification, allowing remove the term $\log (en / d)$ in Eq.(\ref{eq:generalization-bound-VC-dimension}).

    \subsection{Covering and Packing} % (fold)
    \label{sub:covering_and_packing}
    
        We begin by defining the notions of packing and covering a set in a metric space. Recall that a metric space $(T, \rho)$ consists of a non-empty set $T$, equipped with a mapping $\rho: T \times T \mapsto \mathbb{R}$ that satisfies the following properties:
        \begin{enumerate}[(a)]
            \item It is non-negative: $\rho(\theta, \theta')$ for all paris $(\theta, \theta')$, with equality if and only if $\theta = \theta'$

            \item It is symmetric: $\rho(\theta, \theta') = \rho(\theta', \theta)$ for all pairs $(\theta', \theta)$

            \item The triangle inequality holds: $\rho(\theta, \theta') \leq \rho(\theta, \tilde{\theta}) + \rho(\theta', \tilde{\theta})$
        \end{enumerate}
        Familiar examples of metric spaces include the real space $\mathbb{R}^d$ wiht the \textit{Euclidean metric}
        $$ \rho(\theta, \theta') = ||\theta - \theta'||_2 := \sqrt{\sum_{j=1}^{d} (\theta_j - \theta_j')^2 } $$
        and the discrete cube $\{0, 1\}^d$ with the \textit{rescaled Hamming metric}
        $$ \rho_{H}(\theta, \theta') := \frac{1}{d} \sum_{j=1}^{d} \mathbbm{1}\{\theta_j \neq \theta_j' \} $$
        Also of interest are various metric spaces of functions, among them usual spaces $L^2(\mu, [0, 1])$ with its metric
        $$ ||f - g||_2 := \left[\int_{0}^{1} (f(x) - g(x))^2 d \mu(x) \right]^{1/2} $$
        as well as the space $C[0, 1]$ of all continuous functions on $[0, 1]$ equipped with the sup-norm metric
        $$ ||f - g||_{\infty} = \sup_{x \in [0, 1]}|f(x) - g(x)| $$

        Given a metric spaces $(T, \rho)$, a natural way in which to measure its size is in terms of number of balls of a fixed radius $\varepsilon$ required to cover it, a quantity known as the covering number.

        \begin{definition}
            A $\varepsilon$-cover of a set $T$ with respect to a metric $d$ is a set $\{\theta_1, \dots, \theta_n \} \subset T$ such that for each $\theta \in T$, there exists some $i \in \{1, \dots, n\}$ such that $\rho(\theta, \theta_i) \leq \varepsilon$. The $\varepsilon$-covering number $N(\varepsilon; T, \rho)$ is the cardinality of the smallest $\varepsilon$-cover.
        \end{definition}

        It is easy to see that covering number is decreasing in $\varepsilon$, namely, $N(\varepsilon) \geq N(\varepsilon')$ for all $\varepsilon \leq \varepsilon'$. Typically, the covering number diverges as $\varepsilon \rightarrow 0^{+}$, and of interest to us is this growth rate on a logarithmic scale. More specifically, the quantity $$ \log N(\varepsilon; T, \rho) $$ is known as the \textit{metric entroppy} of the set $T$ with respect to $\rho$. Here are some examples which show how covering number can be bounded.

        \begin{figure}[ht]
            \centering
            \includegraphics[width=.8\linewidth]{Pics/covering-packing.png}
            \caption{Covering and packing sets}
            \label{fig:covering-packing}
        \end{figure}

        \begin{itemize}
            \item \textbf{Covering numbers of unit cubes}. Consider the interval $[-1, 1]$ in $\mathbb{R}$, equiped with the metric $\rho(\theta, \theta') = |\theta - \theta'|$. Suppose that we divide the interval $[-1, 1]$ into $L := \lfloor 1/\varepsilon \rfloor + 1$ sub intervals, centered at the points $\theta_i = -1 + 2 (i - 1) \varepsilon$ for $i \in [L] := \{1, 2, \dots, L\}$, and each of length at most $2 \varepsilon$. By construction, for any point $\theta' \in [0, 1]$, there is some $j \in [L]$ such that $|\theta_j - \theta'| \leq \varepsilon$, which shows that
                $$ N(\varepsilon; [-1, 1], |\cdot|) \leq \frac{1}{\varepsilon} + 1 $$ 
                We can easily generalize this analysis for the $d$-dimensional cube $[-1, 1]^d$, we have
            $$ N(\varepsilon; [-1, 1]^d, ||\cdot||_{\infty}) \leq \left(1 + \frac{1}{\varepsilon} \right)^d $$

            \item \textbf{Covering the binary hypercube}. Consider the binary hypercube $\mathcal{H} = \{0, 1\}^d$ equiped with the rescaled Hamming metrics. 
                \begin{itemize}
                    \item First, let us upper bound its $\varepsilon$-covering number. Let $S = \{1, 2, \dots, \lceil(1 - \varepsilon)d \rceil \}$. Consider the set of binary vectors 
                        $$ T(\varepsilon) := \{\theta \in H \mid \theta_j = 0, \ \forall \ j \in S \} $$
                    By construction, for any binary vector $\theta' \in H$, we can find a vector $\theta \in T(\varepsilon)$ such that $\rho_{H}(\theta, \theta') \leq \varepsilon$. Namely, we can match $\theta'$ exactly on all entries $j \in S$, and, in the worst case, disagree on all the remaining $\lfloor \varepsilon d \rfloor$ positions. Since $T(\varepsilon)$ contains $2^{\lceil(1 - \varepsilon) d \rceil}$ vectors, we can conclude that
                        $$ \frac{\log N_{H}(\varepsilon; \mathcal{H}^d) }{\log 2} \leq \lceil (1 - \varepsilon)d \rceil $$

                    \item Now let us lower bound its $\varepsilon$-covering number, where $\varepsilon \in (0, 1/2)$. If $\{\theta_1, \dots, \theta_n \}$ is a $\varepsilon$-covering, then the (unrescaled) Hamming balls of radius $\varepsilon d$ around each $\theta_l$ must contain all $2^d$ vectors in the binary hypercube. 

                    Let $s = \lfloor \varepsilon d \rfloor$, then for each $\theta_l$, there are exactly $\sum_{j=0}^{s} \binom{d}{j} $ binary vectors lying within distance $\varepsilon d$ from it, and hence we must have 
                    $$ n \cdot \sum_{j=0}^{s} \binom{d}{j} \geq 2^{d}$$
                    where $n$ is the cardinality of delta-covering set. Noq let $X_i \in \{0, 1\}$ be i.i.d. Bernoulli variables with parameter $1/2$. Rearranging the previous inequality, we have
                    $$ \frac{1}{n} \leq \sum_{j=0}^{s} \binom{d}{j} 2^{-d} = \mathbb{P}\left(\sum_{i=1}^{d} X_i \leq \varepsilon d \right) \leq e^{- 2d (1/2 - \varepsilon)^2 } $$
                    where the last inequality follows by applying Hoeffding's bound to the sum of $d$ i.i.d. Bernoulli variables. Following some algebra, we obtain the lower bound
                    $$ \log N_{H}(\varepsilon; \mathcal{H}^d) \geq 2 d \left(\frac{1}{2} - \varepsilon \right)^2 $$
                    valid for $\varepsilon \in (0, 1/2)$. This lower bound is qualitatively correct, but can be tightened by using a better upper bound on the binomial tail probability.
                \end{itemize}

        \end{itemize}

        \begin{definition}[Packing Number]
            A $\varepsilon$-packing number of a set $T$ with respect to a metric $\rho$ is a set $\{\theta_1, \dots, \theta_m \} \subset T $ such that $\rho(\theta_i, \theta_j) > \varepsilon$ for all distinct $i, j \in \{1, \dots, m \}$. The $\varepsilon$-packing number $M(\varepsilon; T, \rho)$ is the cardinality of the largest $\varepsilon$-packing.
        \end{definition}

        \begin{lemma}
            For all $\varepsilon > 0$, the packing and covering numbers are related as follows:
            \begin{equation}
                M(2\varepsilon; T, \rho) \leq N(\varepsilon; T, \rho) \leq M(\varepsilon; T, \rho)
            \end{equation}
        \end{lemma}

    % subsection covering_and_packing (end)

    \subsection{Bound Rademacher Complexity via Covering Number} % (fold)
    \label{sub:bound_rademacher_complexity_via_covering_number}
    
        Recall that in Section \ref{sec:rademacher_complexity} where we introduce the Rademacher compelxity,  we define a new variable $Z$ over domain $\mathcal{Z} = \mathcal{X} \times \mathcal{Y}$, and a function $h$ from hypothesis space $\mathcal{H} = \{h: (Y, X) \mapsto l(Y, f(X) ) \mid f \in \mathcal{F} \}$, where $l: \mathcal{X} \times \mathcal{Y} \mapsto \mathbb{R}$ is some loss function.

        Given a sample set $S_n = \{Z_1, \dots, Z_n \} \in \mathcal{Z}^n$, define the following pseudo-norms on the space $\mathcal{H}$: for any $h \in \mathcal{H}$,
        \begin{equation}
            \begin{aligned}
                & ||h||_{p} := \left(\frac{1}{n} \sum_{i=1}^{n}|h(X_i)|^p \right)^{1/p}, \quad p \geq 1 \\
                & ||h||_{\infty} := \max_{i} |h(X_i)| \\
            \end{aligned} 
        \end{equation}
        Note that the reason why we call it pseudo-norm is the space $\mathcal{H}$ is not required to be finite.
        Now, let $\rho_p(\theta, \theta') := ||\theta - \theta'||_p$ be the pseudo-metric that is induced by the pseudonorms $ ||\cdot||_p $ on function space $\mathcal{H}$. Then,  we can similarily define the covering number $N(\delta; \mathcal{H}, \rho)$ and packing numbers $M(\delta; \mathcal{H}, \rho)$ on pseudo-metric space $(\mathcal{H}, \rho)$.

        \begin{lemma}[Monotonicity]
            For any $S_n = \{Z_1, \dots, Z_n\} \in \mathcal{Z}^n$, $1 \leq p \leq q$ and $\delta > 0$, we have
            \begin{equation}
                N(\delta; \mathcal{H}, \rho_p) \leq N(\delta; \mathcal{H}, \rho_q)
            \end{equation}
            \begin{equation}
                M(\delta; \mathcal{H}, \rho_p) \leq M(\delta; \mathcal{H}, \rho_q)
            \end{equation}
        \end{lemma}

        \begin{proof}
            That is because $p$-norm is decreasing in $p$ when $p \geq 1$. Consider two norms $||\cdot||_p$ and $||\cdot||_q$ of vector $x$ where $1 \leq p \leq q$. 
            \begin{itemize}
                \item If $x = 0$, then $||x||_p \geq ||x||_q$ trivially holds;

                \item If $x > 0$, then let $y$ be a vector such that $y_k = ||x_k|| / ||x||_q \leq 1$, which means that $y_k^p \geq y_k^q$. Notice that $\sum_k y_k^q = 1$, we have
                    $$ ||y||_p \geq 1 $$
            \end{itemize}
            Hence we have shown that $p$-norm is decreasing in $p$, and therefore the covering number with respect to the metric $\rho_p$ induced by $p$-norm is less than the covering number with respect to metric $\rho_q$ induced by $q$-norm.
            
        \end{proof}

        We next show that the covering numbers of the pseudo-metric space $(\mathcal{F}, \rho_1)$ can be used to bound the empirical Rademacher complexity.

        \begin{theorem}[Bounding Rademacher Complexity via Covering Number]
            For any fixed sample set $S_n = \{Z_1, \dots, Z_n \} \in \mathcal{Z}^n$ with size $n$, let $\sup_{h \in \mathcal{H}} ||h(Z)||_2 \leq c(Z) $, then the empirical Rademacher complexity is bounded by
            \begin{equation}
                \hat{R}_{S_n}(\mathcal{H}) \leq \inf_{\varepsilon > 0} \left\{\varepsilon + c(Z) \sqrt{\frac{2 \log N(\varepsilon; \mathcal{H}, \rho_1)}{n} } \right\}
            \end{equation}
            recall that $\rho_1$ is a pseudo-metirc induced by pseudo-norm $||\cdot||_1$, and note that the hypothesis space $\mathcal{H}$ is not required to be finite.
            \label{thm:covering-bound-Rademacher-complexity}
        \end{theorem}

        \begin{proof}
            For a fixed sample $S_n = \{Z_1, \dots, Z_n \}$ drawn from a unknown joint probability $P$ and $\varepsilon > 0$, we denote by $\mathcal{H}_{S_n}$ the set of vectors of function $H = (h(Z_1), \dots, h(Z_n))^{\top}$ where $h \in \mathcal{H}$. Since we know the $l_2$-norm of function $h$ with respect to $Z$ is bounded by $c(Z)$, i.e. $\sup_{h \in \mathcal{H}} ||h(Z)||_2 \leq c(Z)$, then the norm of these vectors is bounded by $\sqrt{n} c(Z)$, namely
            \begin{equation*}
                \sup_{H \in \mathcal{H}_{S_n}} ||H||_2 = \sup_{h \in \mathcal{H}} \sqrt{\sum_{i=1}^{n} h(Z_i)^2 } = \sup_{h \in \mathcal{H}} \sqrt{n} \cdot ||h||_2 \leq \sqrt{n} \cdot c(Z)
            \end{equation*}
            Now, Let $\mathcal{C} \subset \mathcal{H}$ be a minimal $\varepsilon$-cover of space $(\mathcal{H}; \rho_1)$, and for any $h \in \mathcal{H}$, let $h_0 \in \mathcal{C}$ such that $||h - h_0||_{1} \leq \varepsilon$. Apply the Massart's lemma, we have
            \begin{equation}
                \begin{aligned}
                    \hat{R}_{S_n}(\mathcal{H}) &= \mathbb{E}_{\sigma}\left[\sup_{h \in \mathcal{H}} \frac{1}{n} \sum_{i=1}^{n} \sigma_i h(Z_i) \right] \\
                    &\leq \mathbb{E}_{\sigma} \left[\sup_{h \in \mathcal{H}} \frac{1}{n} \sum_{i=1}^{n} \sigma_i (h(Z_i) - h_0(Z_i) ) \right] + \mathbb{E}_{\sigma} \left[\sup_{h \in \mathcal{H}} \frac{1}{n} \sum_{i=1}^{n} \sigma_i h_0(Z_i) \right] \\
                    &\leq \varepsilon + \mathbb{E}_{\sigma} \left[\sup_{h \in \mathcal{C}} \frac{1}{n} \sum_{i=1}^{n} \sigma_i h(Z_i) \right] \\
                    &\leq \varepsilon + \sup_{h \in \mathcal{C}} \sqrt{\sum_{i=1}^{n} h(Z_i)^2} \cdot \frac{\sqrt{2 \log |\mathcal{C}|} }{n} \quad (\text{Massart's lemma in Thm. \ref{thm:Massart-lemma}}) \\ 
                    &\leq \varepsilon + c(Z) \sqrt{\frac{2 \log N(\varepsilon; \mathcal{H}, \rho_1) }{n} } \\
                \end{aligned}
            \end{equation}
            It is crucial to notice that Massart's lemma only holds on the finite set and here the $\varepsilon$-cover $\mathcal{C}$ meets the requirement. The final result follows by taking the infimum over $\varepsilon > 0$.

        \end{proof}

        The bound in Theorem \ref{thm:covering-bound-Rademacher-complexity} establishes a tradeoff with respect to the precision parameter $\varepsilon$, as the decrease of $\varepsilon$ woud lead to the increase of covering number $N(\varepsilon; \mathcal{H}, \rho_1)$. In addition, this bound is sample-dependent, namely a random variable, as the right-hand-side depends on $S_n \in \mathcal{Z}^n$

    % subsection bound_rademacher_complexity_via_covering_number (end)

    \subsection{Chaining} % (fold)
    \label{sub:chaining}

        Theorem \ref{thm:covering-bound-Rademacher-complexity} is established by using one fixed level of granularity $(\varepsilon > 0)$ at a time, and taking the infimum over $\varepsilon > 0$ to obtain the final bound. An improved version of this result can be established by integrating over different levels of granularity. In this case, we need to work with covering numbers for the pseudo-metric space $(\mathcal{H}, \rho_2)$ where $\rho_2$ is induced by the pseudo-norm $||\cdot||_2$. 

        \begin{theorem}[Dudley's Entropy Integral Bound]
            For any fixed sample set $S_n = (Z_1, \dots, Z_n) \in \mathcal{Z}^n$ and $\sup_{h \in \mathcal{H}} ||h(Z)||_2 \leq c(Z)$, we have
            \begin{equation}
                \hat{R}_{S_n}(\mathcal{H}) \leq \inf_{\varepsilon \in [0, c(Z)/2]} \left\{4 \varepsilon + \frac{12}{\sqrt{n}} \int_{\varepsilon}^{c(Z)/2} d\nu \sqrt{\log N(\nu; \mathcal{H}, \rho_2 ) } \right\}
            \end{equation}
            note that the hypothesis space $\mathcal{H}$ is not required to be finite.
        \end{theorem}

        \begin{proof}
            Fix the $n$-size sample $S_n = \{Z_1, \dots, Z_n \} \in \mathcal{Z}^n$. For each $j \in \mathbb{N}_{+}$, let 
            $$ \varepsilon_j := c(Z) / 2^j $$ 
            and let $\mathcal{C}_j \subset \mathcal{H}$ be a minimal $\varepsilon_j$-cover of pseudo-metric space $(\mathcal{H}, \rho_2)$. We then have $|\mathcal{C}_j| = N(\varepsilon_j; \mathcal{H}, \rho_2)$. For any $h \in \mathcal{H}$ and $j \in \mathbb{N}_{+}$, let $h_j \in \mathcal{C}_j$ such that $||h - h_j||_2 \leq \varepsilon_j$. The sequence $h_1, h_2, \dots$ (of elements of cover with decreasing radius) converges towards $h$. This sequence can be used to define the following telescoping sum, for a given $m \in \mathbb{N}$ to be choose later:
            \begin{equation*}
                h = h - h_m + \sum_{j=1}^{m} (h_j - h_{j-1})
            \end{equation*}
            with $h_0 := 0$. This telescoping sum can be thought of as a "chain" connecting $h_0 = 0$ to $h$. This is the reason why the technique we are going to describe is called \textit{chaining}. Upon these, we have
            \begin{equation*}
                \begin{aligned}
                    \hat{R}_{S_n}(\mathcal{H}) &= \mathbb{E}_{\sigma} \left[\sup_{h \in \mathcal{H}} \frac{1}{n} \sum_{i=1}^{n} \sigma_i h(Z_i) \right] \\
                    &\leq \mathbb{E}_{\sigma} \left[\sup_{h \in \mathcal{H}} \frac{1}{n} \sum_{i=1}^{n} \sigma_i (h(Z_i) - h_{m}(Z_i)) \right] + \mathbb{E}_{\sigma}\left[\sup_{h \in \mathcal{H}} \frac{1}{n} \sum_{i=1}^{n} \sigma_i \sum_{j=1}^{m} (h_j(Z_i) - h_{j-1}(Z_i) ) \right] \\
                \end{aligned}
            \end{equation*}
            Next, we bound the two summands separately. The first summand is bounded by $\varepsilon_m$ as
            {\color{blue}
            \begin{equation*}
                \sum_{i=1}^{n} \sigma_i (h(Z_i) - h_{m}(Z_i)) \leq \sum_{i=1}^{n} |h(Z_i) - h_m(Z_i) | = {\color{blue} n \cdot ||h - h_m||_1 \leq n \cdot ||h - h_m||_2} \leq n \cdot \varepsilon_m
            \end{equation*}
            }
            Since there are at most $|\mathcal{C}_j| \cdot |\mathcal{C}_{j-1}|$ different ways to create a vector in $\mathbb{R}^n$ of the form
            \begin{equation*}
                \begin{bmatrix}
                    h_j(Z_1) - h_{j-1}(Z_1) \\
                    \vdots \\
                    h_j(Z_n) - h_{j-1}(Z_n) \\
                \end{bmatrix}
            \end{equation*}
            with $h_j \in \mathcal{C}_j$ and $h_{j-1} \in \mathcal{C}_{j-1}$, using Massart's lemma in Theorem \ref{thm:Massart-lemma} and let $\mathcal{C} = \bigcup_{j=1}^{m}$ be the union of all covers, the second summand can be upper bounded by
            \begin{equation*}
                \begin{aligned}
                    \sum_{j=1}^{m} \mathbb{E}\left[\sup_{h \in \mathcal{H}} \frac{1}{n} \sum_{i = 1}^{n} \sum_{i=1}^{n} \sigma_i (h_j(Z_i) - h_{j-1}(Z_i)) \right] &= 
                    \sum_{j=1}^{m} \mathbb{E}\left[\sup_{h \in \mathcal{C}} \frac{1}{n} \sum_{i = 1}^{n} \sum_{i=1}^{n} \sigma_i (h_j(Z_i) - h_{j-1}(Z_i)) \right] \\
                    &\leq \sum_{j=1}^{m} \sup_{h \in \mathcal{C}} \sqrt{\sum_{i=1}^{n} (h_j(Z_i) - h_{j-1}(Z_i))^2 } \cdot \frac{\sqrt{2 \log |\mathcal{C}_j| \cdot |\mathcal{C}_{j-1}| } }{n} \\
                    &= \sum_{j=1}^{m} \sup_{h \in \mathcal{C}} ||h_j - h_{j-1}||_2 \cdot \sqrt{\frac{2 \log |\mathcal{C}_j| \cdot |\mathcal{C}_{j-1}| }{n} }
                \end{aligned}
            \end{equation*}
            Here we can see that the $||\cdot||_2$ norm naturally appears in the application of Massart's lemma. With the triangular inequality for the pseudo-norm $||\cdot||_2$, we have (using that $\varepsilon_{k - 1} = 2 \varepsilon_{k} $)
            \begin{equation*}
                \begin{aligned}
                    ||h_j - h_{j-1}||_2 &\leq ||h_j - h||_2 + ||h - h_{j-1}||_2 \\ 
                    &\leq \varepsilon_j + \varepsilon_{j-1} = 3 \varepsilon_j = 6 (\varepsilon_{j} - \varepsilon_{j+1})    
                \end{aligned}
            \end{equation*}
            Also, $|\mathcal{C}_j| = N(\varepsilon_j; \mathcal{H}, \rho_1)$ and $|\mathcal{C}_{j-1}| \leq |\mathcal{C}_j|$. Putting things together, we have
            \begin{equation*}
                \begin{aligned}
                    \hat{R}_{S_n}(\mathcal{H}) &\leq \varepsilon_m + 12 \sum_{j=1}^{m} (\varepsilon_j - \varepsilon_{j+1}) \sqrt{\frac{\log N(\varepsilon_j; \mathcal{H}, \rho_1)}{n} } \\
                    &\leq 2 \varepsilon_{m+1} + 12 \int_{\varepsilon_{m+1}}^{c(Z)/2} d \nu \sqrt{\log N(\delta; \mathcal{H}, \rho_1) }
                \end{aligned}
            \end{equation*}
            where the last inequality follows as the integral is lower-bound by its lower Riemann sum as the function $\nu \mapsto N(\nu; \mathcal{H}, \rho_1 )$ is decreasing. For any $\varepsilon \in [0, c(Z)]/2$, choose $m$ such that $\varepsilon < \varepsilon_{m+1} \leq 2 \varepsilon$. The statement of the theorem thus follows by taking the infimum over $\varepsilon \in [0, c(Z)/2]$.

        \end{proof}
    
    % subsection chaining (end)

% section covering_number_and_chaining (end)


\section{Optimization Algorithm} % (fold)
\label{sec:optimization_algorithm}

    \begin{table}[h]
        \centering
        \begin{tabularx}{.8\linewidth}{lXX}
            \toprule
                        & convex                            & strongly convex \\
            \midrule
            nonsmooth   & deterministic: $BD / \sqrt{t}$    & deterministic: $B^2 / (t \mu) $ \\
                        & stochastic: $BD / \sqrt{t}$       & stochastic: $B^2 / (t \mu) $  \\
            smooth      & deterministic: $LD^2 / t$         & deterministic: $\exp(- t \sqrt{\mu / L})$ \\
                        & stochasic: $LD^2 / \sqrt{t}$      & stochastic: $L / (t \mu)$ \\
                        & finite sum: $n/t$                 & finite sum: $\exp(- t \min\{1/n, \mu/L\})$ \\
            \bottomrule
        \end{tabularx}
        \label{tab:optimization-algo-rate}
    \end{table}

    \subsection{Optimization in Machine Learning} % (fold)
    \label{sub:optimization_in_machine_learning}

        In supervised machine learning, we are given $n$ i.i.d. samples $(X_i, Y_i)$, $i = 1, \dots, n$ of a couple of random variables $(X, Y)$ on $\mathcal{X} \times \mathcal{Y}$ and the goal is to find a predictor $f: \mathcal{X} \mapsto \mathbb{R}$ with a small risk
        $$ \mathcal{R}(f) := \mathbb{E}[l(Y, f(X))] $$
        where $l: \mathbb{Y} \times \mathcal{Y} \mapsto \mathbb{R}$ is a loss function. This loss is typlically chosen to be convex, whih is thus considered as a weak assumption. 
    
    % subsection optimization_in_machine_learning (end)

% section optimization_algorithm (end)


\section{Kernel Methods} % (fold)
\label{sec:kernel_methods}

    In this section, we study empirical risk minimization for linear models, that is, prediction functions $f_{\theta}: \mathcal{X} \mapsto \mathbb{R}$ which are linear in parameters $\theta$, that is, of the form $f_{\theta} = \langle \theta, \varphi(x) \rangle_{\mathcal{H}} $ and $\mathcal{H}$ is a Hilbert space and $\theta \in \mathcal{H}$.

    \vspace{1em}
    The key difference with least-squares estimation is that, (1) we are not restricted to the square loss, and (2) we explicitly allow infinite-dimensional models, thus extending the dimenison-free bounds from linear least-squares regression (see Chapter 3 in \cite{bach2021learning})

    \vspace{1em}
    \noindent
    \textbf{Why kernel methods}. The study of infinite-dimensional linear models is important for several reasons:
    \begin{itemize}
        \item Understanding linear models in finite but very lage input dimension dimensions requries tools from infinite-dimensional analysis

        \item Kernel methods lead to simple and stable algorithms, with theoretical guarantess, and adaptivity to smoothness of the target function (as oppsed to local averaging techniques). They can be applied in high dimensions, with good practical performance (not state of arts techinique for CV and NLP compared to NN)

        \item They can be easily applied when input observations are not vectors

        \item They are useful to understand other models such as neural networks
    \end{itemize}

    \subsection{Motivating Example to Kernel Function} % (fold)
    \label{sub:motivating_example_to_kernel_function}
    
        Before we go deep into the area of kenrel methods, let's see some examples in classification.
        \begin{figure}[h]
            \centering
            \includegraphics[width=.8\linewidth]{Pics/XOR-classification.png}
            \caption{XOR classification problem}
            \label{fig:XOR}
        \end{figure}
    
        \begin{example}[XOR Classification]
            Given two dimensional input features $X = (X_1, X_2)^{\top}$, we have label $Y = + 1$ if and only if $X_1 = X_2$ and $Y = -1$ if and only if $X_1 \neq X_2$. We use red node to denote the case of $Y = +1$ and blue node the case $Y = -1$.
            As we can see from Figure \ref{fig:XOR}, the XOR problem is linearly non-separable in the input space.
            However, if we define a feature mapping $\varphi: \mathbb{R}^2 \mapsto \mathbb{R}^6$ such that
            $$ (X_1, X_2)^{\top} \mapsto (X_1^2, X_2^2, \sqrt{2} X_1 X_2, \sqrt{2c} X_1, \sqrt{2c} X_2, c)^{\top} $$
            then the XOR problem is linear spearable in $\mathbb{R}^{6}$ space.
        \end{example}

        \begin{example}[Support Vector Machine]
            Consider a two dimensional feature space contains $m$ data points with binary labels as shown in Figure \ref{fig:SVM}. SVM aims at choosing a separating hyperplane with the maximum geometric margin, which can be expressed as $\rho = \max_{w, b} \min_{i \in [m]} \frac{y_i(w^{\top} x_i + b)}{||w||_2} $. The equivalent optimizaiton problem can be formalized as
            \begin{equation*}
                \begin{aligned}
                    \minimize_{w, b} \quad & \frac{1}{2} ||w||_2^2 \\
                    \text{subject to} \quad & y_i(w_i^{\top}x_i + b) \geq 1, \ \forall i \in [m] \\
                \end{aligned}
            \end{equation*}
            The corresponding KKT conditions include
            \begin{equation*}
                \begin{aligned}
                    & w_i = \sum_{i=1}^{m} \alpha_i y_i x_i, \quad \sum_{i=1}^{m} \alpha_i y_i = 0 \\
                    & \alpha_i [y_i(w^{\top} x_i + b) - 1] = 0, \quad \forall \ i \in [m] \\ 
                \end{aligned}
            \end{equation*}
            and the dual problem
            \begin{equation*}
                \begin{aligned}
                    \maximize_{\alpha} \quad & \sum_{i=1}^{m} \alpha_i = \frac{1}{2} \sum_{i,j=1}^{m} \alpha_i \alpha_j y_i y_j \langle x_i, x_j \rangle \\
                    \text{subject to} \quad & \sum_{i=1}^{m} \alpha_i y_i = 0, \quad \alpha_i \geq 0, \quad \forall \ i \in [m] \\
                \end{aligned}
            \end{equation*}
            Once we solve for the $\alpha_i, \ i \in [m]$, we obtain the linear classifier $f(x) = w^{\top} x = \sum_{i=1}^{m} \alpha_i y_i \langle x_i, x \rangle$. We can see from this result that the hypothesis solution only depends on inner products between vectors.
        \end{example}

        \begin{figure}[h]
            \centering
            \includegraphics[width=.8\linewidth]{Pics/SVM-hyperplane.png}
            \caption{Separating hyperplane}
            \label{fig:SVM}
        \end{figure}

        \begin{example}[Inner Product]
            Here we illustrate an example of calculating the inner product in feature space $\mathcal{Z}$ instead of originial space $\mathcal{X}$.
            \begin{equation}
                \begin{aligned}
                    \langle (z_1, z_2, z_3), (z_1', z_2', z_3') \rangle &= \langle \varphi(x_1, x_2), \varphi(x_1', x_2') \rangle \\
                    &= \langle (x_1^2, \sqrt{2} x_1 x_2, x_2^2), (x_1'^2, \sqrt{2}x_1' x_2', x_2'^2 ) \rangle \\
                    &= (x_1 x_1', x_2 x_2')^2 = (\langle x, x' \rangle )^2 \\
                    &= K(x, x')
                \end{aligned}
            \end{equation}
        \end{example}

        \begin{figure}[h]
            \centering
            \includegraphics[width=.6\linewidth]{Pics/SVM-feature-space.jpg}
            \caption{Inner product in feature space}
            \label{fig:SVM-feature-space}
        \end{figure}

        From the example of SVM, we notice that determining a nonlinear prediction function requires multiple inner product computations in high-dimensional (feature) spaces, which can become very costly. A solution to this problem is to use kernel methods, which are based on kernels or kernel functions. We briefly introduce the concept here and will concretely discuss it in the section of reproducing kernel Hilbert space.

        \begin{definition}
            A function $K: \mathcal{X} \times \mathcal{X} \mapsto \mathbb{R}$ is called a kernel over $\mathcal{X}$.
        \end{definition}

        The idea behind is to define a kernel $K$ such that for any two points $x, x' \in \mathcal{X}$, $K(x, x')$ be equal to an inner product of vectors $\varphi(x)$ and $\varphi(x')$:
            $$ K(x, x') = \langle \varphi(x), \varphi(x') \rangle \quad \forall \ x, x' \in \mathcal{X} $$
        for some features mapping $\varphi: \mathcal{X} \mapsto \mathcal{H}$, where Hilbert space $\mathcal{H}$ is called a feature space. Since an inner product is a measure of the similarity of two vectors, $K$ is often interpreted as a similarity measure between elements of the input space $\mathcal{X}$.
        \begin{itemize}
            \item Distance in feature space $\mathcal{H}$: 
                $$ ||\varphi(x) - \varphi(x')||_2^2 = \langle \varphi(x), \varphi(x') \rangle - 2 \langle \varphi(x), \varphi(x') \rangle + \langle \varphi(x'), \varphi(x') \rangle = K(x, x) - 2 K(x, x') + K(x', x') $$

            \item Angle in feature space $\mathcal{H}$: 
                $$ \cos \theta = \frac{\langle \varphi(x), \varphi(x') \rangle}{||\varphi(x)||_2 \cdot ||\varphi(x')||_2} = \frac{K(x, x')}{\sqrt{K(x, x) \cdot K(x', x') }} $$
        \end{itemize}

        The associated kernel matrix $\mathbf{K}$ is then a matrix of dot-products (Gram matrix), and is thus positive semi-definite, that is, all of its eigenvalues are non-negative, or $\forall \ \alpha^{\top} \mathbf{K} \alpha \geq 0$. If $\mathcal{H} = \mathbb{R}^d$ and $\Phi \in \mathbb{R}^{n \times d}$ is the matrix of features (or desgin matrix) with $i$-th row composed of $\varphi(x_i)$, then $\mathbf{K} = \Phi \Phi^{\top} \in \mathbb{R}^{n \times n}$ is the kernel matrix, while $\frac{1}{n} \Phi^{\top} \Phi \in \mathbb{R}^{d \times d} $ is the empirical covariance matrix.

        \begin{remark}
            An important advantage of such a kernel $K$ is efficiency: $K$ is often more efficient to compute than $\varphi$ and an inner product in $\mathcal{H}$. We will see serveal common examples where the computation of $K(X, X')$ can be achieved in $\mathcal{O}(N)$ while that $\langle \varphi(X), \varphi(X') \rangle$ typically requries $\mathcal{O}(d)$ where $d$ is the dimension of space $\mathcal{H}$, and $d \gg N$.

            Perhaps an even more crucial benefit of such a kernel function $K$ is flexibility: there is no need to explicitly define or compute a mapping $\varphi$ especially when the feature mapping is not always easty to find. The kenerl $K$ can be arbitrarily chosen so long as the existence of $\Phi$ is guaranteed, namely, $K$ satisfies Mercer's condition (we will see in next section).
        \end{remark}

    % subsection motivating_example_to_kernel_function (end)

    \subsection{Reproducing Kernel Hilbert Space} % (fold)
    \label{sub:reproducing_kernel_hilbert_space}

        Many problems in statistics -- among them interpolation, regression and density estimation, as well as nonparametric forms of dimension reduction and testing -- involve optimizing over function spaces. Hilbert spaces include a reasonably broad class of functions, and enjoy a geometric structure similar to ordinary Euclidean space. A particular class of function-based Hilbert spaces are those defined by reproducing kernels, and these spaces -- known as reproducing kernel Hilbert spaces (RKHSs) -- have attractive properties from both the computational and statistical points of view.

        \subsubsection{Hilbert Space} % (fold)
        \label{ssub:hilbert_space}
        
            \begin{definition}[Inner Product]
                An inner product on a vector space $V$ is a mapping $\langle \cdot, \cdot \rangle_{V}: V \times V \mapsto R$ such that
                \begin{equation}
                    \begin{aligned}
                        & \langle f, g \rangle_{V} = \langle g, f \rangle_V \quad &\forall \ f, g \in V \\
                        & \langle f, g \rangle_{V} \geq 0, \text{with equality iff } f = 0 \quad &\forall \ f \in V \\
                        & \langle f + \alpha g, h \rangle_V = \langle f, h \rangle_V + \alpha \langle g, h \rangle_V \quad & \forall \ f, g, h \in V, \alpha \in \mathbb{R} \\
                    \end{aligned}
                \end{equation}
            \end{definition}

            A vector space equipped with an inner product is known as an inner product space. Note that any inner product induces a norm via $||f||_V := \sqrt{\langle f, f \rangle_V} $. Given this norm, we can then define the usual notion of Cauchy sequence -- that is, a sequence $(f_n)_{n=1}^{\infty}$ with elements in $V$ is Cauchy if, for any $\epsilon > 0$, there exists some integer $\epsilon > 0$, such that there exists some integer $N(\epsilon)$ such that
                $$ ||f_n - f_m||_V < \epsilon \quad \forall \ n, m \geq N(\epsilon) $$

            \begin{definition}[Hilbert Space]
                A Hilbert space $\mathcal{H}$ is an inner product space $(\langle \cdot, \cdot \rangle_{\mathcal{H}}, \mathcal{H})$ in which every Cauchy sequence $(f_n)_{n=1}^{\infty}$ in $\mathcal{H}$ converges to some element $f^* \in \mathcal{H}$.
            \end{definition}

            A metric space in which every Cauchy sequence $(f_n)_{n=1}^{\infty}$ converges to an element $f^*$ of the space is known as complete. Thus, we can summarize by saying that a Hilbert space is a complete inner product space.

            \begin{example}[Sequence space $l^2(\mathbb{N})$]
                Consider the space of square-summable real-valued sequences, namely
                    $$ l^{2}(\mathbb{N}) := \left\{(\theta_j)_{j=1}^{\infty} \mid \sum_{j=1}^{\infty} \theta_j^2 < \infty \right\} $$
                This set, when endowed with the usual inner product $\langle \theta, \gamma \rangle_{l^2(\mathbb{N})}$, defines a classical Hilbert space. It plays an especially important role in our discussion of eigenfunctions for reproducing kernel Hilbert spaces. Note that the Hilbert space $\mathbb{R}^{m}$, equipped with the usual Euclidean inner product, can be obtained as a finite-dimensional subspace of $l^2(\mathbb{N})$: in particular, the space $\mathbb{R}^m$ is isomorphic to the "slice"
                    $$ \left\{\theta \in l^2(\mathbb{N}) \mid \theta_j = 0, \ \forall j \geq m + 1 \right\} $$
            \end{example}

            \begin{example}[$L^2$ Space]
                Any element of the space $L^2[0, 1]$ is a function $f:[0, 1] \mapsto \mathbb{R}$ that is Lebesgue-integrable, and whose square satisfies the bound 
                    $$ ||f||_{L^2[0, 1]}^2 = \int_{0}^{1} f^2(x) dx < \infty $$
                Since this norm does not distinguish between functions that differ only on a set of zero Lebesgue measure, we are implicitly identifying all such functions. The space $L^2[0, 1]$ is a Hilbert space when equipped with the inner product 
                    $$ \langle f, g \rangle_{L^2[0, 1]} = \int_{0}^{1} f(x) g(x) dx $$
                In a certain sense, the space $L^2[0, 1]$ is equivalent to the sequence space $l^2(\mathbb{N})$. In particular, let $(\phi_j)_{j=1}^{\infty}$ be any complete orthonormal basis of $L^2[0, 1]$. By definition, the basis functions satisfy $||\phi_j||_{L^2[0,1]} = 1$ for all $j \in \mathbb{N}$, and $\langle \phi_i, \phi_j \rangle = 0$ for all $i \neq j$, and moreover, any function $f \in L^2[0, 1]$ has the representation $f = \sum_{j=1}^{\infty} a_j \phi_j$, where $a_j := \langle f, \phi_j \rangle$ is the $j$-th basis coefficient. By Parseval's theorem (in Fourier transform), we have 
                    $$ ||f||_{L^2[0,1]}^2 = \sum_{j=1}^{\infty} a_j^2 $$
                so that $f \in L^2[0, 1]$ if and only if the sequence $a = (a_j)_{j=1}^{\infty} \in l^2(\mathbb{N}) $. The correspondence $f \leftrightarrow (a_j)_{j=1}^{\infty}$ thus defines an isomorphism between $L^2[0,1]$ and $l^2(\mathbb{N})$.
            \end{example}

        % subsubsection hilbert_space (end)

        \subsubsection{Positive Semidefinite Kernel Functions} % (fold)
        \label{ssub:positive_semidefinite_kernel_functions}

            Let us begin with the notion of a positive semidefinite kernel function. It is a natural generalization of the idea of a positive semidefinite matrix to the setting of general functions.

            \begin{definition}[Positive Semidefinite Kernel Function]
                A symmetric bivariate function $\mathcal{K}: \mathcal{X} \times \mathcal{X} \mapsto \mathbb{R}$ is positive semidefinite (PSD) if for all integers $n \geq 1$ and elements $\{x_i\}_{i=1}^{n} \subset \mathcal{X}$, the $n \times n$ matrix with elements $\mathbf{K}_{ij} := K(x_i, x_j) $ is positive semidefinite.
            \end{definition}

            \begin{example}[Linear Kernels]
                When $\mathcal{X} = \mathbb{R}^d$, we can define the linear kernel function $K(x, x') := \langle x, x' \rangle$. It is clearly a symmetric function of its arguments. In order to verify the positive semidefiniteness, let $\{x_i\}_{i=1}^n$ be an arbitrary collection of points in $\mathbb{R}^d$, and consider the matrix $\mathbf{K} \in \mathbb{R}^{n \times n}$ with entries $K_{ij} = \langle x_i, x_j \rangle$. For any vector $\alpha \in \mathbb{R}^n$, we have
                    $$ \alpha^{\top} \mathbf{K} \alpha = \sum_{i, j = 1}^{n} \alpha_i \alpha_j \langle x_i, x_j \rangle = \left|\left|\sum_{i=1}^{n} a_i x_i \right|\right|_2^2 \geq 0 $$
                Since $n \in \mathbb{N}$, $\{x_i\}_{i=1}^{n}$ and $\alpha \in \mathbb{R}^n$ were all arbitrary, we conclude that $K$ is positive semidefinite kernel.
            \end{example}

            \begin{example}[Polynomial Kernels]
                A natural generalization of the linear kernel on $\mathbb{R}^d$ is the homogeneous polynomial kernel $K(x, z) = (\langle x, z \rangle)^m$ of degree $m \geq 2$, also defined on $\mathbb{R}^d$. Let us demonstrate the positive semidefiniteness of this function in the special case $m = 2$. Note that we have
                    $$ K(x, z) = \left(\sum_{j=1}^{d} x_j z_j \right)^2 = \sum_{j=1}^{d} x_j^2 z_j^2 + 2 \sum_{i < j} x_i x_j z_i z_j $$
                Setting $D = d + \binom{d}{2}$, let us define a mapping $\Phi: \mathbb{R}^d \mapsto \mathbb{R}^D$ with entries
                    \begin{equation}
                        \Phi(x) = 
                        \begin{bmatrix}
                            x_j^2, \quad & \text{for } j = 1, \dots, d \\
                            \sqrt{2} x_i x_j, \quad & \text{for } i < j \\
                        \end{bmatrix}
                        \label{eq:feature-map}
                    \end{equation}
                corresponding to all polynomials of degree two in $(x_1, \dots, x_d)$. With this definition, we see that $K$ can be expressed as a Gram matrix -- namely, in the form 
                    $$ K(x, z) = \langle \Phi(x), \Phi(z) \rangle_{\mathbb{R}^D} $$
                Follow the example of linear kernels, it is straightforward to verify that this Gram representation ensures that $K$ must be positive semidefinite.

                An extension of the homogeneous polynomial kernel is the \textit{inhomogeneous} polynomial kernel $K(x, z) = (1 + \langle x, z \rangle)^m $, which is based on polynomials of degree $m$ or less.
            \end{example}

            \begin{example}[Gaussian Kernels]
                As a more exotic example, given some compact subset $\mathcal{X} \subset \mathbb{R}^d$, consider the Gaussian kernel 
                    $$ K(x, z) = \exp\left(- \frac{1}{2 \sigma^2} ||x - z||_2^2 \right) $$
                    It is not immediately obvious that $K$ is positive semi-definite.
            \end{example}
        
        % subsubsection positive_semidefinite_kernel_functions (end)

        \subsubsection{Constructing an RKHS from a Kernel} % (fold)
        \label{ssub:constructing_an_rkhs_from_a_kernel}
        
        % subsubsection constructing_an_rkhs_from_a_kernel (end)

        \subsubsection{Alternative Way to Construct RKHS} % (fold)
        \label{ssub:alternative_way_to_construct_rkhs}
        
        % subsubsection alternative_way_to_construct_rkhs (end)

    % subsection reproducing_kernel_hilbert_space (end)


    \subsection{Algorithms} % (fold)
    \label{sub:algorithms}
    
    % subsection algorithms (end)


% section kernel_methods (end)


\section{Local Averaging Methods} % (fold)
\label{sec:local_averaging_methods}

    \subsection{Quick Review} % (fold)
    \label{sub:quick_review}
    
        In empirical risk minmization, our target is to approximate the Bayes predictor by minimizing the expected risk $\mathcal{R}(f) = \mathbb{E}_{P}[l(Y, f(X))]$. However, the joint distribution of real data $P(X, Y)$ remains unknown, so we have to minimize the empirical risk, which assign uniform weight $1/n$ to each $(X_i, Y_i)$ pair, instead of the expected risk, that is
        \begin{equation}
            \hat{f}(X) = \argmin_{f(X) \in \mathcal{Y}} \ \hat{\mathcal{R}}(f) = \argmin_{f(X) \in \mathcal{Y}} \ \frac{1}{n} \sum_{i=1}^{n} l(Y_i, f(X_i))
        \end{equation} 
        In spite of this, the empirical risk is still difficult to optimize as $f$ could be any measurable function. Therefore, in the Section \ref{sec:empirical_risk_minimization}, we constrain our choice in a hypothesis space $\mathcal{F}$ in order to make the optimization problem solvable.
        The two classical cases we consider previously in Section \ref{sec:introduction_to_supervised_learning} are
        \begin{itemize}
            \item Binary Classification: $\mathcal{Y} = \{-1, +1\}$ (or $\mathcal{Y} = \{0,1\}$) with $0$-$1$ loss $l(y, \hat{y}) = \mathbbm{1}\{y \neq \hat{y} \}$, then the expected risk is $\mathcal{R}(f) = \mathbb{P}(Y \neq f(X))$.

            \item Regression: $\mathcal{Y} = \mathbb{R}$ with square loss $l(y, \hat{y}) = (y - \hat{y})^2$, and the expected risk $\mathcal{R}(f) = \mathbb{E}[(Y - f(X))^2]$
        \end{itemize}
        As seen before, minimizing the expected risk leads to an optimal "target function", call the Bayes predictor.

        \begin{proposition}[Bayes Predictor and Bayes Risk]
            The conditional expected risk is minimized at a Bayes predictor $f^*: \mathcal{X} \rightarrow \mathcal{Y}$ satisfying for all $x \in \mathcal{X}$,
            \begin{equation}
                f^*(X) \in \argmin_{f(X) \in \mathcal{Y}} \ \mathbb{E}_{Y}\big[ l(Y, f(X)) \mid X\big]
            \end{equation}
            The Bayes risk $\mathcal{R}^*$ is the risk of all Bayes predictors and is equal to
            \begin{equation}
                \mathcal{R}^* = \mathbb{E}_{X} \Big[ \inf_{f(X) \in \mathcal{Y}} \mathbb{E}_{Y} \big[l(Y, f(X)) \mid X \big] \Big]
            \end{equation}
        \end{proposition} 
        Note that the Bayes predictor is not unique, but that all Bayes predictors lead to the same Bayes risk, and that the Bayes risk is usually nonzero (unless the dependence between $X$ and $Y$ is deterministic). Specifially, we have the following special cases:
        \begin{itemize}
            \item Binary Classification: $\mathcal{Y} = \{0, 1\}$ and $l(y, \hat{y}) = \mathbbm{1}\{y \neq \hat{y}\}$, the Bayes predictor is equal to 
                $$ f^*(X) \in \argmax_{y \in \{0, 1\}} \ \mathbb{P}(Y = y \mid X) $$
                This result extends naturally to multi-category classification with the Bayes predictor 
                $$ f^*(X) \in \argmax_{y \in \{1, \dots, k\}} \ \mathbb{P}(Y = y \mid X) $$
                Moreover, with the square loss
                \begin{equation*}
                    \begin{aligned}
                        \mathcal{R}(f) - \mathcal{R}^* &= \mathbb{E}_{X,Y}[l(Y - f(X))] - \mathbb{E}_{X, Y}[l(Y - f^*(X))] \\
                        &= \int_{\mathcal{X}} \Big\{\mathbb{E}_Y[\mathbbm{1}(Y \neq f(X)) \mid X = x] - \mathbb{E}_Y[\mathbbm{1}(Y \neq f^*(X)) \mid X = x] \Big\} dP(x) \\
                        &= \int_{\mathcal{X}} \Big\{\mathbb{E}_Y [\mathbbm{1}(Y \neq f(X)) - \mathbbm{1}(Y \neq f^*(X)) \mid X = x] \Big\} dP(x) \\
                        &= \int_{\mathcal{X}} \Big\{\mathbb{E}_Y\big[ (\mathbbm{1}(1 \neq f(X)) - \mathbbm{1}(1 \neq f^*(X)) ) \cdot \mathbbm{1}\{Y = 1\} \mid X = x \big] \Big\} dP(x) \\
                        &+ \int_{\mathcal{X}} \Big\{\mathbb{E}_Y\big[ (\mathbbm{1}(0 \neq f(X)) - \mathbbm{1}(0 \neq f^*(X)) ) \cdot \mathbbm{1}\{Y = 0\} \mid X = x \big] \Big\} dP(x) \\
                        &= \int_{\mathcal{X}} \Big\{\mathbb{E}_Y\big[ (\mathbbm{1}(1 \neq f(X)) - 0) \cdot \mathbbm{1}\{Y = 1\} \mid X = x \big] \Big\} dP(x) \\
                        &+ \int_{\mathcal{X}} \Big\{\mathbb{E}_Y\big[ (\mathbbm{1}(0 \neq f(X)) - 0) \cdot \mathbbm{1}\{Y = 0\} \mid X = x \big] \Big\} dP(x) \quad \text{(1)} \\
                        &= \int_{\mathcal{X}} \Big\{\mathbb{E}_Y \big[ \mathbbm{1}(f^*(X) \neq f(X)) \cdot \mathbbm{1}\{Y = 1\} + \mathbbm{1}(f^*(X) \neq f(X)) \cdot \mathbbm{1}\{Y = 0\} \mid X = x \big] \Big\} dP(x) \quad \text{(2)} \\
                        &= \int_{\mathcal{X}} \mathbb{E}_Y \big[ \mathbbm{1}(f^*(X) \neq f(X)) \mid X = x \big] dP(x) \\
                        &= \int_{\mathcal{X}} \mathbbm{1}(f^*(x) \neq f(x)) dP(x) \\
                    \end{aligned}
                \end{equation*}
                where the equalities (1) and (2) hold by the definition of Bayes predictor $f^*$.

            \item Regression: $\mathcal{Y} = \mathbb{R}$ and $l(y, \hat{y}) = (y - \hat{y})^2$, the Bayes predictor is
                $$ f^*(X) = \mathbb{E}[Y \mid X] $$
                Moreover, with the square loss, we have
                \begin{equation*}
                    \begin{aligned}
                        \mathcal{R}(f) - \mathcal{R}^* &= \mathbb{E}_{X,Y}[l(Y - f(X))] - \mathbb{E}_{X, Y}[l(Y - f^*(X))] \\
                        &= \int_{\mathcal{X}} \Big\{\mathbb{E}_Y[(Y - f(X))^2 \mid X = x] - \mathbb{E}_Y[(Y - f^*(X))^2 \mid X = x] \Big\} dP(x) \\
                        &= \int_{\mathcal{X}} \Big\{\mathbb{E}_Y \big[2Y(f^*(X) - f(X)) + f(X)^2 - f^*(X)^2 \mid X = x \big] \Big\} dP(x) \\
                        &= \int_{\mathcal{X}} \Big\{ 2(f^*(x) - f(x)) \mathbb{E}_Y[Y \mid X = x] + f^{*}(x)^2 - f(x)^2 \Big\} d P(x) \\
                        &= \int_{\mathcal{X}} \Big\{ 2f^*(x)^2 - 2f(x)f^*(x) + f(x)^2 - f^*(x)^2 \Big\} d P(x) \\
                        &= \int_{\mathcal{X}} \big(f(x) - f^*(x) \big)^2 dP(x) \\ 
                        &= ||f - f^*||_{L_2(P(X))}^2 \\
                    \end{aligned}
                \end{equation*}
                Note that in general, such relation does not hold for arbitrary loss function.
        \end{itemize}
        Therefore, the goal of supervised machine learning is thus to estimate $f^*$, knowing the training samples $S_n = \{(X_1, Y_1), \dots, (X_n, Y_n) \}$ and the loss $l$, by minimizing the risk or excess risk $\mathcal{R}(f) - \mathcal{R}^*$.
    
    % subsection quick_review (end)

    \subsection{Local Averaging Methods} % (fold)
    \label{sub:local_averaging_methods}

        Local averaging methods provide a different approach by minimizing the conditional expected risk $\mathbb{E}[l(Y, f(X)) \mid X]$ pointwisely, which leads to the Bayes predictor $f^*(X)$. However, the conditional probability $P(Y \mid X)$ is generally unknown. To overcome this obstacles, this time we approximate the $P(Y \mid X)$ by some estimator $\hat{P}(Y \mid X)$, and the optimal predictor could be obtained by
        \begin{equation}
            \hat{f}(X) = \argmin_{f(X) \in \mathcal{Y}} \int_{\mathcal{Y}} l(y, f(X)) d\hat{P}(Y = y \mid X) 
        \end{equation}
        These are often called “plug-in” estimators. In the usual cases:
        \begin{itemize}
            \item Classification with $0$-$1$ loss: 
                $$ \hat{f}(X) \in \argmax_{y \in \{1, \dots, k\}} \ \hat{\mathbb{P}}(Y = y \mid X) $$

            \item Regression with square loss:
                $$ \min_{f(X) \in \mathcal{Y}} \int_{\mathcal{Y}} (y - f(X))^2 d\hat{P}(Y = y \mid X) $$ 
                The first-order optimal condition yields
                $$ \int_{\mathcal{Y}} (2\hat{f}(X) - 2y) d\hat{P}(Y = y \mid X) = 0 $$
                which implies
                $$ \hat{f}(X) = \int_{\mathcal{Y}} y \ d\hat{P}(Y = y \mid X) $$
        \end{itemize}
        In this way, we don't need to claim a hypothesis on the form of function $f$, but the tradeoff is we have to estimate the conditional distribution $P(Y \mid X)$ as well as the marginal distribution $P(X)$.

        As you shall see later, all of the methods we're going to introduce in this section can provably learn complex non-linear functions $f$ with a convergence rate of the form $\mathcal{O}(n^{-2/(d + 2)})$, where $d$ is the underlying dimension, leading to the curse of dimensionality.

    % subsection local_averaging_methods (end)

    \subsection{Linear Estimator} % (fold)
    \label{sub:linear_estimator}
    
        In this section, we will consider "linear" estimators (which is linear in data points $Y_i$), where the conditional distribution is of the form
        \begin{equation}
            \hat{f}(X) = \hat{P}(Y \mid X) = \sum_{i=1}^{n} \hat{w}_i(X) \cdot \delta_{Y_i}(Y)
            \label{eq:local-avg-linear-estimator}
        \end{equation}
        where $\delta_{Y_i}$ is the Dirac probability distribution at $Y_i$, and the weight function $\hat{w}_i: \mathcal{X} \mapsto \mathbb{R}$, $i = 1, \dots, n$ depends on the input data only (for simplicity) and satisfy for all $i \in \{1, \dots, n \}$ and $X \in \mathcal{X}$
        \begin{equation}
            \hat{w}_i(X) \geq 0, \quad \text{and} \ \sum_{i=1}^{n} \hat{w}_i(X) = 1 
        \end{equation}
        almost surely in $X$. These conditions ensure that for all $x \in \mathcal{X}$, $\hat{P}(Y \mid X)$ is a probability distribution. 
        For our running assumptions, this leads to the following predictions:
        \begin{itemize}
            \item Binary Classification: 
                $$ \hat{f}(X) \in \argmax_{j \in \{1, \dots, k\}} \sum_{i=1}^{n} \hat{w}_i(X) \cdot \mathbbm{1}_{Y_i = j} $$ 
                that is, each observation $(X_i, Y_i)$ votes for its label with weight $\hat{w}_i(X)$. 

            \item Regression on $\mathcal{Y} = \mathbb{R}$: 
                $$ \hat{f}(X) = \sum_{i=1}^{n} \hat{w}_i(X) Y_i $$ 
                This is why the terminology "linear estimators" is sometimes used, since as function of the response vector in $\mathbb{R}^n$, the estimator is linear.
        \end{itemize}

        \noindent
        \textbf{Construction of Weight Functions}. In most cases, for any $i$, the weight function $\hat{w}_i(X)$ is closed to $1$ for training points $X_i$ which are close to $X$ (measure the similarity with $X_i$). We next show three classical ways of building them: (1) partition estimators, (2) Nearest-neighbors, and (3) Nadaraya-Waatson estimator (a.k.a. kernel regression).

        \subsubsection{Partition Estimators} % (fold)
        \label{ssub:partition_estimators}

            If $\mathcal{X} = \bigcup_{j \in J} A_j$ is a partition (such that for all $j, j' \in J, A_j \cap A_{j'} = \emptyset$) of $\mathcal{X}$ with a countable index set $J$ (which we assume finite for simplicity), then we can conider for any $X \in \mathcal{X}$ the corresponding element $A(X)$ of the partition (namely, $A(X)$ is the unique $A_j$ such that $X \in A_j$), and define
            \begin{equation}
                \hat{w}_{i}(X) = \frac{\mathbbm{1}_{X_i \in A(X)}}{\sum_{j=1}^{n} \mathbbm{1}_{X_j \in A(X)}}
                \label{eq:partition-weight}
            \end{equation}
            with the convention that if no training data points lies in $A(X)$, then $\hat{w}_i(X)$ is equal to $1/n$ for each $i \in \{1, \dots, n\}$. This implies that each $w_i$ is piecewise constant with respect to the partition, that is, for any non-empty cell $A_j$ (at least one sample falls in $A_j$) and $X \in A_j$, the vectors $w(X) = (w_1(X), \dots, w_n(X))^{\top}$ has components equals to $1/{n_{A_j}}$ for $X_i \in A_j$ and $0$ otherwise. Here $n_{A_j}$ is the number of training samples in the set $A_j$.

            \vspace{1em}
            \noindent
            \textbf{Equivalence with Least-squares Regression}. When applied to regression where the estimator is $\hat{f}(X) = \sum_{i=1}^{n} \hat{w}_i(X) Y_i $, then using a partition estimators can be seen as a least-square estimator with feature vector $\varphi(X) = (\mathbbm{1}_{X \in A_1}, \dots, \mathbbm{1}_{X \in A_J})^{\top} \in \mathbb{R}^J$. Indeed, from trainning data $(X_1, Y_1), \dots, (X_n, Y_n)$, we need to find the weight vector $\hat{\theta}$ through the normal equations
            $$ \sum_{i=1}^{n} \varphi(X_i) \varphi(X_i)^{\top} \theta = \sum_{i=1}^{n} Y_i \varphi(X_i) $$
            It turns out that the matrix $\hat{\Sigma} = \sum_{i=1}^{n} \varphi(X_i) \varphi(X_i)^{\top} $ is diagonal with the $j$th component equals to $n_{A_j}$, the number of data points lying in cell $A_j$. This implies that for a non-empty cell $A_j$, $\theta_j$ is the average of all $Y_i$'s for $X_i$ lying in $A_j$, namely,
            $$ \theta_j = \frac{1}{n_{A_j}} \sum_{i = 1}^{n} Y_i \cdot \mathbbm{1}_{X_i \in A_j} $$
            Thus, for all $X \in A_j$, the prediction is exactly $\theta_j$, just as the weights obtained from Eq.(\ref{eq:partition-weight}). 
            For empty cells, $\theta_j$ is not determined. Among the many OLS estimators, we select the one for which the variance of the vector $\theta$ is smallest, that is $\sum_{j \in J}(\theta_j - \frac{1}{|J|} \sum_{j' \in J} \theta_j' )^2$ is smallest. A short calculation shows that this exactly leads to $\theta_j = \frac{1}{n} \sum_{i=1}^{n} Y_i $ for these empty cells, which correspond to our chosen convention.

            This equivalence with least-squares estimation with a diagonal (empirical or not) non-centered covariance matrix makes it attractive for theoretical purposes.

            \vspace{1em}
            \noindent
            \textbf{Choice of Partitions}. These are two standard applications of partition estimators:
            \begin{itemize}
                \item Fixed partitions.

                    for example, when $\mathcal{X} = [0, 1]^d$, then we choose the bandwidth $h$, with $|J| = h^{-d}$ (if $h=1/5$ and $d = 2$, we have $|J|=25$). Note here that the computation time for ech $X \in \mathcal{X}$ is not necessarily proportional to $|J|$, but to $n$ (by simply considering the bins where the data lie). This estimator is some times called a "regressogram".

                    \begin{figure}[htb]
                        \centering
                        \includegraphics[width=.8\linewidth]{Pics/fixed-partitions.png}
                        \caption{Regressograms in $d=1$ dimension, with three different values of $|J|$. We can see both underfitting, or overfitting in this example. Note that the target function $f^*$ is piecewise affine, and that on the affine parts, the estimator is far from linear, namely, the estimator cannot take advantage of extra-regularity.}
                    \end{figure}

                \item Decision trees.

                    for data in a hypercube, we can recursively partition it by selecting a variable to split leading to a maximum reduction in errors when defining the partitioning estimate. Note that now the partition depends on the labels (so the analysis below does not apply, unless the partitioning is learned on a different data than the one sued for the estimation).
            \end{itemize}

        % subsubsection partition_estimators (end)

        \subsubsection{Nearest-Neighbors} % (fold)
        \label{ssub:nearest_neighbors}
        
            Given an integer $k \geq 1$, and a distance $d$ on $\mathcal{X}$, for any $X \in \mathcal{X}$, we can order the $n$ samples so that
            $$ d(X_{i_1(X)}, X) \leq d(X_{i_2(X)}, X) \leq \cdots \leq d(X_{i_n(X)}, X) $$
            where $\{i_1(X), \dots, i_n(X)\} = \{1, \dots, n\}$, and ties are broken randomly. We then define
            \begin{equation}
                \hat{w}_i(X) = \frac{1}{k}, \quad \text{if }
            \end{equation}
            if $i \in \{i_1(X), \dots, i_k(X) \}$ and $\hat{w}_i(X) = 0$ otherwise.
            Given a new input $X \in \mathcal{X}$, the nearest neighbor predictor looks at the $k$ nearest points $X_i$ in the data set $\{(X_1, Y_1), \dots, (X_n, Y_n)\}$ and predicts a majority vote among them for classification or simply the averaged response for regression. The number of nearest neighbors $k$ is a hyperparameter which needs to be estimated (typically by cross-validation).

            \vspace{1em}
            \noindent
            \textbf{Algorithms}. Given a test point $X \in \mathcal{X}$, the naive algorithm looks at all training data points for computing the predicted response, thus the complexity is $O(nd)$ per test point in $\mathbb{R}^d$. When $n$ is large, this is costly in time and memory. There exists indexing techniques for (potentially approximate) nearest-neighbor search, such as “k-dimensional-trees”, with typically a logarithmic complexity in n (but with some additional compiling time).

            \begin{figure}[htb]
                \centering
                \includegraphics[width=.8\linewidth]{Pics/k-nearest-neighbors.png}
                \caption{$k$-nearest neighbor regression in $d = 1$ dimension, with three values of $k$ (the number of neighbors). We can see both underfitting ($k$ too large), and overfitting ($k$ too small).}
            \end{figure}

        % subsubsection nearest_neighbors (end)

        \subsubsection{Nadaraya-Watson Estimator (Kernel Regression)} % (fold)
        \label{ssub:nadaraya_watson_estimator_kernel_regression_}

            Given a "kernel" function $K: \mathcal{X} \times \mathcal{X} \mapsto \mathbb{R}_{+}$, which is pointwise non-negative, we define
            \begin{equation}
                \hat{w}_i(X) = \frac{K(X, X_i)}{\sum_{j=1}^n K(X, X_j)}
            \end{equation}
            with the convention that if $K(X, X_j) = 0$ for all $j \in \{1, \dots, n\}$, then $\hat{w}_i(X)$ is equal to $1/n$ for each $i$. In most case where $\mathcal{X} \subset \mathbb{R}^d$, we take
            $$ K(X, X') = \frac{q(\frac{1}{h}(X - X'))}{h^d} $$
            for a certain function $q: \mathbb{R}^d \mapsto \mathbb{R}_{+}$ that has large values around $0$, and $h > 0$ a bandwidth parameter to be selected. If we assume that $q$ is integrable with integral equal to one, then $K(\cdot, X')$ is a probability density with mass around $X'$, which gets more concentrated as $h$ goes to zero. See illustration below for the two typiocal windows.

            \begin{figure}[htb]
                \centering
                \includegraphics[width=.7\linewidth]{Pics/kernels-local-averaging.png}
            \end{figure}

            \begin{itemize}
                \item Box kernel: $q(X) = \mathbbm{1}_{||X||_2 \leq 1}$

                \item Gaussian kernel: $q(X) = e^{-||X||^2/2}$, where we use the fact it is non-negative pointwise (as opposed to positive definitiness).
            \end{itemize}

            In terms of algorithms, with a naive algorithm, for every test point, all the input data have to be considered, that is, a complexity proportionial to $n$. The same techniques used for efficient $k$-nearest-neighbor search (e.g. k-d-tress) can be applied here as well.

            \begin{figure}[htb]
                \centering
                \includegraphics[width=.8\linewidth]{Pics/nadaraya-watson-kernel-regression.png}
                \caption{Nadaraya-Watson regression in $d=1$ dimension, with three values of bandwidth $h$ for the Gaussian kernel.}
            \end{figure}
        
        % subsubsection nadaraya_watson_estimator_kernel_regression_ (end)
        
    % subsection linear_estimator (end)

    \subsection{Generic Simplest Consistency Analysis} % (fold)
    \label{sub:generic_simplest_consistency_analysis}

        We consider for simplicity the regression case. For classification, calibration techniques such as those used in Section \ref{sec:empirical_risk_minimization} can be used.
        We make the following generic assumptions here:
        \begin{enumerate}
            \item Bounded noise: there exists $\sigma \geq 0$ such that $|Y - \mathbb{E}[Y \mid X]|^2 \leq \sigma^2$ almost surely.

            \item Regular target function: the target function $f^{*}(X) = \mathbb{E}[Y \mid X]$ is B-Lipschitz-continuous with respect to a distance $d$
        \end{enumerate}
        We have, with the target function $f^*(X) = \mathbb{E}[Y \mid X]$, at a test point $X \in \mathcal{X}$ and using that the weights $w_i(X)$ sum to one:
        \begin{equation}
            \begin{aligned}
                \hat{f}(X) - f^*(X) &= \sum_{i=1}^{n} \hat{w}_i(X) Y_i - \mathbb{E}[Y \mid X] \\
                &= \sum_{i=1}^{n} \hat{w}_i(X) \cdot (Y_i - \mathbb{E}[Y_i \mid X_i]) + \sum_{i=1}^{n} \hat{w}_i(X) \cdot (\mathbb{E}[Y_i \mid X_i] - \mathbb{E}[Y \mid X]) \\
                &= \sum_{i=1}^{n} \hat{w}_i(X) \cdot (Y_i - \mathbb{E}[Y_i \mid X_i]) + \sum_{i=1}^{n} \hat{w}_i(X) \cdot \left(f^{*}(X_i) - f^{*}(X) \right) \\ 
            \end{aligned}
        \end{equation}
        Given $X_1, \dots, X_n$ and because we have assumed the weight functions do not depend on the labels, the left term has zero expectation by the iterated law of expectation
        \begin{equation*}
            \begin{aligned}
                \mathbb{E}\Big[\hat{w}_i(X) \cdot \big(Y_i - \mathbb{E}[Y_i \mid X_i] \big) \Big] &= \mathbb{E}\Big[\mathbb{E}\big[\hat{w}_i(X) \cdot (Y_i - \mathbb{E}[Y_i \mid X_i]) \mid X_i \big] \Big] \\
                &= \mathbb{E} \Big[\hat{w}_i(X) \cdot \mathbb{E}\big[Y_i - \mathbb{E}[Y \mid X_i] \mid X_i \big] \Big] \\  
                &= \mathbb{E} \Big[\hat{w}_i(X) \cdot \big(\mathbb{E}[Y_i \mid X_i] - \mathbb{E}[Y \mid X] \big) \Big] = 0
            \end{aligned}
        \end{equation*}
        while the right term involving $f^*(X)$ is deterministic (the variance is therefore zero).
        We thus have, using the independence of all $(X_i, Y_i)$, $i = 1, \dots, n$, and for $X$ fixed:
        \begin{equation*}
            \begin{aligned}
                \mathbb{E}\Big[\big(\hat{f}(X) - f^{*}(X) \big)^2 \mid X_1, \dots, X_n \Big] &= \mathbb{E}[\hat{f}(X) - f^*(X) \mid X_1, \dots, X_n]^2 + \var(\hat{f}(X) - f^*(X) \mid X_1, \dots, X_n) \\
                &= \big(\mathbb{E}[\hat{f}(X) \mid X_1, \dots, X_n] - f^*(X) \big)^2 + \var(\hat{f}(X) \mid X_1, \dots, X_n) + 0 \\
                &= \left(\sum_{i=1}^{n} \hat{w}_i(X) \cdot \mathbb{E}[Y_i \mid X_i] - f^*(X) \right)^2 + \var \left(\sum_{i=1}^{n} \hat{w}_i(X) \cdot Y_i \mid X_1 \dots, X_n \right) \\
                &= \left(\sum_{i=1}^{n} \hat{w}_i(X) \cdot f^*(X_i) - f^*(X) \right)^2 + \sum_{i=1}^{n} \hat{w}_i(X)^2 \cdot \var(Y_i \mid X_i) \\
                &= \left(\sum_{i=1}^{n} \hat{w}_i(X) \cdot \big(f^*(X_i) - f^*(X) \big) \right)^2 + \sum_{i=1}^{n} \hat{w}_i(X)^2 \cdot \mathbb{E}\Big[\big(Y_i - \mathbb{E}[Y_i \mid X_i] \big)^2 \mid X_i \Big] \\
                &= \qquad \text{bias} \qquad + \qquad \text{variance} \\
            \end{aligned}
        \end{equation*}
        with a "bias" term which is zero if $f^*$ is constant, and a "variance" term which is zero, when $Y$ is a deterministic function of $X$. We can further bound these as
        \begin{equation*}
            \begin{aligned}
                \mathbb{E}\Big[\big(\hat{f}(X) - f^*(X) \big)^2 \mid X_1, \dots, X_n \Big] &\leq \left( \sum_{i=1}^{n} \hat{w}_i(X) \cdot \big|f^*(X_i) - f^*(X) \big| \right)^2 + \sigma^2 \sum_{i=1}^{n} \hat{w}_i(X) \quad (\text{Assumption 1}) \\
                &\leq \left(\sum_{i=1}^{n} \hat{w}_i(X) \cdot B \cdot d(X_i, X) \right)^2 + \sigma^2 \sum_{i=1}^{n} \hat{w}_i(X)^2 \quad (\text{Assumption 2}) \\
                &\leq B^2 \sum_{i=1}^{n} \hat{w}_i(X) \cdot d(X_i, X)^2 + \sigma^2 \sum_{i=1}^{n} \hat{w}_i(X)^2 \quad (\text{Jensen's inequality}) \\
            \end{aligned}
        \end{equation*}
        We then have for the expected excess risk
        \begin{equation}
            \int_{\mathcal{X}} \mathbb{E}\Big[\big(\hat{f}(X) - f^*(X) \big)^2 \Big] d P(X) \leq B^2 \int_{\mathcal{X}} \mathbb{E}\left[\sum_{i=1}^{n} \hat{w}_i(X) d(X_i, X)^2 \right] dP(X) + \sigma^2 \sum_{i=1}^{n} \int_{\mathcal{X}} \mathbb{E}\left[\hat{w}_i(X)^2 \right] dP(X)
        \end{equation}
        \textbf{Warning}: the expectation is with respect to the training data. The expectation with respect to the testing point $X$ is kept as an integral to avoid confusions.

        This upper bound can be divided into:
        \begin{itemize}
            \item A variance term $\sigma^2 \sum_{i=1}^{n} \int_{\mathcal{X}} \mathbb{E}\left[\hat{w}_i(X)^2 \right] dP(X)$, that depends on the noise on top of the optimal predictions. Since the weights sum to one, we can write $\sum_{i=1}^{n} \mathbb{E}[\hat{W}_i(X)^2] = \sum_{i=1}^{n} \mathbb{E}[(\hat{w}_i(X) - 1/n)^2] + 2/n - 1/n^2$, that is, up to vanishing constant, the variance term measures the deviation ot the uniform weights.

            \item A bias term $B^2 \int_{\mathcal{X}} \mathbb{E}\left[\sum_{i=1}^{n} \hat{w}_i(X) d(X_i, X)^2 \right] dP(X)$, which depends on the regularity of the target function.
        \end{itemize}
        Both variance and bias have to go to zero when $n$ grows, and this corresponds to two simple quantities on the weights. 
        \begin{itemize}
            \item For the variance, the worst case scenario is that $\hat{w}_i(X)^2 \approx \hat{w}_i(X)$, that is, weights are putting all the mass in to a single label (different for different testing sample), thus leading to overfitting.

            \item For the bias, the worst case scenario is that weights are uniform (leading to underfitting).
        \end{itemize}
        
    % subsection generic_simplest_consistency_analysis (end)

% section local_averaging_methods (end)


\clearpage

\printbibliography

\clearpage

\begin{appendices}

    \section{Norms} % (fold)
    \label{sec:norms}

        \subsection{Norms} % (fold)
        \label{sub:norms}

            A function $f:\mathbb{R}^n \rightarrow \mathbb{R}$ with $\text{dom} \ f = \mathbb{R}^n$ is called a norm if
            \begin{itemize}
                \item $f$ is nonnegative: $f(x) \geq 0$ for all $x \in \mathbb{R}$

                \item $f$ is definite: $f(x) = 0$ only if $x = 0$

                \item $f$ is homogeneous: $f(tx) = |t|f(x)$, for all $x \in \mathbb{R}^n$ and $t \in \mathbb{R}$

                \item $f$ satisfies the triangle inequality: $f(x+y) \leq f(x) + f(y)$, for all $x, y \in \mathbb{R}^n$
            \end{itemize}

            A norm is a measure of the length of a vector $x$; we can measure the distance between two vectors $x$ and $y$ as the length of their difference, $i.e.$
            $$ \rm{dist}(x, y) = ||x - y|| $$

            The set of all vectros with norm less than or equal to one,
            $$ \mathcal{B} = \{x \in \mathbb{R}^n \mid ||x|| \leq 1 \} $$
            is called the unit ball of the norm $||\cdot||$. The unit ball satisfies the following properties:
            \begin{itemize}
                \item $\mathcal{B}$ is symmetric about the origin; $x \in \mathcal{B}$ iff $-x \in \mathcal{B}$ 

                \item $\mathcal{B}$ is convex

                \item $\mathcal{B}$ is closed, bounded, and has nonempty interior
            \end{itemize}

        % subsection norms (end)

        \subsection{Examples of Norm} % (fold)
        \label{sub:examples_of_norm}

            Here we consider the norm for vector $x \in \mathbb{R}^n$
            \begin{itemize}
                \item $l_1$-norm
                    $$ ||x||_1 = |x_1| + \cdots + |x_n| $$

                \item $l_{\infty}$-norm
                    $$ ||x||_{\infty} = \max\{|x_1|, \dots, |x_n| \} $$

                \item $l_p$-norm
                    $$ ||x||_p = \big(|x_1|^p + \cdots + |x_n|^{p} \big)^{1/p} $$

                \item $P$-quadratic norms: for matrix $P \in \mathbb{S}_{++}^{n}$, 
                    $$ ||x||_P = (x^\top P x)^{1/2} = ||P^{1/2} x||_2 $$
                The unit ball of a quadratic norm is an ellipsoid (and conversely, if the unit ball of a norm is an ellipsoid, the norm is a quadratic norm)

                \item Frobenius norm: for matrix $X \in \mathbb{R}^{m \times n}$,
                    \begin{equation}
                        ||X||_{F} = \big(\rm{tr}(X^\top X) \big)^{1/2} = \left(\sum_{i=1}^{m} \sum_{j=1}^{n} X_{ij}^2 \right)^{1/2}
                    \end{equation}
                    The Frobenius norm is the Euclidean norm of the vector obtained by listing the coefficients of the matrix. It is different from the $l_2$-norm of matrix.
            \end{itemize}
        
        % subsection examples_of_norm (end)

        \subsection{Equivalence of Norms} % (fold)
        \label{sub:equivalence_of_norms}

            Suppose that $||\cdot||_a$ and $||\cdot||_b$ are norms on $\mathbb{R}^n$. A basic result of analysis is that there exist positive constants $\alpha$ and $\beta$ such that, for all $x \in \mathbb{R}$,
            $$ \alpha ||x||_a \leq ||x||_b \leq \beta ||x||_a $$
            This means that the norms are equivalent, i.e., they define the same set of open subsets, the same set of covergent sequences, and so on. 
            Using convex analysis, we can give a more specific result: if $||\cdot||$ is any norm on $\mathbb{R}^n$, then there exists a quadratic norm $||\cdot||_P$ for which
            $$ ||x||_P \leq ||x|| \leq \sqrt{n}||x||_P $$
            holds for all $x$. In other words, any norm on $\mathbb{R}^n$ can be uniformly approximated, within a factor of $\sqrt{n}$, by a $P$-quadratic norm.
            \textit{We conclude that any norms on all finite-dimensional vector space are equivalent, but on infinite-dimensional vector spaces, the result need not hold}.

            \begin{theorem}[Holder's Inequality]
                Let $(S, \sigma, \mu)$ be a measure space and let $p, q \in [1, \infty]$ with $1/p + 1/q = 1$. Then for all measurable real- or complex-valued functions $f$ and $g$ on $S$,
                \begin{equation}
                    ||f g||_1 \leq ||f||_p ||g||_q
                \end{equation}
                If, in addition, $p, q \in (1, \infty)$ and $f \in L_p(\mu)$ and $g \in L_q(\mu)$, then Holder's inequality becomes an equality if and only if $|f|_p$ and $|g|_q$ are linearly dependent in $L_1(\mu)$, meaning that there exist real numbers $\alpha, \beta \geq 0$, not both of them zero, such that $\alpha |f|_p = \beta |g|_q$ $\mu$-almost everywhere.
            \end{theorem}
        
            The pair of numbers $(p, q)$ are called conjugate pair and the special case of $p = q = 2$ gives a form of the Cauchy-Schwarz inequality.

        % subsection equivalence_of_norms (end)

        \subsection{Operator Norms} % (fold)
        \label{sub:operator_norms}
        
            Supose $||\cdot||_a$ and $||\cdot||_b$ are norms on $\mathbb{R}^{m}$ and $\mathbb{R}^{n}$, respectively. We define the \textit{opertaor norm} of $X \in \mathbb{R}^{m \times n}$, induced by the norms $||\cdot||_a and ||\cdot||_b$, as
            \begin{equation}
                ||X||_{a,b} = \sup \big\{||Xu||_a \mid ||u||_b \leq 1 \big\}
            \end{equation}
            It can be shown that this defines a norm on $\mathbb{R}^{m \times n}$.

            \begin{itemize}
                \item When $||\cdot||_a$ and $||\cdot||_b$ are both Euclidean norms, the operator norm of $X$ is its \textit{maximum singular value}, and is denoted $||X||_2$:
                    \begin{equation}
                        ||X||_2 = \sigma_{\max}(X) = \big( \lambda_{\max}(X^\top X) \big)^{1/2}
                    \end{equation}
                    That is because, $X^{\top} X$ is a symmetric matrix, which satisfy
                    $$ u^{\top} (X^\top X) u \leq \lambda_{\max}(X^\top X) u^\top u $$
                    This agress with the Euclidean norm on $\mathbb{R}^m$, when $X \in \mathbb{R}^{m \times 1}$, so there is not clash of notation. This norm is also called the \textit{spectral norm} or \textit{$l_2$-norm} of $X$.

                \item The norm induced by the $l_{\infty}$-norm on $\mathbb{R}^{m}$ and $\mathbb{R}^n$, denoted $||X||_{\infty}$, is the \textit{max-row-sum} norm
                    \begin{equation*}
                        ||X||_{\infty} = \sup\big\{||Xu||_{\infty} \mid ||u||_{\infty} \leq 1 \big\} = \max_{i=1, \dots, m} \sum_{j=1}^{n} |X_{ij}|
                    \end{equation*}

                \item The norm induced by the $l_1$-norm on $\mathbb{R}^{m}$ and $\mathbb{R}^n$, denoted $||X||_1$, is the \textit{max-column-sum} norm
                    \begin{equation*}
                        ||X||_1 = \max_{j=1, \dots, n} \sum_{i=1}^{m}|X_{ij}|
                    \end{equation*}
            \end{itemize}
            
        % subsection operator_norms (end)

    % section norms (end)

    
    \section{Probability Theory} % (fold)
    \label{sec:probability_theory}

        \subsection{Independence} % (fold)
        \label{sub:independence}

            \begin{definition}[Independent]
                Two random variables $X$ and $Y$ are independent if, for every $A$ and $B$,
                $$ \mathbb{P}(X \in A, Y \in B) = \mathbb{P}(X \in A) \mathbb{P}(Y \in B) $$
                and we write $X \ind Y$.
            \end{definition}
            In principle, to check whether $X$ and $Y$ are independent we need to check the above equation for all subsets $A$ and $B$. Fortunately, we have the following result which we state for continuous random variables though it is true for discrete random variables too.
            
            \begin{theorem}
                Let $X$ and $Y$ have joint PDF $f_{X, Y}$. Then $X \ind Y$ if and only if 
                $$ f_{X, Y}(x, y) = f_X(x) f_Y(y) $$ 
                for all values $x$ and $y$.    
            \end{theorem}

            \begin{definition}[Conditional Independent]
                Let $X$, $Y$ and $Z$ be random variables. $X$ and $Y$ are conditionally independent given $Z$, wirtten $X \ind Y \mid Z$, if
                $$ f_{X, Y \mid Z}(x, y \mid z) = f_{X \mid Z}(x \mid z) f_{Y \mid Z}(y \mid z) $$
                for all $x$, $y$ and $z$.
            \end{definition}
            Intuitively, this means that, once you know $Z$, $Y$ provides no extra information about $X$. An equivalent definition is that
            $$ f_{X \mid Y, Z}(x \mid y, z) = f_{X \mid Z}(x \mid z) $$
            Here are some rules of the conditional independence:
            
            \begin{itemize}
                \item \textbf{Symmetry}
                    $$ X \ind Y \quad \Rightarrow \quad Y \ind X $$

                \item \textbf{Decomposition}
                    \begin{equation*}
                        X \ind (A, B) \quad \Rightarrow \quad \text{and} \ \left\{
                        \begin{aligned}
                            & X \ind A \\
                            & X \ind B \\
                        \end{aligned}
                        \right.
                    \end{equation*}
                    Proof:
                    \begin{equation*}
                        \begin{aligned}
                            f_{X, A}(x, a) &= \int_{B} f_{X, A, B}(x, a, b) db \\
                            &= \int_{B} f_{X}(x) f_{A, B}(a, b) db \\
                            &= f_X(x) f_{A}(a) \\
                        \end{aligned}
                    \end{equation*}
                    A similar proof shows the independence of $X$ and $B$.

                \item \textbf{Weak Union}
                    \begin{equation*}
                        X \ind (A, B) \quad \Rightarrow \quad \text{and} \ \left\{
                        \begin{aligned}
                            & X \ind A \mid B \\
                            & X \ind B \mid A \\
                        \end{aligned}
                        \right.
                    \end{equation*}
                    Proof: 
                    \begin{itemize}
                        \item by assumption, we have $\mathbb{P}(X) = \mathbb{P}(X \mid A, B)$
                        
                        \item due to the property of decomposition $X \ind B$, we have $\mathbb{P}(X ) = \mathbb{P}(X \mid B)$
                    \end{itemize}
                    Combining the above two equalities yields
                    $$ \mathbb{P}(X \mid B) = \mathbb{P}(X \mid A, B) $$
                    which establishes $X \ind A \mid B$. A similar proof shows the second condition.

                \item \textbf{Contraction}
                    \begin{equation*}
                        \left.
                        \begin{aligned}
                            X \ind A \mid B \\
                            X \ind B \\
                        \end{aligned}
                        \right\} \ \text{and} \quad \Rightarrow \quad X \ind (A, B)
                    \end{equation*}
                    or similarily
                    \begin{equation*}
                        \left.
                        \begin{aligned}
                            X \ind B \mid A \\
                            X \ind A \\
                        \end{aligned}
                        \right\} \ \text{and} \quad \Rightarrow \quad X \ind (A, B)
                    \end{equation*}
                    Proof:
                    this property can be proved by noticing that
                    $$ \mathbb{P}(X \mid A, B) = \mathbb{P}(X \mid B) = \mathbb{P}(X) $$
                    each equality of which is asserted by $X \ind A \mid B$ and $X \ind B$, respectively. A similar proof shows the second one.

                \item \textbf{Intersection}

                    for strictly positive probability distributions, the following also hold
                    \begin{equation*}
                        \left.
                        \begin{aligned}
                            X \ind Y \mid Z, W \\
                            X \ind W \mid Z, Y \\        
                        \end{aligned}    
                        \right \} \ \text{and} \quad \Rightarrow \quad X \ind (W, Y) \mid Z
                    \end{equation*}
                    Proof:
                    by assumption
                    $$ \mathbb{P}(X \mid Z, W, Y) = \mathbb{P}(X \mid Z, W) = \mathbb{P}(X \mid Z, Y) $$
                    Using this equality, together with the law of total probability applied to $\mathbb{P}(X \mid Z)$
                    \begin{equation*}
                        \begin{aligned}
                            \mathbb{P}(X \mid Z) &= \sum_{w \in W} \mathbb{P}(X \mid Z, W = w) \mathbb{P}(W = w \mid Z) \\
                            &= \sum_{w \in W} \mathbb{P}(X \mid Y, Z) \mathbb{P}(W = w \mid Z) \\
                            &= \mathbb{P}(X \mid Z, Y) \sum_{w \in W} \mathbb{P}(W = w \mid Z) \\
                            &= \mathbb{P}(X \mid Z, Y) \\
                        \end{aligned}
                    \end{equation*}
                    This suggest
                    $$ \mathbb{P}(X \mid Z, W, Y) = \mathbb{P}(X \mid Z) $$
                    which establishs $X \ind (W, Y) \mid Z$.
            \end{itemize}

            In general, we have
            \begin{equation}
                \begin{aligned}
                    X \ind (Y, Z) \quad &\Leftrightarrow \quad X \ind Y \ \text{and} \ X \ind Y \mid Z \\
                    &\Leftrightarrow \quad X \ind Z \ \text{and} \ X \ind Z \mid Y \\
                \end{aligned}
            \end{equation}
        
        % subsection independence (end)

        \subsection{Expectations} % (fold)
        \label{sub:expectations}

            \begin{definition}
                The expectation, or mean, or first moment, of random variable $X$ is defined to be
                \begin{equation*}
                    \mathbb{E}[X] = \int_{\mathcal{X}} x dF(x) = \left\{
                    \begin{aligned}
                        & \sum_{\mathcal{X}} x f(x) \\
                        & \int_{\mathcal{X}} x f(x) dx \\
                    \end{aligned}
                    \right.
                \end{equation*}
                assuming that the sum (or integral) is well defined.
            \end{definition}

            \begin{theorem}
                Let $Y = r(X)$, then
                $$ \mathbb{E}[Y] = \mathbb{E}[r(X)] = \int_{\mathcal{X}} r(x) dF_X(x) $$
            \end{theorem}

            \begin{definition}
                The conditional expectation of $X$ given $Y = y$ is
                $$ \mathbb{E}[X \mid Y = y] = \int_{-\infty}^{\infty} x f_{X\mid Y}(x) dx $$
                If $r(x, y)$ is a function of $x$ and $y$ then
                $$ \mathbb{E}[r(X, Y) \mid Y = y] = \int_{-\infty}^{\infty} r(x, y) f_{X\mid Y}(x) dx $$
                \label{def:cond-expectation}
            \end{definition}

            \begin{theorem}[Law of Total Expectations]
                For random variables $X$ and $Y$, assumping $\mathbb{E}[X]$ and $\mathbb{E}[Y]$ exists, then we have
                $$ \mathbb{E}\big[ \mathbb{E}[X \mid Y] \big] = \mathbb{E}[X] $$
                and more generally for any function $r(x, y)$
                $$ \mathbb{E}\big[ \mathbb{E}[r(X, Y) \mid X] \big] = \mathbb{E}[r(X, Y)] $$
            \end{theorem}

            \begin{proof}
                By definition \ref{def:cond-expectation}, we have
                \begin{equation*}
                    \begin{aligned}
                        \mathbb{E}\big[\mathbb{E}[X \mid Y] \big] &= \int_{-\infty}^{\infty} \mathbb{E}[X \mid Y = y] f_Y(y) dy \\
                        &= \int_{-\infty}^{\infty} \left( \int_{-\infty}^{\infty} x f_{X\mid Y}(x) dx \right) f_Y(y) dy \\
                        &= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} x \frac{f_{X, Y}(x, y) }{f_Y(y) } f_Y(y) dx dy \\
                        &= \int_{-\infty}^{\infty} x dx \int_{-\infty}^{\infty} f_{X,Y}(x,y) dy \\
                        &= \int_{-\infty}^{\infty} x f_X(x) dx = \mathbb{E}[X]
                    \end{aligned}
                \end{equation*}
                and similarily, we have
                \begin{equation}
                    \begin{aligned}
                        \mathbb{E}\big[\mathbb{E}[r(X, Y) \mid Y ] \big] &= \int_{-\infty}^{\infty} \mathbb{E}[r(X, Y) \mid Y = y] f_Y(y) dy \\
                        &= \int_{-\infty}^{\infty} \left( \int_{-\infty}^{\infty}  r(x, y) f_{X\mid Y}(x) dx \right) f_Y(y) dy \\
                        &= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty}  r(x, y) f_{X,Y}(x, y) dx dy \\
                        &= \mathbb{E}[r(X, Y)] \\
                    \end{aligned}
                \end{equation}
            \end{proof}

            \begin{corollary}[Law of Iterated Expectation]
                For random variables $X, Y, Z$, we have
                $$ \mathbb{E}\big[\mathbb{E}[Z \mid X, Y] \mid Y \big] = \mathbb{E}[Z \mid Y] = \mathbb{E}\big[\mathbb{E}[Z \mid Y] \mid X, Y \big] $$
            \end{corollary}

            \begin{proof}
                The first equality holds because of the fact that, for any $y$
                \begin{equation*}
                    \begin{aligned}
                        \mathbb{E}\big[\mathbb{E}[Z \mid X, Y] \mid Y = y \big] &= \mathbb{E}[r(X, Y) \mid Y = y] \\ 
                        &= \int_{-\infty}^{\infty} r(X, Y) f_{X\mid Y}(x) dx \\
                        &= \int_{-\infty}^{\infty} \mathbb{E}[Z \mid X, Y] f_{X\mid Y}(x) dx \\
                        &= \int_{-\infty}^{\infty} \left(\int_{-\infty}^{\infty} z f_{Z\mid X, Y}(z) dz \right) f_{X \mid Y}(x) dx \\
                        &= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} z \frac{f_{Z,X,Y}(z, x, y) }{f_{X, Y}(x, y) } \frac{f_{X,Y}(x, y) }{f_{Y}(y) } dx dz \\
                        &= \int_{-\infty}^{\infty} z dz \int_{-\infty}^{\infty} f_{Z, X \mid Y}(z, x) dx \\
                        &= \int_{-\infty}^{\infty} z f_{Z \mid Y}(z) dz = \mathbb{E}[Z \mid Y = y] \\
                    \end{aligned}
                \end{equation*}
                and for any $y$
                \begin{equation*}
                    \begin{aligned}
                        \mathbb{E}\big[\mathbb{E}[X \mid Y] \mid X, Y = y \big] &= \mathbb{E}\big[\mathbb{E}[X \mid Y = y] \mid X, Y = y \big] \\ 
                        &= \mathbb{E}[X \mid Y = y] \cdot \mathbb{E}[1 \mid X, Y = y] = \mathbb{E}[X \mid Y = y] \\
                    \end{aligned}
                \end{equation*}
            \end{proof}

            \begin{theorem}[Independencec]
                For random variables $X$ and $Y$, we have
                $$ \mathbb{E}[X Y] = \mathbb{E}[X \mid Y] \cdot \mathbb{E}[Y] $$
                If $X$ is independent of $Y$, i.e. \ $X \ind Y$, then we have
                $$ \mathbb{E}[X \mid Y] = \mathbb{E}[X] $$
                and consequently 
                $$ \mathbb{E}[X Y] = \mathbb{E}[X] \cdot \mathbb{E}[Y] $$
            \end{theorem}

            \begin{proof}
                By definition
                \begin{equation*}
                    \begin{aligned}
                        \mathbb{E}[X \mid Y] &= \int x f_{X \mid Y}(x) dx \\
                        &= \int x \frac{f_{X,Y}(x, y) }{f_{Y}(y) } dx \\
                        &= \int x f_X(x) dx \qquad (X \ind Y) \\
                        &= \mathbb{E}[X]
                    \end{aligned}
                \end{equation*}
            \end{proof}

            \begin{definition}
                The condtional variance is defined as
                \begin{equation}
                    \begin{aligned}
                        \var(X \mid Y = y) &= \mathbb{E}\big(X - \mathbb{E}[X \mid Y = y] \big)^2 \\
                        &= \int_{-\infty}^{\infty} \big(x - \mathbb{E}[X \mid Y = y] \big)^2 f_{X\mid Y}(x) dx \\
                    \end{aligned}
                \end{equation}
            \end{definition}

            \begin{theorem}[Law of Total Variance]
                For random variables $X$ and $Y$,
                \begin{equation}
                    \var(X) = \mathbb{E}[\var(X \mid Y) ] + \var(\mathbb{E}[X \mid Y] )
                \end{equation}
            \end{theorem}

            \begin{proof}
                Notice that
                \begin{equation*}
                    \begin{aligned}
                        \mathbb{E}[\var(X \mid Y)] &= \mathbb{E}\Big[\mathbb{E}[X^2 \mid Y] - \mathbb{E}[X \mid Y]^2 \Big] \\
                        &= \mathbb{E}[X^2] - \mathbb{E}\big[\mathbb{E}[X \mid Y]^2 \big] \\
                    \end{aligned}
                \end{equation*}
                and 
                \begin{equation*}
                    \begin{aligned}
                        \var(\mathbb{E}[X \mid Y] ) &= \mathbb{E}\big[\mathbb{E}[X \mid Y]^2 \big] - \mathbb{E}\big[\mathbb{E}[X \mid Y] \big]^2 \\
                        &= \mathbb{E}\big[\mathbb{E}[X \mid Y]^2 \big] - \mathbb{E}[X]^2 \\
                    \end{aligned}
                \end{equation*}
                Adding these two together yields $\var(X)$.
            \end{proof}
            
        % subsection expectations (end)

        \subsection{Convergences} % (fold)
        \label{sub:convergences}

            \begin{definition}[Type of Convergence]
                Let $X_1, X_2, \dots,$ be a sequence of random variables and let $X$ be another random variable. Let $F_n$ denote the CDF of $X_n$ and let $F$ denote the CDF of $X_n$ and $F$ the CDF of $X$.
                \begin{enumerate}
                    \item $X_n$ converges to $X$ \textbf{in quadratic mean} (convergence in $L_2$), written $X_n \overset{qm}{\longrightarrow} X$, if
                        \begin{equation}
                            \mathbb{E}[X_n - X]^2 \rightarrow 0
                        \end{equation}
                        as $n \rightarrow \infty$.

                    \item $X_n$ converges to $x$ in $L_1$, written $X_n \overset{L_1}{\longrightarrow} X$, if
                        \begin{equation}
                            \mathbb{E}|X_n - X| \rightarrow 0
                        \end{equation}
                        as $n \rightarrow \infty$.

                    \item $X_n$ converges to $X$ \textbf{almost surely}, written $X_n \overset{a.s.}{\longrightarrow} X $, if
                        \begin{equation}
                            \mathbb{P}\big( \{s: X_n(s) \rightarrow X(s) \} \big) = 1
                        \end{equation}

                    \item $X_n$ converges to $X$ \textbf{in probability}, written $X_n \overset{P}{\longrightarrow} X$, if, for every $\varepsilon > 0$,
                        \begin{equation}
                            \mathbb{P}(|X_n - X| > \varepsilon) \rightarrow 0
                        \end{equation}
                        as $n \rightarrow \infty$.

                    \item $X_n$ converges to $X$ \textbf{in distribution}, written $X_n \rightsquigarrow X$, if
                        \begin{equation}
                            \lim_{n \rightarrow \infty} F_n(t) = F(t)
                        \end{equation}
                        at all $t$ for which $F$ is continuous
                \end{enumerate}
            \end{definition}

            \begin{theorem}
                The following relationships hold:
                \begin{enumerate}
                    \item $X_n \overset{qm}{\longrightarrow} X$ implies that $X_n \overset{L_1}{\longrightarrow} X$

                    \item $X_n \overset{L_1}{\longrightarrow} X$ implies that $X_n \overset{P}{\longrightarrow} X$            

                    \item $X_n \overset{a.s.}{\longrightarrow} X$ implies that $X_n \overset{P}{\longrightarrow} X$

                    \item $X_n \overset{qm}{\longrightarrow} X$ implies that $X_n \overset{P}{\longrightarrow} X$

                    \item $X_n \overset{P}{\longrightarrow} X$ implies that $X_n \rightsquigarrow X$

                    \item If $X_n \rightsquigarrow X$ and if $\mathbb{P}(X = c) = 1$ for some real number $c$, then $X_n \overset{P}{\longrightarrow} P$
                \end{enumerate}
                In general, none of the reverse implications hold except the special case in 3.
            \end{theorem}

            \begin{proof}
                Consider the following
                \begin{enumerate}
                    \item Suppose 

                    \item 

                    \item 

                    \item Suppose that $X_n \overset{qm}{\longrightarrow} X$. Fix $\varepsilon > 0$ and use Markov's inequalit,
                        \begin{equation*}
                            \mathbb{P}(|X_n - X| > \varepsilon) = \mathbb{P}(|X_n - X|^2 > \varepsilon^2) \leq \frac{\mathbb{E}|X_n - X|^2 }{\varepsilon^2} \rightarrow 0
                        \end{equation*}

                    \item Fix $\varepsilon > 0$ and let $x$ be a continuity point of $F$, then
                        \begin{equation*}
                            \begin{aligned}
                                F_n(x) = \mathbb{P}(X_n \leq x) &= \mathbb{P}(X_n \leq x, X \leq x + \varepsilon) + \mathbb{P}(X_n \leq x, X > x + \varepsilon) \\
                                &\leq \mathbb{P}(X \leq x + \varepsilon) + \mathbb{P}(|X_n - X| > \varepsilon) \\
                                &= F(x + \varepsilon) + \mathbb{P}(|X_n - X| > \varepsilon)
                            \end{aligned}
                        \end{equation*}
                        Also,
                        \begin{equation*}
                            \begin{aligned}
                                F(x - \varepsilon) = \mathbb{P}(X \leq x - \varepsilon) &= \mathbb{P}(X \le x - \varepsilon, X_n \leq x) + \mathbb{P}(X \leq x - \varepsilon, X_n > x) \\
                                &\leq F_n(x) + \mathbb{P}(|X_n - X| > \varepsilon) \\
                            \end{aligned}
                        \end{equation*}
                        Hence, 
                        \begin{equation*}
                            F(x - \varepsilon) - \mathbb{P}(|X_n - X| > \varepsilon) \leq F_n(x) \leq F(x + \varepsilon) + \mathbb{P}(|X_n - X| > \varepsilon)
                        \end{equation*}
                        Take the limit as $n \rightarrow \infty$ to conclude that
                        \begin{equation*}
                            F(x - \varepsilon) \leq \lim_{n \rightarrow \infty} \inf F_n(x) \leq \lim_{n \rightarrow \infty} \sup F_n(x) \leq F(x + \varepsilon)
                        \end{equation*}
                        This holds for any $\varepsilon > 0$. Take the limit as $\varepsilon \rightarrow 0$ and use the fact that $F$ is continuous at $x$, we conclude that 
                        $$ \lim_{n \rightarrow \infty} F_n(x) = F(x) $$

                    \item Fix $\varepsilon > 0$, then
                        \begin{equation*}
                            \begin{aligned}
                                \mathbb{P}(|X_n - c| > \varepsilon) &= \mathbb{P}(X_n < c - \varepsilon) + \mathbb{P}(X_n > c + \varepsilon) \\
                                &\leq \mathbb{P}(X_n \leq c - \varepsilon) + \mathbb{P}(X_n > c + \varepsilon) \\
                                &= F_n(c - \varepsilon) + 1 - F_n(c + \varepsilon) \\
                                &\rightarrow F(c - \varepsilon) + 1 - F(c + \varepsilon) \\
                                = 0 + 1 - 1 = 0
                            \end{aligned}
                        \end{equation*}
                \end{enumerate}
            \end{proof}

            \begin{theorem}[Slutsky's Theorem]
                Let $X_n, Y_n$ and $X, Y$ be random variables,
                \begin{enumerate}
                    \item If $X_n \overset{P}{\rightarrow} X$ and $Y_n \overset{P}{\rightarrow} Y$, then $X_n + Y_n \overset{P}{\rightarrow} X + Y $
                    \item If $X_n \overset{P}{\rightarrow} X$ and $Y_n \overset{P}{\rightarrow} Y$, then $X_n Y_n \overset{P}{\rightarrow} XY $
                    \item If $X_n \rightsquigarrow X$ and $Y_n \rightsquigarrow c$, then $X_n + Y_n \rightsquigarrow X + c $
                    \item If $X_n \rightsquigarrow X$ and $Y_n \rightsquigarrow c$, then $X_n Y_n \rightsquigarrow cX $
                \end{enumerate}
            \end{theorem}

            \begin{theorem}[Continuous Mapping Theorem]
                Let $X$ be a random variable, ${X_n}$ be a sequence of random variables and $g$ be a continuous function.
                \begin{enumerate}
                    \item If $X_n \overset{a.s.}{\longrightarrow} X$, then $g(X_n) \overset{a.s.}{\longrightarrow} g(X)$
                    \item If $X_n \overset{P}{\longrightarrow} X$, then $g(X_n) \overset{P}{\longrightarrow} g(X)$
                    \item If $X_n \rightsquigarrow X$, then $g(X_n) \rightsquigarrow g(X)$
                \end{enumerate}
            \end{theorem}

            % \begin{figure}[h]
            %     \centering
            %     \includegraphics[width=.8\textwidth]{Pics/WLLN.jpg}
            %     \caption{Intuitions behind the Weak Law of Large Numbers.}
            % \end{figure}

            \begin{theorem}[The Weak Law of Large Numbers]
                If $X_1, \dots, X_n$ are I.I.D, then
                $$ \bar{X}_n \overset{P}{\longrightarrow} \mu $$
            \end{theorem}
            WLLN means that the distribution of $\bar{X}_n$ becomes more concentrated around $\mu$ as $n$ gets large.

            \begin{proof}
                Assume that $\sigma < \infty$. This is not necessary but it simplifies the proof. Using Chebyshev inequality,
                \begin{equation*}
                    \mathbb{P}(|\bar{X}_n - \mu| \geq \varepsilon) \leq \frac{\var(\bar{X}_n)}{\varepsilon^2} = \frac{\sigma^2}{n \varepsilon^2} \rightarrow 0
                \end{equation*}
            \end{proof}

            % \begin{figure}[h]
            %     \centering
            %     \includegraphics[width=.8\textwidth]{Pics/CLT.jpg}
            %     \caption{Intuition behind the Central Limit Theorem.}
            % \end{figure}

            \begin{theorem}[The Central Limit Theorem]
                Let $X_1, \dots, X_n$ be I.I.D with mean $\mu$ and variance $\sigma^2$, then
                \begin{equation}
                    Z_n \equiv \frac{\bar{X}_n - \mu}{\sqrt{\var(\bar{X}_n) }} = \frac{\sqrt{n}(\bar{X}_n - \mu) }{\sigma} \rightsquigarrow Z
                \end{equation}
                where $Z \sim N(0, 1)$. In other words,
                \begin{equation}
                    \lim_{n \rightarrow \infty} \mathbb{P}(Z_n \leq z) = \Phi(z) = \int_{-\infty}^{z} \frac{1}{\sqrt{2\pi} } e^{-x^2 / 2} dx
                \end{equation}
            \end{theorem}
            CLT suggests that the distribution (CDF, not PDF) of $\bar{X}_n$ can be approximated using a Normal disitribution. It's the probability statements that we are approximating, not the random variable itself.

            \begin{proof}
                Suppose there are $n$ I.I.D random variables $X_i$ with mean $\mu$ and variance $\sigma^2$. Let
                \begin{equation*}
                    Y_i = \frac{X_i - \mu}{\sigma}
                \end{equation*} 
                and
                \begin{equation*}
                    Z_n = \frac{\sum_{i} Y_i }{\sqrt{n} } = \frac{\sqrt{n} (\bar{X}_n - \mu) }{\sigma}
                \end{equation*}
                Suppose the moment generating function (MGF) of $Y_i$ is $\varphi_{Y}(t) = \mathbb{E}[ e^{tY} ] $, and it is finite in a neighborhood around $t = 0$.
                Then we have
                \begin{equation*}
                    \varphi_{Y_1 + \cdots + Y_n}(t) = \mathbb{E}\left[e^{t(Y_1 + \cdots + Y_n) } \right] = \mathbb{E}\left[e^{tY_i} \right]^n = \big(\varphi_{Y}(t) \big)^n
                \end{equation*}
                and consequently
                \begin{equation*}
                    \varphi_{Z_n}(t) = \mathbb{E}\left[e^{t \frac{Y_1 + \cdots + Y_n}{\sqrt{n}} } \right] = \left[\varphi_{Y} \left(\frac{t}{\sqrt{n} } \right) \right]^n
                \end{equation*}
                Notice that 
                \begin{equation*}
                    \varphi_Y'(0) = \mathbb{E}[Y] = 0 \quad \text{and} \quad \varphi_Y''(0) = \mathbb{E}[Y^2] = \var{Y} = 1
                \end{equation*}
                So the Taylor expansion gives us
                \begin{equation*}
                    \begin{aligned}
                        \varphi_Y(t) &= \varphi_Y(0) + t \varphi_Y'(0) + \frac{t^2}{2!} \varphi_Y''(0) + \frac{t^3}{3!} \varphi_Y'''(0) + \cdots \\
                        &= 1 + 0 + \frac{t^2}{2} + \frac{t^3}{3!} \varphi_Y'''(0) + \cdots \\
                        &= 1 + \frac{t^2}{2} + \frac{t^3}{3!} \varphi_Y'''(0) + \cdots \\ 
                    \end{aligned}
                \end{equation*}
                Therefore,
                \begin{equation*}
                    \begin{aligned}
                        \varphi_{Z_n}(t) &= \left[\varphi_{Y} \left(\frac{t}{\sqrt{n} } \right) \right]^n \\
                        &= \left[1 + \frac{t^2}{2! n} + \frac{t^3}{3! n^{3/2}} \varphi_Y'''(0) + \cdots \right]^n \\
                        &= \left[1 + \frac{\frac{t^2}{2} + \frac{t^3}{3! n^{1/2}} \varphi'''(0) + \cdots }{n} \right]^n \\
                        &\rightarrow e^{t^2/2}
                    \end{aligned}
                \end{equation*}
                The last step results from the fact that $\left(1 + \frac{a_n}{n} \right)^n \rightarrow e^a$ if $a_n \rightarrow a$. Notice that the MGF of standard normal variable $Z \sim N(0, 1)$ is just 
                $$ \varphi_Z(t) = \mathbb{E}\left[e^{tZ} \right] = e^{t^2/2} $$
                So we have 
                \begin{equation*}
                    \varphi_{Z_n}(t) \rightarrow \varphi_Z(t) \quad \Rightarrow \quad Z_n \rightsquigarrow Z
                \end{equation*}
            \end{proof}

            \vspace{1em}
            \noindent
            \textbf{Example}. CLT implies that $Z_n = \sqrt{n}(\bar{X}_n - \mu) / \sigma $ approximately follows $N(0, 1)$. However, we rarely know $\sigma$. Instead, we can estimate $\sigma^2$ from i.i.d samples $X_1, \dots, X_n$ by
            $$ S_n^2 = \frac{1}{n-1} \sum_{i=1}^{n} (X_i - \bar{X}_n)^2 $$
            Suppose the assumptions in CLT hold, prove the following
            \begin{itemize}
                \item $S_n^2 \overset{P}{\rightarrow} \sigma^2$

                \item $\sqrt{n}(\bar{X}_n - \mu) / S_n \rightsquigarrow N(0, 1)$ 
            \end{itemize}

            \begin{proof}
                For the first statement, notice that by CLT, we have
                $$ \frac{1}{n} \sum_{i=1}^{n} X_i \overset{P}{\rightarrow} \mathbb{E}[X] $$
                $$ \frac{1}{n} \sum_{i=1}^{n} X_i^2 \overset{P}{\rightarrow} \mathbb{E}[X^2] $$
                Therefore, we can utilize the continuous mapping theorem (or Slutsky's theorem) and get
                \begin{equation*}
                    \begin{aligned}
                        S_n^2 &= \frac{n}{n - 1} \frac{1}{n} \sum_{i=1}^{n} (X_i - \bar{X}_n)^2 \\
                        &= \frac{n}{n - 1} \left(\frac{1}{n} \sum_{i=1}^{n} X_i^2 - \left(\frac{1}{n} \sum_{i=1}^{n} X_i \right)^2 \right) \\
                        &=\frac{n}{n-1} \left( \mathbb{E}[X^2] - \mathbb{E}[X]^2 \right) \quad (\text{continous mapping theorem}) \\
                        &= \frac{n}{n - 1} \sigma^2 \rightarrow \sigma^2 \quad (\text{as } n \rightarrow \infty)
                    \end{aligned}
                \end{equation*}
                Now, the second statement can be shown trivially by Slutsky's theorem, that is
                \begin{equation*}
                    \frac{\sqrt{n} (\bar{X}_n - \mu)}{S_n} = \frac{\sqrt{n} (\bar{X}_n - \mu)}{\sigma} \frac{\sigma}{S_n} \rightsquigarrow N(0, 1)
                \end{equation*}
                by noticing that $\frac{\sqrt{n} (\bar{X}_n - \mu)}{\sigma} \rightsquigarrow N(0, 1)$ and $\sigma / S_n \overset{P}{\rightarrow} 1$.
            \end{proof}

            \begin{theorem}[Multivariate Central Limit Theorem]
                Let $X_1, \dots, X_n$ be IID random vectors where
                \begin{equation*}
                    X_i = (X_{1i} \ X_{2i} \cdots X_{ki} )^{\top}
                \end{equation*}
                with mean
                \begin{equation*}
                    \mu = (\mu_1 \ \mu_2 \cdots \mu_k )^{\top}
                    = (\mathbb{E}[X_{1i} ] \ \mathbb{E}[X_{2i} ] \cdots \mathbb{E}[X_{ki}] )^{\top}
                \end{equation*}
                and variance matrix $\Sigma$. Let
                \begin{equation*}
                    \bar{X} = (\bar{X}_1 \ \bar{X}_2 \cdots \bar{X}_k )
                \end{equation*}
                where $\bar{X}_j = n^{-1} \sum_{i=1}^{n} X_{ji}$. Then
                \begin{equation*}
                    \sqrt{n} (\bar{X} - \mu) \rightsquigarrow N(0, \Sigma)
                \end{equation*}

            \end{theorem}

            If $Y_n$ has a limiting Normal distribution then the delta method allows us to find the limiting distribution of $g(Y_n)$ where $g$ is any smooth function (differentiable).

            \begin{theorem}[The Delta Method]
                Suppose that 
                \begin{equation*}
                    \frac{\sqrt{n}(Y_n - \mu) }{\sigma} \rightsquigarrow N(0, 1)
                \end{equation*}
                and that $g$ is a differentiable function such that $g'(\mu) \neq 0$. Then
                \begin{equation*}
                    \frac{\sqrt{n}(g(Y_n) - g(\mu) )}{|g'(\mu)| \sigma} \rightsquigarrow N(0, 1)
                \end{equation*}
                In other words,
                \begin{equation}
                    Y_n \simeq N\left(\mu, \frac{\sigma^2}{n} \right) \quad \text{implies that} \quad g(Y_n) \simeq N\left(g(\mu), (g'(\mu) )^2 \frac{\sigma^2}{n} \right)
                \end{equation}
            \end{theorem}

            \begin{theorem}[The Multivariate Delta Method]
                Suppose that $Y_n = (Y_{n1}, \dots, Y_{nk})$ is a sequence of random vectors such that
                $$ \sqrt{n}(Y_n - \mu) \rightsquigarrow N(0, \Sigma) $$
                Let $g: \mathbb{R}^k \rightarrow \mathbb{R}$ and let
                \begin{equation*}
                    \nabla g(y) = \left(\frac{\partial g}{\partial y_1} \cdots \frac{\partial g }{\partial y_k } \right)^{\top}
                \end{equation*}
                Let $\nabla_{\mu}$ denote $\nabla g(y)$ evaluated at $y = \mu$ and assume that the elements of $\nabla_{\mu}$ are nonzero. Then
                \begin{equation*}
                    \sqrt{n}(g(Y_n) - g(\mu) ) \rightsquigarrow N(0, \nabla_{\mu}^{\top} \Sigma \nabla_{\mu} )
                \end{equation*}
            \end{theorem}

        % subsection convergences (end)

    % section probability_theory (end)


    \section{Concentration of Measure} % (fold)
    \label{sec:concentration_of_measure}

        \subsection{Markov Inequality} % (fold)
        \label{sub:markov_inequality}

            \begin{theorem}[Markov Inequality]
                For any nonnegative random variable $X \geq 0$
                \begin{equation}
                    P(X \geq t) \leq \frac{\mathbb{E}[X]}{t} = O\left(\frac{1}{t} \right)
                    \label{eq:Markov} 
                \end{equation}        
            \end{theorem}

            \begin{proof}
                \begin{equation*}
                    \mathbb{E}[X] = \int_{0}^{\infty} x p(x) dx \geq \int_{t}^{\infty} x p(x) dx
                    \geq t \int_{t}^{\infty} = t \mathbb{P}(X \geq t)
                \end{equation*}
            \end{proof}
            Similarily, we can apply the same calculation and get
            \begin{equation*}
                \mathbb{E}\left[(X - \mu)^k \right] \geq t^k \mathbb{P}\left( (X - \mu)^k \geq t^k \right) = t^k \mathbb{P}(|X - \mu| \geq t)
            \end{equation*}        
            that is to say
            \begin{equation}
                \mathbb{P}(|X - \mu| \geq t) \leq \frac{\mathbb{E}\left[(X - \mu)^k \right] }{t^k} 
                \label{eq:Markov-higher-moment}    
            \end{equation}
        
        % subsubsection markov_inequality (end)
        
        \subsection{Chebyshev Inequality} % (fold)
        \label{sub:chebyshev_inequality}

            \begin{theorem}[Chebyshev Inequality]
                For any random variable $X$ with variance $\sigma^2$, for any $t \geq 0$
                \begin{equation}
                    \mathbb{P}\big(|X - \mu| \geq t \sigma \big) \leq \frac{1}{t^2}
                    \label{eq:Chebyshev}
                \end{equation}        
            \end{theorem}

            \begin{proof}
                This could be obatained immediatly by choosing $k = 2$ and $t = n \sigma$ from inequality (\ref{eq:Markov-higher-moment}), that is,
                \begin{equation*}
                    \mathbb{P}\big(|X - \mu | \geq t \sigma \big) \leq \frac{\sigma^2}{t^2 \sigma^2} = \frac{1}{t^2}
                \end{equation*}
            \end{proof}
            Here is an example. Consider the average of i.i.d. random variables with mean $\mu$ and variance $\sigma^2$
            $$ \bar{X}_n = \frac{1}{n} \sum_{i=1}^{n} X_i $$
            It has mean $\mu$ and variance $\sigma^2/ n$. Applying Chebyshev inequality, we have
            \begin{equation*}
                \mathbb{P}\left(|\bar{X}_n - \mu | \geq \frac{t \sigma}{\sqrt{n}} \right) \leq \frac{1}{t^2}
            \end{equation*}
            with $0.99$ proabability ($t = 10$), the average $\bar{X}_n$ would not exceed $\mu + 10 \sigma / \sqrt{n}$. This would lead to the Weak Law of Large Numbers.

        % subsection chebyshev_inequality (end)

        \subsection{Chernoff's Methods} % (fold)
        \label{sub:chernoff_s_methods}

            \begin{theorem}[Chernoff Bound]
                Suppose the moment generating function of random variable $X$ exists, and is finite for all $|t| \leq b, b > 0$. Let $\mu = \mathbb{E}[X]$, for any $t > 0$
                \begin{equation}
                    \mathbb{P}((X - \mu) \geq u) \leq \inf_{0 \leq t \leq b} \frac{\mathbb{E}[e^{t X}] }{e^{(u + \mu)t} }
                    \label{eq:Chernoff}
                \end{equation}
            \end{theorem}

            \begin{proof}
                By Markov inequality, we have
                \begin{equation*}
                    \mathbb{P}((X - \mu) \geq u ) = \mathbb{P}\left( e^{t (X - \mu)} \geq e^{t u} \right) \leq \frac{\mathbb{E}\left[ e^{t (X - \mu)} \right] }{e^{tu} }
                \end{equation*}
                Since this bound is true for any $t$, we have
                $$ \mathbb{P}((X - \mu) \geq u) \leq \inf_{0 \leq t \leq b} \frac{\mathbb{E}[e^{t X}] }{e^{(u + \mu)t} } $$
            \end{proof}
        
            % \begin{figure}[htb]
            %     \centering
            %     \includegraphics[width=0.60\textwidth]{Pics/Chenoff.png}
            %     \caption{Comparison of Chernoff and Chebyshev}
            % \end{figure}

            \vspace{1em}
            \noindent
            \textbf{Bounded Random Variables}.

            We are going to consider the case of bounded random variables and derive the so called Hoeffding's bound for them. As we know, the bounded random variables are the special case of sub-Gaussian variables.

            \begin{lemma}[MGF of Rademacher Variables]
                The Rademacher variable is the random variable $X \in \{+1, -1\}$ with equally probability. The MGF of Rademacher variable satifies
                \begin{equation}
                    \mathbb{E}[e^{tX}] \leq e^{t^2/2}
                    \label{eq:Rademacher-tail}
                \end{equation}
            \end{lemma}

            \begin{proof}
                By definition,
                \begin{equation*}
                    \begin{aligned}
                        \mathbb{E}[e^{tX}] = \frac{1}{2}(e^t + e^{-t}) &= \frac{1}{2}\left(\sum_{k=0}^{\infty} \frac{t^k}{k!} + \sum_{k=0}^{\infty} \frac{(-t)^k}{k!} \right) \\
                        &= \sum_{k=0}^{\infty} \frac{t^{2k} }{(2k)!} \leq \sum_{k=0}^{\infty} \frac{t^{2k}}{2^k k!} = e^{t^2/2} \\
                    \end{aligned}
                \end{equation*}
            \end{proof}

            \begin{lemma}[Jensen's inequality]
                A function $g$ is convex if 
                \begin{equation*}
                    g(\alpha x + (1 - \alpha) y) \leq \alpha g(x) + (1 - \alpha) g(y)
                \end{equation*}
                for all $x, y$ and all $\alpha \in [0, 1]$; then for random variable $X$ we have
                \begin{equation*}
                    g(\mathbb{E}[X] ) \leq \mathbb{E}[g(X)]
                \end{equation*}
            \end{lemma}

            \begin{proof}
                Let $\mu = \mathbb{E}[X]$ and let $L_{\mu}(x) = a + b x$ be the tangent line for the function $g$ at $\mu$, then we have $L_{\mu}(\mu) = g(\mu)$. By convexity, we know $g(x) \geq L_{\mu}(x)$ for all $x$; thus we have
                \begin{equation*}
                    \mathbb{E}[g(X)] \geq \mathbb{E}[L_{\mu}(X)] = \mathbb{E}[a + bX] = a + b \mu = L_{\mu}(\mu) = g(\mu)
                \end{equation*}
            \end{proof}

            \begin{lemma}[MGF of Bounded Variables]
                The bounded variables is the random variable $X$ with zero mean and with support on some bounded interval $[a, b]$. The MGF of bounded variable is
                \begin{equation}
                    \mathbb{E}_X[e^{tX}] \leq \exp\left(\frac{(b-a)^2 t^2}{2} \right)
                    \label{eq:Bounded-tail}
                \end{equation}
                which in turn show that bounded random variables are $(b - a)$ sub-Gaussian.
            \end{lemma}

            \begin{proof}
                Let $X$ be a random variable with zero mean and with support on some bounded interval $[a, b]$, and (note that one can always subtract the means and get a new rv)
                $$ Y = X - \mathbb{E}[X] $$
                using Jensen's inequality and the convexity of $g(x) = e^x$, we have
                \begin{equation*}
                    \mathbb{E}_X[e^{tX}] = \mathbb{E}_{X}\left[e^{t(X - \mathbb{E}[X'])} \right] \leq \mathbb{E}_{X,X'}\left[e^{t(X - X') } \right] \\
                \end{equation*}
                now let $\varepsilon$ be a Rademacher random variable, and note that the distribution 
                $$ X - X' \overset{d}{=} X'- X \overset{d}{=} \varepsilon(X - X') $$
                so we have
                \begin{equation*}
                    \begin{aligned}
                        \mathbb{E}_{X,X'}\left[e^{t(X - X') } \right] &= \mathbb{E}_{X,X'} \left[\mathbb{E}_{\varepsilon}[ e^{\varepsilon t (X - X') } ] \right] \leq \mathbb{E}_{X,X'}\left[e^{t^2 (X - X')^2/2} \right] \leq e^{t^2 (b-a)^2 / 2}
                    \end{aligned}
                \end{equation*}
                with the notice that $X$ is bounded and $(X - X')$ is at most $(b-a)$.        
            \end{proof}
            This in turn yields the simple version of Hoeffding's bound.
                
            \vspace{1em}
            \noindent
            \textbf{Gaussian Random Variables}.

            \begin{corollary}[Gaussian Tail Bound]
                Suppose random variable $X \sim N(\mu, \sigma^2)$, the MGF of X is then $\mathbb{E}[e^{tX}] = e^{\mu t + \sigma^2 t^2/2}$. Applying the Chernoff bound, we have one-sided upper bound
                \begin{equation}
                    \mathbb{P}(X - \mu \geq u) \leq \exp\left(-\frac{u^2}{2 \sigma^2} \right)
                    \label{eq:Gaussian-tail-bound}
                \end{equation}
                and \textit{lower tail bound}
                \begin{equation*}
                    \mathbb{P}(-X + \mu \geq u) \leq \exp\left(-\frac{u^2}{2 \sigma^2} \right)
                \end{equation*}
                putting these together, we have the \textit{two-sided Gaussian tail bound}:
                \begin{equation*}
                    \mathbb{P}(|X - \mu| \geq u) \leq 2 \exp\left(- \frac{u^2}{2 \sigma^2} \right)
                \end{equation*}
            \end{corollary}

            \begin{proof}
                Suppose $X \sim N(\mu, \sigma^2)$, then the MGF of $X$ is
                \begin{equation*}
                    \begin{aligned}
                        M_X(t) = \mathbb{E}\left[e^{t X} \right] &= \frac{1}{\sqrt{2\pi \sigma^2}} \int_{-\infty}^{\infty} e^{tx} e^{-\frac{(x - \mu)^2}{2 \sigma^2} } dx \\ 
                        &= \frac{1}{\sqrt{2\pi \sigma^2}} \int_{-\infty}^{\infty} e^{- \frac{(x - (\mu + \sigma^2 t))^2 }{2\sigma^2} } e^{\frac{\sigma^2 t^2}{2} + \mu t} dx \\
                        &= e^{\mu t + \sigma^2 t^2 / 2} \\
                    \end{aligned}
                \end{equation*}
                or equivalently
                \begin{equation}
                    \mathbb{E}[e^{t(X - \mu)}] = \exp\left(\frac{\sigma^2 t^2}{2} \right)
                    \label{eq:Gaussian-tail}
                \end{equation}
                to apply the Chernoff bound we then need to compute
                \begin{equation*}
                    \inf_{t \geq 0} \ \frac{e^{\mu t + \sigma^2 t^2 / 2 } }{e^{(u + \mu)t } } = \inf_{t \geq 0} \ e^{- ut + \sigma^2 t^2 / 2 } = e^{- ut + \sigma^2 t^2 / 2 } |_{t = u / \sigma^2 } = e^{- \frac{u^2}{2 \sigma^2} }
                \end{equation*}
                therefore, we obtain one-sided upper tail bound,
                \begin{equation*}
                    \mathbb{P}(X - \mu \geq u) \leq \exp\left(-\frac{u^2}{2 \sigma^2} \right)
                \end{equation*}
            \end{proof}
            The Gaussian tail bound is much sharper than Chebyshev's inequality; consider the average of i.i.d. Gaussian random variables, $X_1, \dots, X_n \sim N(\mu, \sigma^2)$ and we construct the estimate
            \begin{equation*}
                \bar{X}_n = \frac{1}{n} \sum_{i=1}^{n} X_i
            \end{equation*}
            where $\bar{X}_n \sim N(\mu, \sigma^2/n)$, in this case, the Gaussian tail bound is
            \begin{equation*}
                \mathbb{P}\left( \big|\bar{X}_n - \mu \big| \geq t \frac{\sigma}{\sqrt{n} } \right) \leq 2 \exp\left(-\frac{t^2}{2} \right)
            \end{equation*}
            with probability $0.99$ ($t = \sqrt{2 \ln (1/0.0005) } = 3.25$), that the average $\bar{X}_n$ is within $3.25 \sigma / \sqrt{n}$. More generally, with probability at least $1 - \delta$
            \begin{itemize}
                \item Chebyshev tells us that $$ |\bar{X}_n - \mu | \leq \frac{\sigma}{\sqrt{n \sigma}} $$
                \item Chernoff tail bound tells us that $$ |\bar{X}_n - \mu | \leq \sigma \sqrt{\frac{2 \ln(2/ \delta)}{n} } $$
            \end{itemize}
            
            \vspace{1em}
            \noindent
            \textbf{Sub-Gaussian Random Variables}.
            
            \begin{corollary}[Sub-Gaussian Tail Bound]
                Formally, a random variable $X$ with mean $\mu$ is called $\sigma$-sub-Gaussian if there exists a positive number $\sigma$ such that
                \begin{equation}
                    \mathbb{E}[e^{t(X - \mu)}] \leq \exp\left(\frac{\sigma^2 t^2}{2} \right)
                    \label{eq:sub-Gaussian-tail}
                \end{equation}
                Roughly, these are random variables whose tails decay faster than a Gaussian. Similar to Gaussian tail bound, here we can derive the two-sided sub-Gaussian tail bound
                \begin{equation}
                    \mathbb{P}(|X - \mu| \geq u) \leq 2 \exp\left(-\frac{u^2}{2 \sigma^2} \right)
                    \label{eq:Sub-Gaussian-tail-bound}
                \end{equation}
            \end{corollary}   

            Now, suppose we have $n$ i.i.d. $\sigma$ sub-Gaussian random variables $X_1, X_2, \dots, X_n$, again
            $$ \bar{X}_n = \frac{1}{n} \sum_{i} X_i $$
            by independence we have
            \begin{equation*}
                \mathbb{E}\left[e^{t(\bar{X}_n - \mu)}\right] = \mathbb{E}\left[e^{\frac{t}{n}\sum_{i}(X_i - \mu) } \right] 
                = \prod_{i=1}^{n} \mathbb{E}[e^{\frac{t}{n}(X_i - \mu) } ] \leq \prod_{i=1}^{n} e^{\frac{\sigma^2 t^2}{2 n^2} } 
                = \exp\left( \frac{\sigma^2 t^2}{2n} \right)
            \end{equation*}
            alternatively, $\bar{X}_n$ is $\sigma/\sqrt{n}$ sub-Gaussian, this yields the tail bound for the average of sub-Gaussian rvs:
            \begin{equation*}
                \mathbb{P}\left( \big|\bar{X}_n - \mu \big| \geq k \frac{\sigma}{\sqrt{n}} \right) \leq 2 \exp \left(- \frac{k^2}{2} \right)
            \end{equation*}      
            
            \vspace{1em}
            \noindent
            \textbf{Exponential Random Variables}

            \begin{theorem}[Exponential Tail Bound]
                Suppose that we have $X_1, \dots, X_n$ which are each $\sigma_1, \dots, \sigma_n$ sub-Gaussian; they are not identically distributed, but using just \textit{independence}, one can verify that the average $\bar{X}_n$ is $\sigma$ sub-Gaussian where
                $$ \sigma = \frac{1}{n} \sqrt{\sum_{i=1}^{n} \sigma_i^2 } $$   
                this yields the \textit{exponential tail inequality}
                \begin{equation}
                    \mathbb{P}\left(\left|\frac{1}{n} \sum_{i=1}^{n} (X_i - \mu_i) \right| \geq t \right) \leq \exp\left(- \frac{t^2}{2 \sigma^2} \right)
                \end{equation}
                note the these random variables still need to be independent.
            \end{theorem}
             
        % subsection chernoff_s_methods (end)

        \subsection{Hoeffding's Inequality} % (fold)
        \label{sub:hoeffding_s_inequality}

            Here we use the information of bounded variable (first-order info) to bound the MGF of random variable, then we utilize the methodds of Chernoff bound.

            \begin{theorem}[Hoeffding's Inequality]
                Suppose $X_1, \dots, X_n$ are i.i.d bounded random variables, with $X_i \in [a, b]$, then the sample average, $ \bar{X}_n = \frac{1}{n} \sum_i X_i $ has the bound
                \begin{equation}
                    \mathbb{P}\left( \left|\frac{1}{n} \sum_{k=1}^{n} X_k - \mu \right| \geq t \right) \leq 2 \exp \left(- \frac{2nt^2}{(b - a)^2} \right)
                    \label{eq:Hoeffding}
                \end{equation}
            \end{theorem}
            The Hoeffding's inequality tells us that, with probability at least $1 - \delta$,
            \begin{equation}
                \left| \frac{1}{n} \sum_{k=1}^{n} X_k - \mu \right| \leq (b - a) \sqrt{\frac{\ln (2 / \delta) }{2n} }  
            \end{equation} 
            where $\hat{\mu} = \bar{X}_n$ is the sample-average estimator of mean $\mu$.

            \begin{proof}
                Suppose the random variable $X$ has mean $\mu$ and is bounded by $[a, b]$. The logarithmic moment generating function of $X$ is then
                $$ \varphi(s) = \log \mathbb{E}\left[e^{s(X - \mu)} \right] $$
                The usual derivatives of $\varphi(s)$ is then
                \begin{equation*}
                    \begin{aligned}
                        \varphi'(s) &= \frac{\mathbb{E}[(X - \mu)e^{s(X - \mu) } ] }{\mathbb{E}[e^{s(X - \mu)}]} \\
                        \varphi''(s) &= \frac{\mathbb{E}[(X - \mu)^2 e^{s(X - \mu)} ] }{\mathbb{E}[e^{s(X - \mu)} ] } - \left( \frac{\mathbb{E}[(X - \mu) e^{s(X - \mu)} ] }{\mathbb{E}[e^{s(X - \mu)} ] } \right)^2 \\
                        &= \frac{\int_{a}^{b}(x - \mu)^2 e^{s(x - \mu)} dP(x) }{\int_{a}^{b} e^{s(x - \mu)} dP(x)} - \left(\frac{\int_{a}^{b} (x - \mu) e ^{s(x - \mu)} dP(x) }{\int_{a}^{b} e^{s(x - \mu)}dP(x) } \right)^2 \\
                    \end{aligned}
                \end{equation*}
                where we assume $P(x)$ is the distribution of $X$. It is essential to notice that $\varphi''(s)$ is the varaince of some random variable $\tilde{X} \in [a, b]$ with the distribution proportional to $e^{s(x - \mu)} dP(x) $. Therefore, we can bound the variance of $\tilde{X}$ as
                \begin{equation*}
                    \var(\tilde{X}) = \inf_{\mu \in [a, b]} \mathbb{E}[\tilde{X} - \mu]^2 \leq \mathbb{E}\left[\tilde{X} - \frac{a + b}{2} \right]^2 \leq \frac{(b - a)^2}{4}
                \end{equation*}
                for any $s$ almost surely. Since $\varphi(0) = 0$ and $\varphi'(0) = 0$, the Taylor's expansion with Lagrange remainder of $\varphi(s)$ at point $s = 0$ satisfies
                \begin{equation*}
                    \varphi(s) = \varphi(0) + \frac{\varphi'(0)}{1 !} s + \frac{\varphi''(\xi) }{2 !} s^2 \leq \frac{(b - a)^2}{8} s^2 \quad \text{almost surely}
                \end{equation*}
                where $\xi \in [0, s]$. That means
                \begin{equation}
                    \mathbb{E}\left[e^{s(X - \mu)} \right] \leq \exp \left(\frac{(b - a)^2}{8} s^2 \right)
                    \label{eq:Hoeffding-lemma}
                \end{equation}
                Now we have complete the key part of the proof. Next, recall the Markov's inequality for any non-negative random $X$ and $\varepsilon > 0$,
                \begin{equation*}
                    \begin{aligned}
                        \mathbb{P}(\bar{X}_n - \mu \geq t) &= \mathbb{P}(e^{s(\bar{X}_n - \mu) } \geq e^{s t}) \\
                        &\leq \inf_s \ e^{-s t} \mathbb{E}\left[e^{s(\bar{X}_n - \mu)} \right] \\
                        &= \inf_s \ e^{- s t} \prod_{i=1}^{n} \mathbb{E}\left[e^{\frac{s}{n} (X_i - \mu)} \right] \\
                        &\leq \inf_s \ e^{- s t} \prod_{i=1}^{n} \exp\left(\frac{s^2(b - a)^2}{8n} \right) \\
                        &= \inf_s \ \exp \left(- st + \frac{s^2 (b - a)^2}{8n} \right) \\
                        &= \exp \left(- \frac{2 n t^2}{(b - a)^2} \right), \qquad s = \frac{4n t}{(b - a)^2}
                    \end{aligned}
                \end{equation*}
                Repeating this in the other direction we get
                $$ \mathbb{P}\big(|\bar{X}_n - \mu| \geq t \big) \leq 2 \exp \left(- \frac{2n t^2}{(b - a)^2} \right) $$
            \end{proof}
        
        % subsection hoeffding_s_inequality (end)

        \subsection{Bernstein's Inequality} % (fold)
        \label{sub:bernstein_s_inequality}
        
            The Hoeffding's bound depended only on the bounds of the random variable but not explicityly on the variance. The bound $b-a$, provides a (possibly loss) upper bound on the standard deviation. One might at least hope that if the random variables were bounded, and additionally had small variance, we might be able to imporve Hoeffding's bound.

            \begin{theorem}[Berstein's Inequality]
                Suppose we have $X_1, \dots, X_n$ which were i.i.d from a distribution with mean $\mu$, bounded suppport $[a, b]$, with variance $\mathbb{E}(X - \mu)^2 = \sigma^2$, then
                \begin{equation}
                    \mathbb{P}\left(\left| \frac{1}{n} \sum_{k=1}^{n} X_k - \mu \right| \geq t \right) \leq 2 \exp \left(- \frac{n t^2}{2\sigma^2 + 2(b - a)t } \right)
                    \label{eq:Bernstein}
                \end{equation}
            \end{theorem}

            The inequality implies that, with probability at least $1 -\delta$,
            $$ \left| \frac{1}{n} \sum_{k=1}^{n} X_k - \mu \right| \leq \sigma \sqrt{\frac{2 \ln (1 / \delta)}{n}} + \frac{2(b - a) \ln(1 / \delta) }{3n} $$

            \begin{proof}
                Using the Taylor's expansion of the exponential, we can bound the moment generating function of $X$ by
                \begin{equation*}
                    \begin{aligned}
                        \mathbb{E}\left[e^{s(X - \mu)} \right] &= 1 + s\mathbb{E}(X - \mu) + \sum_{k=2}^{\infty} \frac{s^k}{k!} \mathbb{E}(X - \mu)^k = 1 + \sum_{k=2}^{\infty} \frac{s^k}{k!} \mathbb{E}(X - \mu)^k \\
                        &\leq 1 + \sum_{k=2}^{\infty} \frac{s^k}{k!} \mathbb{E}\left[ |X - \mu|^{k-2} |X - \mu|^2 \right] \\
                        &\leq 1 + \sum_{k=2}^{\infty} \frac{s^k}{k!} \mathbb{E}|X - \mu|^{k-2} \sigma^2 \qquad (\text{Cauchy-Schwartz inequality}) \\
                        &= 1 + \frac{\sigma^2}{c^2} \sum_{k=2}^{\infty} \frac{s^k}{k!}, c^k \qquad \text{let } c = \mathbb{E}|X - \mu| \leq (b - a) \\
                        &= 1 + \frac{\sigma^2}{c^2}(e^{sc} - 1 - sc) \\
                        &\leq \exp \left(\frac{\sigma^2}{c^2}(e^{sc} - 1 - sc) \right) \\
                    \end{aligned}
                \end{equation*}
                With $\sigma^2 = \var(X_i)$, we have
                \begin{equation*}
                    \begin{aligned}
                        \mathbb{P}\left(\bar{X}_n - \mu \geq t \right) &= \mathbb{P}\left(e^{s(\bar{X}_n - \mu) } \geq e^{st} \right) \qquad (\text{Markov's inequality}) \\ 
                        &\leq \inf_s \ e^{-st} \mathbb{E}\left[e^{s(\bar{X}_n - \mu) } \right] \\
                        &= \inf_s e^{-st} \prod_{i=1}^{n} \mathbb{E}\left[e^{\frac{s}{n} (X_i - \mu) } \right] \\
                        &\leq \inf_s \ e^{-st} \prod_{i=1}^{n} \exp \left(\frac{\sigma^2}{c^2}\left( e^{\frac{sc}{n} } - 1 - \frac{sc}{n} \right) \right) \\
                        &= \inf_s \ \exp\left(-st + \frac{\sigma^2}{c^2} \left(n e^{\frac{sc}{n}} - n - sc \right) \right) \\ 
                        &= \exp \left(\frac{nt}{c} - \frac{nt}{c} \ln\left(1 + \frac{tc}{\sigma^2} \right) - \frac{n \sigma^2}{c^2} \ln \left(1 + \frac{tc}{\sigma^2} \right) \right) , \qquad s = \frac{n}{c} \ln \left(1 + \frac{tc}{\sigma^2} \right) \\
                        &= \exp \left(-\frac{n \sigma^2}{c^2} \Big( (1 + \alpha) \ln(1 + \alpha) - \alpha \Big) \right), \qquad \alpha = \frac{tc}{\sigma^2} \leq \frac{t(b - a)}{\sigma^2 } \\
                    \end{aligned}
                \end{equation*}
                With the knowing that 
                $$ (1 + \alpha) \ln(1 + \alpha) \geq \frac{\alpha^2}{2 + 2 \alpha / 3} $$
                we can get what we desired.
            \end{proof}

        % subsection bernstein_s_inequality (end)

        \subsection{McDiarmid's Inequality} % (fold)
        \label{sub:mcdiarmid_s_inequality}

            So far we have focused on the concentration of averages. A natural question is whether other functions of i.i.d. random variables also show exponential concentration.
            It turns out that many other functions do concentrate sharply, and roughly the main property of the function that we need is that if we change the value of one random variable the function does not change dramatically.

            \begin{theorem}[McDiarmid's Inequality]
                Suppose we have i.i.d random variables $X_1, \dots, X_n$ where each $X_i \in \mathbb{R}^n$. We have a Lipschitz function $f: \mathbb{R}^n \rightarrow \mathbb{R}$, that satisfies the property that:
                $$ |f(x_1, \dots, x_{k-1}, x_k, x_{k+1}, \dots, x_n) - f(x_1, \dots, x_{k-1}, x_k', x_{k+1}, \dots, x_n) | \leq L_k $$
                for every $x, x' \in \mathbb{R}^n$. Then for any $t \geq 0$
                \begin{equation}
                    \mathbb{P}\big(|f(X_1, \dots, X_n) - \mathbb{E}f(X_1, \dots, X_n)| \geq t \big) \leq 2 \exp \left(- \frac{2 t^2}{\sum_{k=1}^{n} L_k^2} \right)
                    \label{eq:McDiarmid}
                \end{equation}
            \end{theorem}

            \begin{proof}
                The proof generalizes Hoeffding's inequality, which corresponds to $f(x) = \frac{1}{n} \sum_{i=1}^{n} x_i$. Now, we introduce the random variables $V_k$ for $k = 1, \dots, n$,
                $$ V_k = \mathbb{E}[f(X_1, \dots, X_n) \mid X_1, \dots, X_k] - \mathbb{E}[f(X_1, \dots, X_n) \mid X_1, \dots, X_{k-1} ] $$
                By the law of iterated expectation, we have
                $$ \mathbb{E}[V_k \mid X_1, \dots, X_{k-1}] = \mathbb{E}[f(X_1, \dots, X_n) \mid X_1, \dots, X_{k-1}] - \mathbb{E}[f(X_1, \dots, X_n) \mid X_1, \dots, X_{k-1} ] = 0 $$
                and in the mean time,
                $$ \sum_{k=1}^{n} V_k = f(X_1, \dots, X_n) - \mathbb{E}f(X_1, \dots, X_n) $$
                Since $|V_k| \leq L_k$ almost surley, $V_k$ is also an bounded variable. Using the exact method as in the proof of Hoeffding's inequality, we have
                $$ \mathbb{E}\left[e^{s V_k} \right] \leq \exp \left(\frac{L_k^2}{8} s^2 \right) $$
                Then
                \begin{equation*}
                    \begin{aligned}
                        \mathbb{P}\left(\sum_{k=1}^{n} V_k \geq t \right) &= \mathbb{P}\left(e^{s \sum_{k=1}^{n} V_k} \geq e^{st} \right) \\
                        &\leq \inf_{s} \ e^{-st} \mathbb{E}\left[e^{s \sum_{k=1}^{n} V_k } \right] \qquad (\text{Markov's inequality}) \\
                        &= \inf_{s} \ e^{-st} \prod_{k=1}^{n} \mathbb{E}\left[s V_k \right] \\
                        &\leq \inf_{s} \ e^{-st} \prod_{k=1}^{n} \exp \left(\frac{L_k^2}{8} s^2 \right) \\
                        &= \inf_{s} \exp \left(-st + \frac{\sum_{k=1}^{n}L_k^2}{8} s^2 \right) \\
                        &= \exp \left(- \frac{2 t^2}{\sum_{k=1}^{n} L_k^2} \right) , \qquad s = \frac{4 t}{\sum_{k=1}^{n} L_k^2} \\
                    \end{aligned}
                \end{equation*}
            \end{proof}
        
        % subsection mcdiarmid_s_inequality (end)

        \subsection{Expectation of the Maximum} % (fold)
        \label{sub:expectation_of_the_maximum}

            \begin{theorem}[Expectation of the Maximum]
                If $Z_1, \dots, Z_n$ are (potentially dependent) random variables which are $\sigma$-sub-Gaussian, then
                $$ \mathbb{E}[\max\{Z_1 - \mathbb{E}[Z_1], \dots, Z_n - \mathbb{E}[Z_n] \} ] \leq \sqrt{2 \sigma^2 \log n} $$
                \label{thm:expectation-of-maximum}
            \end{theorem}
            \begin{proof}
                By using the Jensen's inequality for logarithm, which is concave, we have
                \begin{equation*}
                    \begin{aligned}
                        \mathbb{E}\Big[ \max \big \{Z_1 - \mathbb{E}[Z_1], \dots, Z_n - \mathbb{E}[Z_n] \big\} \Big] &\leq \frac{1}{t} \log \mathbb{E} \left[e^{t \max\{Z_1 - \mathbb{E}[Z_1], \dots, Z_n - \mathbb{E}[Z_n] \} } \right] \quad (\text{by Jensen's inequality}) \\
                        &= \frac{1}{t} \log \mathbb{E} \left[\max \left\{e^{t(Z_1 - \mathbb{E}[Z_1])}, \dots, e^{t(Z_n - \mathbb{E}[Z_n])} \right\} \right] \\
                        &\leq \frac{1}{t} \log \mathbb{E} \left[e^{t(Z_1 - \mathbb{E}[Z_1])} + \dots + e^{t(Z_n - \mathbb{E}[Z_n]) } \right] \quad (\text{bounding the max by the sum}) \\
                        &\leq \frac{1}{t} \log \left(n e^{\sigma^2 t^2 / 2} \right) = \frac{\log n}{t} + \sigma^2 \frac{t}{2} \quad (\text{by sub-Gaussian property}) \\
                    \end{aligned}
                \end{equation*}
                Since such inequality is hold for any $t \in \mathbb{R}$, then we can minimize over $t$ and get $t = \sigma^{-1} \sqrt{2 \log n}$. Thus
                \begin{equation*}
                    \mathbb{E}\Big[ \max \big \{Z_1 - \mathbb{E}[Z_1], \dots, Z_n - \mathbb{E}[Z_n] \big\} \Big] \leq \sigma \sqrt{2 \log n}
                \end{equation*}
            \end{proof}
        
        % subsection expectation_of_the_maximum (end)

    % section concentration_of_measure (end)


    \section{Concentration for Matrices} % (fold)
    \label{sec:concentration_for_matrices}

        \subsection{Matrix Analysis} % (fold)
        \label{sub:matrix_analysis}

            \subsubsection{Matrix Functions} % (fold)
            \label{ssub:matrix_functions}

                Consider a function $f: \mathbb{R} \rightarrow \mathbb{R}$. We define a map on diagonal matrices by applying the function to each diagonal entry. We then extend $f$ to a function on Hermitian matrices using the eigenvalue decomposition:
                \begin{equation}
                    f(A) := Q f(\Lambda) Q^{*}   
                    \label{eq:matrix-function}             
                \end{equation}
                where $A = Q \Lambda Q^*$. The \textit{spectral mapping theorem} states that each eigenvalue of $f(A)$ is equal to $f(\lambda)$ for some eigenvalue $\lambda$ of $A$. This point is obvious from our definition.

                Standard inequalities for real functions typically do not have parallel versions that hold for the semi-definite ordering. Nevertheless, there is one type of relation for real functions that always extends to the semi-definite setting.
                \begin{equation}
                    f(a) \leq g(a) \quad \forall \ a \in I \quad \Longrightarrow \quad f(A) \preceq g(A)
                    \label{eq:transfer-rule}
                \end{equation}
                when the eigenvalues of $A$ lie in $I$. We sometimes refer to this as the \textit{transfer rule}. 
            
            % subsubsection matrix_functions (end)

            \subsubsection{Matrix Exponential} % (fold)
            \label{ssub:matrix_exponential}

                The exponential of an Hermitian matrix $A$ can be defined by applying (\ref{eq:matrix-function}) with the function $f(x) = e^x$. Alternatively, we may use the power series expansion
                \begin{equation*}
                    \exp(A) := I + \sum_{p=1}^{\infty} \frac{A^p}{p!}
                \end{equation*}
                The exponential of an Hermitian matrix $H$ is always \textbf{positive definite} from the spectral mapping theorem. Here is a sketch proof,
                \begin{equation}
                    x^{\top} e^{H} x = x^{\top} e^{H/2} e^{H/2} x = \left(e^{H/2} x \right)^{\top} \left(e^{H/2} x \right) = \left|\left| e^{H/2} x \right|\right|_2^2 \geq 0 
                \end{equation}
                because of the eigenvalue decomposition of Hermitian matrix.
                On account of the transfer rule (\ref{eq:transfer-rule}), the matrix exponential satisfies some simple semidefinite relations that we collect here. For each Hermitian matrix $A$, it holds that
                \begin{equation}
                    \begin{aligned}
                        I + A &\preceq e^{A} \\
                        \cosh(A) &\preceq e^{A^2 / 2} \\
                    \end{aligned}
                    \label{eq:matrix-exponential}
                \end{equation}

                We often work with the trace of the matrix exponential, $\tr \exp: A \mapsto \tr e^A$. The trace exponential function is \textbf{convex}. It is also monotone with respect to the semi-definite order:
                \begin{equation}
                    A \preceq H \quad \Rightarrow \quad \tr e^A \leq \tr e^H
                    \label{eq:monotone-trace-exponential}
                \end{equation}

                The matrix exponential does not convert sums into products, but the trace exponential has a related property that serves as a limited substitue. The Golden-Thompson inequality states that
                \begin{equation}
                    \tr e^{A + H} \leq \tr \left(e^A e^H \right) \quad \text{for all Hermitian} \ A, H
                    \label{eq:Golden-Thompson}
                \end{equation}
                The obvious generalization of the bound (\ref{eq:Golden-Thompson}) to three matrices is \textbf{false}. The pperator monotone functions and operator convex functions are depressingly rare. In particular, the matrix exponential does not belong to either.
            
            % subsubsection matrix_exponential (end)

            \subsubsection{Matrix Logarithm} % (fold)
            \label{ssub:matrix_logarithm}

                We define the matrix logarithm as the functional inverse of the matrix exponential
                \begin{equation*}
                    \log e^A := A \quad \text{for all Hermitian} \ A
                \end{equation*}
                This formular determines the logarithm on the positive definite cone, which is adequate for our purposes.

                The matrix logarithm interacts beautifully with the semidefinite order. Indeed, the logarithm is opeartor monotone:
                \begin{equation}
                    0 \prec A \preceq H \quad \Longrightarrow \quad \log(A) \preceq \log(H)
                \end{equation}
                The logarithm is also operator concave:
                \begin{equation}
                    \alpha \log A + (1 - \alpha) \log H \preceq \log(\alpha A + (1 - \alpha) H )
                \end{equation}
                for all PD $A, H$ and $\alpha \in [0, 1]$.
            
            % subsubsection matrix_logarithm (end)

            \subsubsection{Expectation and the Semidefinite Order} % (fold)
            \label{ssub:expectation_and_the_semidefinite_order}

                Since the expectation of a random matrix can be viewed as a convex combination and the PSD cone is convex, expectation preserves the semi-definite order:
                \begin{equation}
                    X \preceq Y \ \text{almost surely} \quad \Longrightarrow \quad \mathbb{E}[X] \preceq \mathbb{E}[Y]
                \end{equation}
                
                Every operator convex function admits an operator Jensen's inequality. In particular, the matrix square is operator convex, which implies that
                \begin{equation}
                    (\mathbb{E} X)^2 \preceq \mathbb{E} X^2
                \end{equation}

            % subsubsection expectation_and_the_semidefinite_order (end)

            \subsubsection{Matrix Martingales} % (fold)
            \label{ssub:matrix_martingales}

                Let $(\Omega, \mathcal{F}, \mathbb{P})$ be a master probability space. Consider a filtration $\{\mathcal{F}_k\}$ contained in the master sigma algebra:
                $$ \mathcal{F}_0 \subset \mathcal{F}_1 \subset \cdots \subset \mathcal{F}_{\infty} \subset \mathcal{F} $$
                Given such a filtration, we define the conditional expectation $\mathbb{E}_k[\cdot] := \mathbb{E}[\cdot \mid \mathcal{F}_k]$. A sequence of $\{X_k\}$ of random matrices is adapated to the filtration when each $X_k$ is measurable with respect to $\mathcal{F}_k$. Loosely speaking, an adapted sequence is one where the present depends only upon the past.

                An adapted sequence $\{Y_k\}$ of Hermitian matrices is called a matrix martingale when
                $$ \mathbb{E}_{k-1}[Y_k] = Y_{k-1} \quad \text{and} \quad \mathbb{E}||Y_k|| < \infty \quad \text{for} \quad k = 1, 2, 3, \dots $$
                We obtain a scalar martingale if we track any fixed coordinate of a matrix martingale $\{Y_k\}$. Given a matrix martingale $\{Y_k\}$, we can construct the difference sequence
                $$ X_k := Y_k - Y_{k-1} \quad \text{for} \quad k = 1,2,3 $$
                Note that the difference sequence is conditionally zero mean, $\mathbb{E}_{k-1} X_k = 0$.
            
            % subsubsection matrix_martingales (end)
        
        % subsection matrix_analysis (end)

        \subsection{Tail Bounds via the Matrix Laplace Transform Method} % (fold)
        \label{sub:tail_bounds_via_the_matrix_laplace_transform_method}
        
            \subsubsection{Matrix Moments and Cumulants} % (fold)
            \label{ssub:matrix_moments_and_cumulants}

                Consider a random Hermitian matrix $X$ that has momens of all orders. By analogy with the classical scalar definitions, we may construct matrix extensions of the moment-generating function (MGF) and the cumulant-generating function (CGF):
                \begin{equation}
                    M_X(\theta) := \mathbb{E} e^{\theta X} \quad \text{and} \quad C_X(\theta) := \log\left(\mathbb{E} e^{\theta X} \right)
                \end{equation}
                We admit the possibility that these expectations do not exist for all value of $\theta$. The matrix MGF and CGF have formal power series expansions:
                \begin{equation*}
                    M_{X}(\theta) = I + \sum_{p=1}^{\infty} \frac{\theta^p}{p!} \cdot \mathbb{E}[X^p] \quad \text{and} \quad C_X(\theta) = \sum_{p=1}^{\infty} \frac{\theta^p}{p!} \cdot \Psi_p
                \end{equation*}
                The coefficients $\mathbb{E}[X^p]$ are caled matrix moments, and we refer to $\Psi_p$ as a matrix cumulant. The first cumulant is the mean and the second cumulant is the variance
                $$ \Psi_1 = \mathbb{E}[X] \quad \text{and} \quad \Psi_2 = \mathbb{E}[X^2] - (\mathbb{E} X)^2 $$
                Higher-order cumulants are harder to write down and interpret.
            
            % subsubsection matrix_moments_and_cumulants (end)

            \subsubsection{Laplace Transform Method} % (fold)
            \label{ssub:laplace_transform_method}
            
                \begin{proposition}[The Laplace Transform Method]
                    Let $Y$ be a random Hermitian matrix. Then for all $t \in \mathbb{R}$,
                    \begin{equation}
                        \mathbb{P}\left(\lambda_{\max}(Y) \geq t\right) \leq \inf_{\theta > 0} \left\{ e^{-\theta t} \cdot \mathbb{E}\left[\tr e^{\theta Y} \right] \right\}
                    \end{equation}
                    \label{prop:Laplace-transform}
                \end{proposition}
                In words, we can control tail probabilities for the maximum eigenvalue of a random matrix by producing a bound for the trace of the matrix MGF.
                \begin{proof}
                    Fix a positive number $\theta$, we have the chain of relations
                    \begin{equation*}
                        \mathbb{P}(\lambda_{\max} \geq t) = \mathbb{P}(\lambda_{\max}(\theta Y) \geq \theta t) = \mathbb{P} \left(e^{\lambda_{\max}(\theta Y) } \geq e^{\theta t} \right) \leq e^{- \theta t} \cdot \mathbb{E} e^{\lambda_{\max}(\theta Y)}
                    \end{equation*}
                    The first identity uses the homogeneity of the maximum eigenvalue map, and the second relies on the monotonicity of the scalar exponential function; the third relation is Markov's inequality. To bound the exponential, note that
                    \begin{equation*}
                        e^{\lambda_{\max}(\theta Y)} = \lambda_{\max}\left(e^{\theta Y}\right) \leq \tr e^{\theta Y}
                    \end{equation*}
                    The identity is the spectral mapping theorem; the inequality holds because the exponential of an Hermitian matrix is positive definite and the maximum eigenvalue of a positive definitie matrix is dominated by the trace. Combine the latter two relations to reach
                    \begin{equation*}
                        \mathbb{P}(\lambda_{\max}(Y) \geq t) \leq e^{-\theta t} \cdot \mathbb{E}\left[\tr e^{\theta Y} \right]
                    \end{equation*}
                    This inequality holds for any positive $\theta$, wo we take an infimum to complete the proof.
                \end{proof}

            % subsubsection laplace_transform_method (end)

            \subsubsection{Failure of the Matrix MGF} % (fold)
            \label{ssub:failure_of_the_matrix_mgf}

                In the scalar setting, the Laplace transform method is very effective for studying sums of independent random variables because the MGF decomposes. Consider an independent sequence $\{Z_k\}$ of real random variables. We see that the scalar MGF of the sum satisfies a multiplication rule
                \begin{equation}
                    M_{\sum X_k}(\theta) = \mathbb{E} \exp \left(\sum_k \theta X_k \right) = \mathbb{E} \prod_k e^{\theta X_k} = \prod_k \mathbb{E} e^{\theta X_k} = \prod_k M_{X_k} (\theta)
                    \label{eq:scalar-mgf-multiplication}
                \end{equation}
                This calculation relies on the fact that the scalar exponential function converts sums to products, a property th matrix exponential does not share. As a consequence, there is no immediate analog of (\ref{eq:scalar-mgf-multiplication}) in the matrix setting
            
            % subsubsection failure_of_the_matrix_mgf (end)

            \subsubsection{A Concave Trace Function} % (fold)
            \label{ssub:a_concave_trace_function}

                \begin{theorem}[Lieb]
                    Fix a Hermitian matrix  $H$. The function
                    $$ f: A \mapsto \tr \exp (H + \log A) $$
                    is \textbf{concave} on the positive definite cone.
                \end{theorem}

                We require a simple but powerful corollary of Lieb's theorem. This result describes how expectation interacts with the trace exponential.

                \begin{corollary}
                    Let $H$ be a fixed Hermitian matrix, and let $X$ be a random Hermitian matrix. Then
                    $$ \mathbb{E}\left[\tr \exp (H + X) \right] \leq \tr \exp\left(H + \log\left(\mathbb{E} e^X \right) \right) $$
                    \label{col:concave-trace-exponential}
                \end{corollary}

                \begin{proof}
                    Define the random matrix $Y = e^{X}$, and calculate that 
                    $$ \mathbb{E}[ \tr \exp (H + X)] = \mathbb{E}[\tr \exp (H + \log Y)] \leq \tr \exp(H + \log(\mathbb{E} Y) ) = \tr \exp\left(H + \log \left(\mathbb{E} e^X \right) \right) $$
                    The first identity follows from the definition of the matrix logarithm because $Y$ is always PD. Lieb's result, ensures that the trace function is concave in $Y$, so we may invoke Jensen's inequality to draw the expectation inside the logarithm.
                \end{proof}
            
            % subsubsection a_concave_trace_function (end)

            \subsubsection{Subadditivity of the Matrix CGF} % (fold)
            \label{ssub:subadditivity_of_the_matrix_cgf}
            
                Although the multiplication rule (\ref{eq:scalar-mgf-multiplication}) of MGF is a dead end in the matrix case, the scalar CGF has a related property that submits to generalization.
                For an independent family $\{X_k\}$ or real random variables, the scalar CGF is additive:
                \begin{equation}
                    C_{\sum_K X_k}(\theta) = \log \mathbb{E} \exp\left(\sum_{k} \theta X_K \right) = \sum_k \log \mathbb{E} e^{\theta X_k} = \sum_{k} C_{X_k}(\theta)
                    \label{eq:scalar-cgf-multiplication}
                \end{equation}
                where the second identity comes from (\ref{eq:scalar-mgf-multiplication}) when take logarithms. 

                One key insight is that Corollary \ref{col:concave-trace-exponential} offers a completely satisfactory way to extend the addition rule (\ref{eq:scalar-cgf-multiplication}) for scalar CGF's the matrix setting. We have the following result.

                \begin{lemma}[Subadditiveity of Matrix CGF's]
                    Consider a finite sequence $\{X_k\}$ of independent, random, Hermitian matrices. Then
                    \begin{equation}
                        \mathbb{E}\left[ \tr \exp \left(\sum_{k} \theta X_k \right) \right] \leq \tr \exp \left(\sum_{k} \log \mathbb{E}\left[e^{\theta X_k} \right] \right) \quad \forall \ \theta \in \mathbb{R}
                    \end{equation}
                    \label{lm:subadditivity-matrix-cgf}
                \end{lemma}

                \begin{proof}
                    It does not harm to assume $\theta = 1$. Let $\mathbb{E}_k$ denote the expectation, conditioned on $X_1, \dots, X_k$. Abbreviate
                    $$ C_k := \log \mathbb{E}_{k-1} \left[e^{X_k} \right] = \log \mathbb{E} \left[e^{X_k} \right] = X_k $$
                    where the equality holds because the family $\{X_k\}$ is independent. We see that
                    \begin{equation*}
                        \begin{aligned}
                            \mathbb{E} \tr \exp \left(\sum_{k=1}^{n} X_k \right) &= \mathbb{E}_0 \cdots \mathbb{E}_{n-1} \tr \exp \left(\sum_{k=1}^{n-1} X_k + X_n \right) \\
                            &\leq \mathbb{E}_0 \cdots \mathbb{E}_{n-2} \tr \exp \left(\sum_{k=1}^{n} X_k + \log \mathbb{E}_{n-1} \left[e^{X_n} \right] \right) \quad \text{by Corollary \ref{col:concave-trace-exponential}} \\
                            &= \mathbb{E}_{0} \cdots \mathbb{E}_{n-2} \tr \exp \left(\sum_{k=1}^{n-2} + X_{n-1} + C_n \right) \\
                            &\leq \mathbb{E}_0 \cdots \mathbb{E}_{n-3} \tr \exp \left(\sum_{k=1}^{n-2} X_k + C_{n-1} + C_n \right) \\
                            &\cdots \\
                            &\leq \tr \exp \left(\sum_{k=1}^{n} C_k \right) \\
                        \end{aligned}
                    \end{equation*}
                \end{proof}
                To make the parallel with the addition rule (\ref{eq:scalar-cgf-multiplication}) clearer, we can rewrite the conclusion of this lemma in the form
                \begin{equation}
                    \tr \exp \left(C_{\sum_k X_K}(\theta) \right) \leq \tr \exp \left(\sum_{k} C_{X_k} (\theta) \right)
                \end{equation}
                by applyting the definition of the matrix CGF.

            % subsubsection subadditivity_of_the_matrix_cgf (end)

            \subsubsection{Tail Bounds of Independent Sums} % (fold)
            \label{ssub:tail_bounds_of_independent_sums}

                This section contains abstract tail bounds for the sum of independent random matrices. Later, we will specialize these results to some specific situations. We begin with a very general inequality, which is the progenitor of other results.

                \begin{theorem}[Master Tail Bound for Independence Sums]
                    Consider a finite sequence $\{X_k\}$ of independent, random, Hermitian matrices. For all $t \in \mathbb{R}$
                    \begin{equation}
                        \mathbb{P} \left(\lambda_{\max}\left(\sum_k X_k \right) \geq t \right) \leq \inf_{\theta \geq 0} \left\{e^{- \theta t} \cdot \tr \exp \left(\sum_k \log \mathbb{E}\left[e^{\theta X_k} \right] \right) \right\}
                        \label{eq:master-tail-bound-independent-sum}
                    \end{equation}
                \end{theorem}

                \begin{proof}
                    From Laplace transform bound, for random Hermitian matrice $Y$, we have
                    $$ \mathbb{P}(\lambda_{\max}(Y) \geq t) \leq \inf_{\theta > 0} \left\{e^{-\theta t} \cdot \mathbb{E}\left[\tr \exp(\theta Y) \right] \right\} $$
                    Notice that the sum of i.i.d random Hermitian matrices $\sum_{k} X_k$ is still a random Hermitian matrice, and hence we have
                    \begin{equation*}
                        \mathbb{P}\left( \lambda_{\max}\left(\sum_{k} X_k \right) \geq t \right) \leq \inf_{\theta > 0} \left\{ e^{-\theta t} \cdot \mathbb{E}\left[\tr \exp \left(\sum_k \theta X_k \right) \right] \right\}
                    \end{equation*}
                    By applying the subadditivity of matrix CGF in Lemma \ref{lm:subadditivity-matrix-cgf}, we have
                    \begin{equation*}
                        \mathbb{P}\left( \lambda_{\max}\left(\sum_{k} X_k \right) \geq t \right) \leq \inf_{\theta > 0} \left\{ e^{- \theta t} \cdot \tr \exp \left(\sum_k \log \mathbb{E}\left[e^{\theta X_k} \right] \right) \right\}
                    \end{equation*}
                \end{proof}

                \begin{corollary}
                    Consider a finite sequence $\{X_k \}$ of independent, random, self-adjoint matrices with dimension $d$. Assume there is a function $g: (0, \infty) \rightarrow [0, \infty]$ and a sequence $\{A_k\}$ of fixed Hermitian matrices that satisfy the relations
                    \begin{equation}
                        \mathbb{E}\left[ e^{\theta X_k} \right] \preceq e^{g(\theta) \cdot A_k}
                    \end{equation}
                    for $\theta > 0$. Define the scale parameter $\rho:= \lambda_{\max} (\sum_k A_k)$. Then for all $t \in \mathbb{R}$,
                    \begin{equation}
                        \mathbb{P}\left(\lambda_{\max} \left(\sum_k X_k \right) \geq t \right) \leq d \cdot \inf_{\theta > 0}\left\{ e^{- \theta t + g(\theta) \cdot \rho} \right\}
                    \end{equation}
                    \label{col:tail-bound-independent-sum}
                \end{corollary}

                \begin{proof}
                    The hypothesis implies that
                    \begin{equation*}
                        \log \mathbb{E} \left[e^{\theta X_k} \right] \preceq g(\theta) \cdot A_k
                    \end{equation*}
                    for $\theta > 0$ becasue of the property that the matrix logarithm is operator monotone. Recall that the trace exponential is monotone with respect to the semidefinite order, i.e.
                    $$ A \preceq H \quad \Rightarrow \quad \tr e^A \preceq \tr e^H $$
                    As a consequence, we can introduce such relation into the master inequality (\ref{eq:master-tail-bound-independent-sum}), that is, for each $\theta > 0$
                    \begin{equation*}
                        \begin{aligned}
                            \mathbb{P} \left(\lambda_{\max}\left(\sum_k X_k \right) \geq t \right) &\leq e^{- \theta t} \cdot \tr \exp \left(\sum_k \log \mathbb{E}\left[e^{\theta X_k} \right] \right) \\
                            &\leq e^{- \theta t} \cdot \tr \exp \left( g(\theta) \sum_k A_k \right) \\
                            &\leq e^{- \theta t} \cdot d \cdot \lambda_{\max}\left(\exp \left( g(\theta) \sum_k A_k \right) \right) \\
                            &= d \cdot e^{- \theta t} \cdot \exp \left(g(\theta) \cdot \lambda_{\max} \left(\sum_{k} A_k \right) \right)
                        \end{aligned}
                    \end{equation*}
                    The third inequality holds because the trace of a PD matrix, such as the exponential of matrix), is bounded by the dimension $d$ times the maximum eigenvalue. The last line depends on the spectral mapping theorem and the fact that the function $g$ is nonnegative. Identify the quantity $\rho := \lambda_{\max} (\sum_k A_k) $, and take the infimum over positive $\theta$ to reach the conclusion.
                \end{proof}
            
                \begin{remark}[Minimum Eigenvalue]
                    We can study the minimum eigenvalue of a sum of random Hermitian matrices because $\lambda_{\min}(X) = - \lambda_{\max}(-X)$. As a result
                    \begin{equation*}
                        \mathbb{P}\left(\lambda_{\min}\left(\sum_{k} X_k \right) \leq t \right) = \mathbb{P}\left(\lambda_{\max}\left(\sum_k -X_k \right) \geq - t \right)
                    \end{equation*}
                \end{remark}

                \begin{remark}[Maximum Singular Value]
                    We can also analyze the maximum singular value of a sum of random rectangular matrices $B$ by applying these results to the Hermitian dilation, that is
                    \begin{equation*}
                        \varphi(B) := 
                        \begin{bmatrix}
                            0      &  B \\
                            B^{*}  &  0 \\
                        \end{bmatrix}     
                    \end{equation*} 
                    For a finite sequence $\{Z_k\}$ of independent, random, rectangular matrices, we have
                    \begin{equation*}
                        \mathbb{P}\left( \left|\left|\sum_k Z_k \right|\right| \geq t \right) = \mathbb{P}\left(\lambda_{\max} \left(\sum_k \varphi(Z_k) \right) \geq t \right)
                    \end{equation*}
                    and the property that dilation is real-linear. This device allows us to extend most of the tail bounds in this paper to rectangular matrices.
                \end{remark}
            
            % subsubsection tail_bounds_of_independent_sums (end)

        % subsection tail_bounds_via_the_matrix_laplace_transform_method (end)

        \subsection{Matrix Gaussian and Rademacher} % (fold)
        \label{sub:matrix_gaussian_and_rademacher}
        
            We begin with the scalar case. Consider a finite sequence $\{a_k \}$ of real numbers and a finite sequence $\{\gamma_k \}$ of independent standard Gaussian variables. We have the probability inequality
            \begin{equation}
                \mathbb{P}\left(\sum_{k} a_k \gamma_k \geq t \right) \leq \exp \left(- \frac{t^2}{2 \sigma^2} \right)
            \end{equation}
            where $\sigma^2 := \sum_k a_k^2$. This result testifies that a Gaussian series with real coefficients satisfies a normal-type tail bound where the variance is controlled by the sum of the squared coefficients. The relation follows easily from the scalar Laplace transform method.

            \begin{lemma}[Rademacher and Gaussian MGF's]
                Suppose that $A$ is an Hermitian matrix. Let $\sigma$ be a Rademacher random variable, and let $\gamma$ be a standard normal random variable. Then
                \begin{equation}
                    \mathbb{E} \left[e^{\sigma \theta A}\right] \preceq e^{\theta^2 A^2 / 2} \quad \text{and} \quad \mathbb{E} \left[e^{\gamma \theta A}\right] = \mathbb{E} e^{\theta^2 A^2 / 2} \quad \theta \in \mathbb{R}
                \end{equation}
                \label{lm:Rademacher-Gaussian-MGF}
            \end{lemma}
            
            \begin{proof}[Proof of Lemma \ref{lm:Rademacher-Gaussian-MGF}]
                Absorbing $\theta$ into $A$, we may assume $\theta = 1$ in each case. We begin with the Rademacher MGF. By direct calculation,
                $$ \mathbb{E} e^{\varepsilon A} = \cosh (A) \preceq e^{A^2 / 2} $$
                where the second realtion is (\ref{eq:matrix-exponential}). For the Gaussian case, recall that the moments of a standard normal variable satisfy
                $$ \mathbb{E}[\gamma^{2p + 1} ] = 0 \quad \text{and} \quad \mathbb{E}[\gamma^{2p} ] = \frac{(2p)!}{p! 2^p} \quad p = 0, 1, 2 \dots $$
                Therefore,
                $$ \mathbb{E} e^{\gamma A} = I + \sum_{p=1}^{\infty} \frac{\mathbb{E}[\gamma^{2p}] A^{2p} }{(2p)!} = I + \sum_{p=1}^{\infty} \frac{(A^2 / 2)^p }{p!} = e^{A^2 / 2} $$           
            \end{proof}        

            \begin{theorem}[Matrix Gaussian and Rademacher Series]
               Consider a finite sequence $\{A_k\}$ of fixed (nonrandom) Hermitian matrices with dimension $d$m, and let $\{\gamma_k\}$ be a finite sequence of independent standard normal variables. Compute the variance parameter $ \sigma^2 := || \sum_k A_k^2 ||_{2} $. 
               Then, for all $t \geq 0$
               \begin{equation}
                   \mathbb{P} \left(\lambda_{\max}\left(\sum_k \gamma_k A_k \right) \geq t \right) \leq d \cdot \exp \left(- \frac{t^2}{2 \sigma^2} \right)
                   \label{eq:matrix-Gaussian-Rademacher-bound}
               \end{equation}
               In particular, 
               \begin{equation}
                   \mathbb{P} \left(\left|\left|\sum_k \gamma_k A_k \right|\right|_{2} \geq t \right) \leq 2d \cdot \exp \left(- \frac{t^2}{2 \sigma^2} \right)
                   \label{eq:matrix-Gaussian-Rademacher-norm-bound}
               \end{equation}
               The same bounds hold when we replace $\{\gamma_k\}$ by a finite sequence of independent Rademacher random variables.
               \label{thm:matrix-Gaussian-Rademacher-tail}
            \end{theorem}

            \begin{proof}[Proof of Theorem \ref{thm:matrix-Gaussian-Rademacher-tail}]
                Let $\xi_k$ be a finite sequence of independent standard normal variables or independent Rademacher variables. Invoke Lemma (\ref{lm:Rademacher-Gaussian-MGF}) to obtain
                $$ \mathbb{E} e^{\xi_k \theta A_k } \preceq e^{g(\theta) A_k^2} $$
                where $g(\theta) := \theta^2 / 2$ for $\theta > 0$. Recall that
                $$ \sigma^2 = \left|\left| \sum_k A_k^2 \right|\right| = \lambda_{\max} \left(\sum_{k} A_k^2 \right) $$
                Corollary \ref{col:tail-bound-independent-sum} delivers
                \begin{equation}
                    \mathbb{P} \left(\lambda_{\max} \left(\sum_k \xi_k A_k \right) \geq t \right) \leq d \cdot \inf_{\theta > 0} e^{- \theta t + g(\theta) \cdot \sigma^2 } = d \cdot e^{- t^2 / 2 \sigma^2}
                \end{equation}
                where the infimum is attained when $\theta = t / \sigma^2$. 

                To obtain the norm bound (\ref{eq:matrix-Gaussian-Rademacher-norm-bound}), recall that $||Y||_2 = \max(\lambda_{\max}(Y), - \lambda_{\min}(Y) )$. Standard Gaussian variables and Rademacher variables are symmetric, so the inequality above implies
                \begin{equation*}
                    \mathbb{P} \left(-\lambda_{\min} \left(\sum_k \xi_k A_k \right) \geq t \right) = \mathbb{P} \left(\lambda_{\max} \left(\sum_k (-\xi_k) A_k \right) \geq t \right) \leq d \cdot e^{- t^2 / 2 \sigma^2}
                \end{equation*}
                Apply the union bound we have
                \begin{equation*}
                    \mathbb{P} \left(\left|\left|\sum_k \gamma_k A_k \right|\right|_{2} \geq t \right) \leq \mathbb{P} \left(-\lambda_{\min} \left(\sum_k \xi_k A_k \right) \geq t \right) + \mathbb{P} \left(\lambda_{\max} \left(\sum_k (-\xi_k) A_k \right) \geq t \right) \leq 2d \cdot e^{- t^2 / 2 \sigma^2}
                \end{equation*}
            \end{proof}

        % subsection matrix_gaussian_and_rademacher (end)

        \subsection{Matrix Bennett and Bernstein Bounds} % (fold)
        \label{sub:matrix_bennett_and_bernstein_bounds}
        
            In the scalar setting, Bennett and Bernstein inequalities describe the upper tail of a sum of independent, zero-mean random variables that are either bounded or subexponential. In the matrix case, the analogous results concern a sum of zero-mean random matrices.

            \begin{lemma}[Bounded Bernstein MGF]
                Suppose that $X$ is a random Hermitian matrix that satisfies
                $$ \mathbb{E}[X] = 0 \quad \text{and} \quad \lambda_{\max}(X) \leq 1 \quad \rm{a.s.} $$
                Then we have, for $\theta > 0$,
                $$ \mathbb{E} e^{\theta X} \preceq e^{\left(e^{\theta} - \theta - 1 \right) \cdot \mathbb{E}[X^2]} $$
                \label{lm:bounded-Bernstein-MGF}
            \end{lemma}

            \begin{proof}[Proof of Lemma \ref{lm:bounded-Bernstein-MGF}]
                Fix the parameter $\theta > 0$, and define a smooth function $f$ on the real line:
                $$ f(x) = \frac{e^{\theta x} - \theta x - 1}{x^2} $$
                for $x \neq 0$ and $f(0) = \theta^2 / 2$. An exercise in differential calculus verifies that $f$ is increasing. Therefore, $f(x) \leq f(1)$ when $x \leq 1$. The eigenvalues of $X$ do not exceed one, so the transfer rule (\ref{eq:transfer-rule}) implies that
                $$ f(X) \preceq f(I) = f(1) \cdot I $$
                Expanding the matrix exponential and applying the latter relation, we discover that $$ e^{\theta X} = I + \theta X + X \cdot f(X) \cdot X $$
                To complete the proof, we take the expectation of this semidefinite bound:
                $$ \mathbb{E} e^{\theta X} \preceq I + f(1) \cdot \mathbb{E}[X^2] \preceq \exp \left( f(1) \cdot \mathbb{E}[X^2] \right) = \exp \left( \left(e^{\theta} - \theta - 1 \right) \cdot \mathbb{E}[X^2] \right) $$
                The second semidefinite relation follows from (\ref{eq:matrix-exponential}).
            \end{proof}

            \begin{theorem}[Matrix Bernstein - Bounded Case]
                Consider a finite sequence $\{X_k \}$ of independent, random, Hermitian matrices with dimension $d$. Assume that 
                $$ \mathbb{E}[X_k] = 0 \quad \text{and} \quad \lambda_{\max}(X_k) \leq R \quad \rm{a.s.} $$
                Compute the norm of the total variance, 
                $$\sigma^2 := \left|\left| \sum_k \mathbb{E}[X_k^2] \right|\right|_2 $$
                Then the following chain of inequalities holds for all $t \geq 0$:
                \begin{equation}
                    \begin{aligned}
                        \mathbb{P} \left(\lambda_{\max} \left(\sum_k X_k \right) \geq t \right) &\leq d \cdot \exp \left(- \frac{\sigma^2}{R^2} \cdot h \left(\frac{Rt}{\sigma^2} \right) \right) \quad (\text{Bennett inequality}) \\
                        &\leq d \cdot \exp \left(- \frac{t^2 / 2}{\sigma^2 + Rt / 3} \right) \quad (\text{Bernstein inequality}) \\
                        &\leq \left\{
                        \begin{aligned}
                            & d \cdot \exp(- 3t^2 / 8 \sigma^2 ), \quad \text{for} \ t \leq \sigma^2 / R \\
                            & d \cdot \exp(- 3t / 8R ), \quad \text{for} \ t \geq \sigma^2 / R \\
                        \end{aligned}
                        \right. \quad (\text{split Bernstein inequality})
                    \end{aligned}
                \end{equation}
                The function $h(u):= (1 + u) \log(1 + u) - u$ for $u \geq 0$.
                \label{thm:matrix-Bernstein-bounded}
            \end{theorem}

            \begin{proof}[Proof of Theorem \ref{thm:matrix-Bernstein-bounded}]
                We assume that $R = 1$; the general result follows by a scaling argument once we note that the summands are $1$-homogeneous and the variance $\sigma^2$ is $2$-homogeneous.

                The main challenge is to establish the Bennett inequality, part (i); the ramining bounds are consequences of simple numerical estimates. Invoke Lemma \ref{lm:bounded-Bernstein-MGF} to see that
                $$ \mathbb{E}[e^{\theta X_k} ] \preceq e^{g(\theta) \cdot \mathbb{E}[X_k^2]} $$
                where $g(\theta):= e^{\theta} - \theta - 1$ for $\theta > 0$. For each $\theta > 0$, Corollary \ref{col:tail-bound-independent-sum} implies that
                \begin{equation*}
                    \begin{aligned}
                        \mathbb{P} \left(\lambda_{\max}\left(\sum_k X_k \right) \geq t \right) &\leq d \cdot \exp \left(- \theta t + g(\theta) \cdot \lambda_{\max} \left(\sum_k \mathbb{E}[X_k^2] \right) \right) \\
                        &= d \cdot \exp \left(- \theta + g(\theta) \cdot \sigma^2 \right)
                    \end{aligned}
                \end{equation*}
                The right-hand side attains its minimal value when $\theta = \log(1 + t/\sigma^2)$. Substitute and simplify this value yields
                $$ - \theta t + g(\theta \cdot \sigma^2 ) = \sigma^2 \left(\frac{t}{\sigma^2} - \left(1 + \frac{t}{\sigma^2} \right) \log \left(1 + \frac{t}{\sigma^2} \right) \right) = - \sigma^2 h\left(\frac{t}{\sigma^2} \right) $$
                where $h(u) := (1 + u)\log(1 + u) - u$ for $u \geq 0$, which leads to the result in establish part (i)
                $$ \mathbb{P} \left(\lambda_{\max}\left(\sum_k X_k \right) \geq t \right) \leq d \cdot \exp\left(- \sigma^2 h\left(\frac{t}{\sigma^2} \right) \right) $$
                The Bennett inequality (i) implies the Bernstein inequality (ii) because of the numerical bound
                $$ h(u) \geq \frac{u^2 / 2}{1 + u/3} \quad \text{for} \quad u \geq 0 $$
                The latter relation is established by comparing derivatives. The Bernstein inequality (ii) implies the split Bernstein inequality (iii). To obtain the sub-Gaussian piece of (iii), observe that
                \begin{equation*}
                    \frac{1}{\sigma^2 + Rt / 3} \geq \frac{1}{\sigma^2 + R (\sigma^2 / R)/3 } = \frac{3}{4 \sigma^2} \quad \text{for} \quad t \leq \sigma^2/R 
                \end{equation*}
                becasue the left-hand side is a decreasing function of $t$ for $t \geq 0$. Similarily, we obatin the subexponential piece of (iii) from the fact that
                \begin{equation*}
                    \frac{t}{\sigma^2 + Rt/3} \geq \frac{\sigma^2 /R }{\sigma^2 + R (\sigma^2/R) 3} = \frac{3}{4R} \quad \text{and} \quad t \geq \sigma^2 / R
                \end{equation*}
                which holds because the left-hand side is an increasing function of $t$ for $t \geq 0$.
            \end{proof}

            \vspace{1em}
            \begin{theorem}[Matrix Bernstein - Subexponential Case]
                Consider a finite sequence $\{X_k\}$ of independent, random, Hermitian matrices with dimension $d$. Assume that
                $$ \mathbb{E} [X_k] = 0 \quad \text{and} \quad \mathbb{E}[X_k^p] \preceq \frac{p!}{2} \cdot R^{p-2} A_k^2 $$
                for $p = 2, 3, 4, \dots$. Coompute the variance parameter $\sigma^2 := ||\sum_k A_k^2||_2$. Then the following chain of inequalities holds for all $t \geq 0$:
                \begin{equation}
                    \begin{aligned}
                        \mathbb{P}\left(\lambda_{\max}\left(\sum_k X_k \right) \geq t \right) &\leq d \cdot \exp \left(- \frac{t^2/2}{\sigma^2 + Rt} \right) \\
                        &\leq \left\{
                        \begin{aligned}
                            & d \cdot \exp (- t^2 / 4 \sigma^2) \quad \text{for} \ t \leq \sigma^2/R \\
                            & d \cdot \exp (- t / 4R) \quad \text{for} \ t \geq \sigma^2 / R \\
                        \end{aligned}
                        \right.                    
                    \end{aligned}
                \end{equation}
                \label{thm:matrix-Bernstein-subexponential}
            \end{theorem}
            The hypotheses of Theorem \ref{thm:matrix-Bernstein-subexponential} are not fully comparable with the hypotheses of Theorem \ref{thm:matrix-Bernstein-bounded}, because Theorem \ref{thm:matrix-Bernstein-subexponential} allows the random matrices to be unbounded but it also demands that we control the fluctuation of the maximum and minimum eigenvalues.

        % subsection matrix_bennett_and_bernstein_bounds (end)

        \subsection{Matrix Hoeffding and Azuma and McDiarmid} % (fold)
        \label{sub:matrix_hoeffding_and_azuma_and_mcdiarmid}
        
            The scalar version of Azuma’s inequality states that a scalar martingale exhibits normal concentration about its mean value, and the scale for deviations is controlled by the total maximum squared range of the difference sequence. Here is a matrix extension.

            \begin{lemma}[Symmetrization]
                Let $H$ be a fixed Hermitian matrix, and let $X$ bea random Hermitian matrix with $\mathbb{E}[X] = 0$. Then
                \begin{equation}
                    \mathbb{E}\left[\tr e^{H + X} \right] \leq \mathbb{E}\left[\tr e^{H + 2 \sigma X} \right]
                \end{equation}
                where $\sigma$ is a Rademacher variable independent from $X$
                \label{lm:symmetrization}
            \end{lemma}

            \begin{proof}
                Consturct an independent copy $X'$ of the random matrix, and let $\mathbb{E}'$ denote integration with respect to the new variable. Since the matrix is zero mean,
                $$ \mathbb{E}\left[\tr e^{H + X} \right] = \mathbb{E}\left[\tr \ e^{H + X - \mathbb{E}'[X'] } \right] \leq \mathbb{E}\left[\tr \ e^{H + (X - X')} \right] = \mathbb{E}\left[\tr \ e^{H + \sigma(X - X')} \right] $$
                We have use the convexity of the trace exponential (both $\tr \exp(A)$ and $\tr \exp(-A)$ are convex) to justify the Jensen's inequality.
                Since $X - X'$ is a symmetric random matrix, we can modulate it by an independent Rademacher variable $\sigma$ without changing its distribution. The final bound depends on a short sequence of inequalities:
                \begin{equation*}
                    \begin{aligned}
                        \mathbb{E}\left[\tr e^{H + X} \right] &\leq \mathbb{E}\tr \left(e^{H/2 + \sigma X} \cdot e^{H/2 - \sigma X'} \right) \quad (\text{Golden–Thompson (\ref{eq:Golden-Thompson})}) \\
                        &\leq \mathbb{E}\left[ \left(\tr \ e^{H + 2 \sigma X} \right)^{1/2} \cdot \left(\tr \ e^{H - 2 \sigma X} \right)^{1/2}  \right] \quad (\text{Cauchy-Schwarz for the trace}) \\
                        &\leq \left(\mathbb{E} \left[\tr \ e^{H + 2 \sigma X} \right] \right)^{1/2} \cdot \left(\mathbb{E} \left[\tr \ e^{H + 2 \sigma X} \right] \right)^{1/2} \\ 
                        &= \mathbb{E}\left[\tr \ e^{H + 2 \sigma X} \right] \\
                    \end{aligned}
                \end{equation*}
            \end{proof}

            \begin{lemma}[Azuma CGF]
                Suppose that $X$ is a random Hermitian matrix and $A$ is fixed Hermitian matrix satisfy $X^2 \preceq A^2$. Let $\sigma$ be a Rademacher random variable independent from $X$. Then
                \begin{equation*}
                    \log \mathbb{E}\left[e^{2 \sigma \theta X} \mid X \right] \preceq 2 \theta^2 A^2 \quad \text{for} \quad \theta \in \mathbb{R}
                \end{equation*}
                \label{lm:Azuma-CGF}
            \end{lemma}

            \begin{proof}
                We apply the Rademacher MGF bound, Lemma \ref{lm:Rademacher-Gaussian-MGF}, conditionally to obtain
                $$ \mathbb{E}\left[e^{2 \theta \sigma X} \mid X \right] \preceq e^{2 \theta^2 X^2} $$
                The fact that the logarithm is operator monotone implies that
                $$ \log \mathbb{E}\left[e^{2 \theta \sigma X} \mid X \right] \preceq 2 \theta^2 X^2 \preceq 2 \theta^2 A^2 $$
                where the second realtion follows from the hypothesis on $X$.
            \end{proof}

            \begin{theorem}[Matrix Azuma]
                Consider a finite adpated sequence $\{X_k\}$ of Hermitian matrices in dimension $d$, and a fixed sequence $\{A_k\}$ of Hermitian matrices that satisfy
                $$ \mathbb{E}_{k-1}[X_k] = 0 \quad \text{and} \quad X_k^2 \preceq A_k^2 \quad \rm{a.e.} $$
                Compute the variance parameter
                $$ \sigma^2 := \left|\left| \sum_k A_k^2 \right| \right|_2 = \lambda_{\max}\left(\sum_k A_k^2 \right) $$
                Then, for all $t \geq 0$,
                \begin{equation}
                    \mathbb{P}\left(\lambda_{\max} \left(\sum_{k} X_k \right) \geq t \right) \leq d \cdot e^{- t^2 / 8 \sigma^2}
                \end{equation}
                \label{thm:matrix-Azuma}
            \end{theorem}

            \begin{proof}
                The matrix Laplace transform method, Proposition \ref{prop:Laplace-transform}, states that
                \begin{equation*}
                    \mathbb{P}\left(\lambda_{\max} \left(\sum_k X_k \right) \geq t \right) \leq \inf_{\theta > 0} \left\{ e^{-\theta t} \cdot \mathbb{E}\left[\tr \exp \left(\sum_k \theta X_k \right) \right] \right\}
                \end{equation*}
                The main difficulty in the proof is to bound the matrix MGF, which we accomplish by an iterative argument that alternates between symmetrization and cumulant bounds.

                Let us detail the first step of the iteration. Define the natural filtration $\mathcal{F}_{k} := \mathcal{F}(X_1, \dots, X_k)$ of the process $\{X_k\}$. Then we may compute
                \begin{equation*}
                    \begin{aligned}
                        \mathbb{E}\left[ \tr \exp\left(\sum_k \theta X_k \right) \right] &= \mathbb{E} \left[ \mathbb{E} \left[\tr \exp \left(\sum_{k=1}^{n-1} \theta X_k + \theta X_n \right) \Bigg| \mathcal{F}_{n-1} \right] \right] \quad (\text{iterated law}) \\
                        &\leq \mathbb{E} \left[ \mathbb{E} \left[\tr \exp \left(\sum_{k=1}^{n} \theta X_k + 2 \sigma \theta X_n \right) \Bigg| \mathcal{F}_n \right] \right] \quad (\text{symmetrization }) \\
                        &\leq \mathbb{E} \left[ \tr \exp \left( \sum_{k=1}^{n-1} \theta X_k + \log \mathbb{E}[e^{2 \sigma \theta X_n} \mid \mathcal{F}_n ] \right) \right] (\text{concavity of trace exponential}) \\
                        &\leq \mathbb{E} \left[\tr \exp \left(\sum_{k=1}^{n-1} \theta X_k + 2 \theta^2 A_n^2 \right) \right] \quad (\text{Azuma CGF}) \\
                    \end{aligned}
                \end{equation*}
                \begin{itemize}
                    \item the first identity is the tower property of the conditional expectation

                    \item in the second line, we winvoke the symmetrization method, Lemma \ref{lm:symmetrization}, conditional on $\mathcal{F}_{n-1}$, and then we relax the conditioning on the inner expectation to the larger algebra $\mathcal{F}_n$

                    \item by construction, the Rademacher variable $\sigma$ is independent from $\mathcal{F}_n$, so we can appy the concavity result, Corollary \ref{col:concave-trace-exponential}, conditional on $\mathcal{F}_n$

                    \item finally we use the fact (\ref{eq:monotone-trace-exponential}) that trace exponential is monotone to introduce the Azuma CGF bound, Lemma \ref{lm:Azuma-CGF}, in the last inequality
                \end{itemize}
                By iteration, we achieve
                \begin{equation*}
                    \mathbb{E}\left[\tr \exp \left(\sum_k \theta X_k \right) \right] \leq \tr \exp \left(2 \theta^2 \sum_k A_k^2 \right)
                \end{equation*}
                Note that this procedure relies on the fact that the sequence $\{A_k\}$ of upper bounds does not depend on the values of the random sequence $\{X_k\}$. Substitute the MGF bound into the Laplace transform bound above, and observe that the infimum is achieved when $\theta = t/4\sigma^2$, we have
                \begin{equation*}
                    \begin{aligned}
                        \mathbb{P}\left(\lambda_{\max} \left(\sum_k X_k \right) \geq t \right) &\leq \inf_{\theta > 0} \left\{ e^{-\theta t} \cdot \mathbb{E}\left[\tr \exp \left(\sum_k \theta X_k \right) \right] \right\} \\
                        &\leq \inf_{\theta > 0} \left\{e^{- \theta t} \cdot \tr \exp \left(2 \theta^2 \sum_k A_k^2 \right) \right\} \\
                        &\leq \inf_{\theta > 0} \left\{ e^{- \theta t} \cdot d \cdot \lambda_{\max} \left( \exp \left(2 \theta^2 \sum_k A_k^2 \right) \right) \right\} \\
                        &= d \cdot \inf_{\theta > 0} \left\{e^{- \theta t} e^{2 \theta^2 \sigma^2} \right\} \\
                        &= d \cdot e^{- \frac{t^2 }{8 \sigma^2} } \\
                    \end{aligned}
                \end{equation*}
            \end{proof}

        % subsection matrix_hoeffding_and_azuma_and_mcdiarmid (end)

    % section concentration_for_matrices (end)

\end{appendices}

\end{document}
