\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand*\HyPL@Entry[1]{}
\abx@aux@refcontext{nyt/global//global/global}
\HyPL@Entry{0<</S/D>>}
\abx@aux@cite{0}{bach2021learning}
\abx@aux@segm{0}{0}{bach2021learning}
\abx@aux@cite{0}{mohri2018foundations}
\abx@aux@segm{0}{0}{mohri2018foundations}
\abx@aux@cite{0}{shalev2014understanding}
\abx@aux@segm{0}{0}{shalev2014understanding}
\abx@aux@cite{0}{tropp2012user}
\abx@aux@segm{0}{0}{tropp2012user}
\abx@aux@cite{0}{wainwright2019high}
\abx@aux@segm{0}{0}{wainwright2019high}
\abx@aux@page{1}{4}
\abx@aux@page{2}{4}
\abx@aux@page{3}{4}
\abx@aux@page{4}{4}
\abx@aux@page{5}{4}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction to Supervised Learning}{5}{section.1}\protected@file@percent }
\newlabel{sec:introduction_to_supervised_learning}{{1}{5}{Introduction to Supervised Learning}{section.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Decision Theory}{5}{subsection.1.1}\protected@file@percent }
\newlabel{sub:decision_theory}{{1.1}{5}{Decision Theory}{subsection.1.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.1}Loss Functions}{5}{subsubsection.1.1.1}\protected@file@percent }
\newlabel{ssub:loss_functions}{{1.1.1}{5}{Loss Functions}{subsubsection.1.1.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.2}Risks}{5}{subsubsection.1.1.2}\protected@file@percent }
\newlabel{ssub:risks}{{1.1.2}{5}{Risks}{subsubsection.1.1.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.3}Bayes Risk}{6}{subsubsection.1.1.3}\protected@file@percent }
\newlabel{ssub:bayes_risk}{{1.1.3}{6}{Bayes Risk}{subsubsection.1.1.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Learning from Data}{9}{subsection.1.2}\protected@file@percent }
\newlabel{sub:learning_from_data}{{1.2}{9}{Learning from Data}{subsection.1.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.1}Local Averaging Methods}{10}{subsubsection.1.2.1}\protected@file@percent }
\newlabel{ssub:local_averaging_methods}{{1.2.1}{10}{Local Averaging Methods}{subsubsection.1.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces $k$-nearest-neighbor\relax }}{10}{figure.caption.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.2}Empirical Risk Minimiation}{10}{subsubsection.1.2.2}\protected@file@percent }
\newlabel{ssub:empirical_risk_minimiation}{{1.2.2}{10}{Empirical Risk Minimiation}{subsubsection.1.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces empirical risk\relax }}{11}{figure.caption.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Statistical Learning Theory}{12}{subsection.1.3}\protected@file@percent }
\newlabel{sub:statistical_learning_theory}{{1.3}{12}{Statistical Learning Theory}{subsection.1.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.1}Measures of Performance}{12}{subsubsection.1.3.1}\protected@file@percent }
\newlabel{ssub:measures_of_performance}{{1.3.1}{12}{Measures of Performance}{subsubsection.1.3.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.2}Some Notions in Learning Problems}{13}{subsubsection.1.3.2}\protected@file@percent }
\newlabel{ssub:some_notions_in_learning_problems}{{1.3.2}{13}{Some Notions in Learning Problems}{subsubsection.1.3.2}{}}
\abx@aux@cite{0}{bach2021learning}
\abx@aux@segm{0}{0}{bach2021learning}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.3}No Free Lunch Theorems}{14}{subsubsection.1.3.3}\protected@file@percent }
\newlabel{ssub:no_free_lunch_theorems}{{1.3.3}{14}{No Free Lunch Theorems}{subsubsection.1.3.3}{}}
\abx@aux@page{6}{14}
\@writefile{toc}{\contentsline {section}{\numberline {2}Convexification of the Risk}{15}{section.2}\protected@file@percent }
\newlabel{sec:convexification_of_the_risk}{{2}{15}{Convexification of the Risk}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Convex Surrogates}{15}{subsection.2.1}\protected@file@percent }
\newlabel{sub:convex_surrogates}{{2.1}{15}{Convex Surrogates}{subsection.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Geometric Interpretation of the Support Vector Machine}{16}{subsection.2.2}\protected@file@percent }
\newlabel{sub:geometric_interpretation_of_the_support_vector_machine}{{2.2}{16}{Geometric Interpretation of the Support Vector Machine}{subsection.2.2}{}}
\newlabel{eq:svc-problem}{{2.2}{17}{Geometric Interpretation of the Support Vector Machine}{equation.2.2}{}}
\abx@aux@cite{0}{bartlett2006convexity}
\abx@aux@segm{0}{0}{bartlett2006convexity}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Conditional Surrogate Risk and Classification Calibration}{19}{subsection.2.3}\protected@file@percent }
\newlabel{sub:conditional_surrogate_risk_and_classification_calibration}{{2.3}{19}{Conditional Surrogate Risk and Classification Calibration}{subsection.2.3}{}}
\newlabel{eq:classification-calibrated}{{2.5}{19}{Conditional Surrogate Risk and Classification Calibration}{equation.2.5}{}}
\abx@aux@page{7}{19}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Classification calibration\relax }}{20}{figure.caption.6}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:classfication-calibration}{{3}{20}{Classification calibration\relax }{figure.caption.6}{}}
\newlabel{eq:calibrated-differentiate}{{2.6}{20}{Conditional Surrogate Risk and Classification Calibration}{equation.2.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Relationship between Risk and Surrogate Risk}{20}{subsection.2.4}\protected@file@percent }
\newlabel{sub:relationship_between_risk_and_surrogate_risk}{{2.4}{20}{Relationship between Risk and Surrogate Risk}{subsection.2.4}{}}
\abx@aux@cite{0}{bartlett2006convexity}
\abx@aux@segm{0}{0}{bartlett2006convexity}
\newlabel{lm:excess-risk-bound}{{2.1}{21}{}{theorem.2.1}{}}
\abx@aux@page{8}{21}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The excess risk of hinge loss and $0$-$1$ loss\relax }}{22}{figure.caption.7}\protected@file@percent }
\newlabel{fig:excess-risk-hinge-loss}{{4}{22}{The excess risk of hinge loss and $0$-$1$ loss\relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Impact on Approximation Errors}{23}{subsection.2.5}\protected@file@percent }
\newlabel{sub:impact_on_approximation_errors}{{2.5}{23}{Impact on Approximation Errors}{subsection.2.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Empirical Risk Minimization}{24}{section.3}\protected@file@percent }
\newlabel{sec:empirical_risk_minimization}{{3}{24}{Empirical Risk Minimization}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Risk Minimization Decomposition}{24}{subsection.3.1}\protected@file@percent }
\newlabel{sub:risk_minimization_decomposition}{{3.1}{24}{Risk Minimization Decomposition}{subsection.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Approximation Error}{24}{subsection.3.2}\protected@file@percent }
\newlabel{sub:approximation_error}{{3.2}{24}{Approximation Error}{subsection.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces The distance between minimizer $\theta _{*}$ and set $\Theta $ on $\mathbb  {R}$\relax }}{25}{figure.caption.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Estimation Error}{25}{subsection.3.3}\protected@file@percent }
\newlabel{sub:estimation_error}{{3.3}{25}{Estimation Error}{subsection.3.3}{}}
\newlabel{eq:estimation-error}{{3.3}{25}{Estimation Error}{equation.3.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.1}Uniform Deviation from Expectation}{26}{subsubsection.3.3.1}\protected@file@percent }
\newlabel{ssub:uniform_deviation_from_expectation}{{3.3.1}{26}{Uniform Deviation from Expectation}{subsubsection.3.3.1}{}}
\newlabel{eq:expected-unifrom-deviation}{{3.4}{26}{Uniform Deviation from Expectation}{equation.3.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.2}Linear Hypothesis Space}{26}{subsubsection.3.3.2}\protected@file@percent }
\newlabel{ssub:linear_hypothesis_space}{{3.3.2}{26}{Linear Hypothesis Space}{subsubsection.3.3.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.3}Finite Hypothesis Space}{28}{subsubsection.3.3.3}\protected@file@percent }
\newlabel{ssub:finite_hypothesis_space}{{3.3.3}{28}{Finite Hypothesis Space}{subsubsection.3.3.3}{}}
\newlabel{eq:finite-expectation-bounds}{{3.7}{29}{Finite Hypothesis Space}{equation.3.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.4}Beyond the Finite Hypothesis Space}{30}{subsubsection.3.3.4}\protected@file@percent }
\newlabel{ssub:beyond_the_finite_hypothesis_space}{{3.3.4}{30}{Beyond the Finite Hypothesis Space}{subsubsection.3.3.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces The left picture is an example in two dimensions of a covering with Euclidean balls; The right is an example of $l_{\infty }$-balls\relax }}{30}{figure.caption.9}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}PAC Learning and Uniform Convergence}{32}{section.4}\protected@file@percent }
\newlabel{sec:pac_learn_and_uniform_convergence}{{4}{32}{PAC Learning and Uniform Convergence}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}PAC Learning}{32}{subsection.4.1}\protected@file@percent }
\newlabel{sub:pac_learning}{{4.1}{32}{PAC Learning}{subsection.4.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Agnostic PAC Learning}{32}{subsection.4.2}\protected@file@percent }
\newlabel{sub:agnostic_pac_learning}{{4.2}{32}{Agnostic PAC Learning}{subsection.4.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Uniform Convergence}{33}{subsection.4.3}\protected@file@percent }
\newlabel{sub:uniform_convergence}{{4.3}{33}{Uniform Convergence}{subsection.4.3}{}}
\newlabel{eq:estimation-error-bound}{{4.1}{33}{Uniform Convergence}{equation.4.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Rademacher Complexity}{35}{section.5}\protected@file@percent }
\newlabel{sec:rademacher_complexity}{{5}{35}{Rademacher Complexity}{section.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Motivation for Rademacher Complexity}{35}{subsection.5.1}\protected@file@percent }
\newlabel{sub:motivation_for_rademacher_complexity}{{5.1}{35}{Motivation for Rademacher Complexity}{subsection.5.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Rademacher Complexity}{36}{subsection.5.2}\protected@file@percent }
\newlabel{sub:rademacher_complexity}{{5.2}{36}{Rademacher Complexity}{subsection.5.2}{}}
\newlabel{eq:empirical-Rademacher-complexity}{{5.4}{36}{Empirical Rademacher Complexity}{equation.5.4}{}}
\newlabel{eq:Rademacher-complexity}{{5.5}{36}{Rademacher Complexity}{equation.5.5}{}}
\newlabel{thm:symmetrization-Rademacher-complexity}{{5.1}{37}{Symmetrization}{equation.5.7}{}}
\newlabel{eq:empirical-process-bound-Rademacher-complexity}{{5.8}{37}{Empirical Process bonud via Rademacher Complexity}{equation.5.8}{}}
\newlabel{thm:empirical-process-bound-Radeamcher-complexity}{{5.2}{37}{Empirical Process bonud via Rademacher Complexity}{equation.5.8}{}}
\newlabel{eq:generlization-bound-Rademacher-complexity}{{5.9}{40}{Generalization Bonud via Rademacher Complexity}{equation.5.9}{}}
\newlabel{thm:generalization-bound-Radeamcher-complexity}{{5.3}{40}{Generalization Bonud via Rademacher Complexity}{equation.5.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Lipschitz-continuous Losses}{40}{subsection.5.3}\protected@file@percent }
\newlabel{sub:lipschitz_continuous_losses}{{5.3}{40}{Lipschitz-continuous Losses}{subsection.5.3}{}}
\newlabel{eq:Rademacher-complexity-of-hypothesis-class}{{5.12}{41}{Lipschitz-continuous Losses}{equation.5.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Ball-constrained Linear Predictions}{41}{subsection.5.4}\protected@file@percent }
\newlabel{sub:ball_constrained_linear_predictions}{{5.4}{41}{Ball-constrained Linear Predictions}{subsection.5.4}{}}
\newlabel{eq:Rademacher-complexity-of-function-class}{{5.14}{42}{Ball-constrained Linear Predictions}{equation.5.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5}Putting Things Together (Linear Predictions)}{42}{subsection.5.5}\protected@file@percent }
\newlabel{sub:putting_things_together_}{{5.5}{42}{Putting Things Together (Linear Predictions)}{subsection.5.5}{}}
\abx@aux@cite{0}{sridharan2008fast}
\abx@aux@segm{0}{0}{sridharan2008fast}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6}From Constrained to Regularized Estimation}{43}{subsection.5.6}\protected@file@percent }
\newlabel{sub:from_constrained_to_regularized_estimation}{{5.6}{43}{From Constrained to Regularized Estimation}{subsection.5.6}{}}
\newlabel{eq:regularized-empirical-risk}{{5.16}{43}{From Constrained to Regularized Estimation}{equation.5.16}{}}
\abx@aux@page{9}{43}
\@writefile{toc}{\contentsline {section}{\numberline {6}Growth Function and VC-Dimension}{44}{section.6}\protected@file@percent }
\newlabel{sec:growth_function_and_vc_dimension}{{6}{44}{Growth Function and VC-Dimension}{section.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Growth Function}{44}{subsection.6.1}\protected@file@percent }
\newlabel{sub:growth_function}{{6.1}{44}{Growth Function}{subsection.6.1}{}}
\newlabel{eq:growth-function}{{6.1}{44}{Growth Function}{equation.6.1}{}}
\newlabel{thm:Massart-lemma}{{6.1}{44}{Massart's Lemma}{equation.6.2}{}}
\newlabel{eq:generalization-bound-growth-function}{{6.5}{46}{Generalization Bound via Growth Function}{equation.6.5}{}}
\newlabel{thm:generalization-bound-growth-function}{{6.3}{46}{Generalization Bound via Growth Function}{equation.6.5}{}}
\newlabel{eq:direct-generalization-bound-growth-function}{{6.6}{46}{Growth Function}{equation.6.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}VC-dimension}{46}{subsection.6.2}\protected@file@percent }
\newlabel{sub:vc_dimension}{{6.2}{46}{VC-dimension}{subsection.6.2}{}}
\newlabel{eq:VC-dimension}{{6.7}{46}{VC-dimension}{equation.6.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces An example of a sine function used for classification\relax }}{48}{figure.caption.10}\protected@file@percent }
\newlabel{fig:sine-function}{{7}{48}{An example of a sine function used for classification\relax }{figure.caption.10}{}}
\newlabel{thm:Radon-thm}{{6.4}{48}{Radon's theorem}{theorem.6.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Link Growth Function and VC-dimension}{49}{subsection.6.3}\protected@file@percent }
\newlabel{sub:link_growth_function_and_vc_dimension}{{6.3}{49}{Link Growth Function and VC-dimension}{subsection.6.3}{}}
\newlabel{eq:generalization-bound-VC-dimension}{{6.11}{49}{Generalization Bounds via VC-dimension}{equation.6.11}{}}
\abx@aux@cite{0}{mohri2018foundations}
\abx@aux@segm{0}{0}{mohri2018foundations}
\newlabel{thm:generalization-bound-VC-dimension}{{6.7}{50}{Generalization Bounds via VC-dimension}{equation.6.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4}Lower Bounds}{50}{subsection.6.4}\protected@file@percent }
\newlabel{sub:lower_bounds}{{6.4}{50}{Lower Bounds}{subsection.6.4}{}}
\abx@aux@page{10}{50}
\@writefile{toc}{\contentsline {section}{\numberline {7}Covering Number and Chaining}{51}{section.7}\protected@file@percent }
\newlabel{sec:covering_number_and_chaining}{{7}{51}{Covering Number and Chaining}{section.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Covering and Packing}{51}{subsection.7.1}\protected@file@percent }
\newlabel{sub:covering_and_packing}{{7.1}{51}{Covering and Packing}{subsection.7.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Covering and packing sets\relax }}{52}{figure.caption.11}\protected@file@percent }
\newlabel{fig:covering-packing}{{8}{52}{Covering and packing sets\relax }{figure.caption.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Bound Rademacher Complexity via Covering Number}{53}{subsection.7.2}\protected@file@percent }
\newlabel{sub:bound_rademacher_complexity_via_covering_number}{{7.2}{53}{Bound Rademacher Complexity via Covering Number}{subsection.7.2}{}}
\newlabel{thm:covering-bound-Rademacher-complexity}{{7.3}{54}{Bounding Rademacher Complexity via Covering Number}{equation.7.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3}Chaining}{55}{subsection.7.3}\protected@file@percent }
\newlabel{sub:chaining}{{7.3}{55}{Chaining}{subsection.7.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Optimization for Machine Learning}{58}{section.8}\protected@file@percent }
\newlabel{sec:optimization_for_machine_learning}{{8}{58}{Optimization for Machine Learning}{section.8}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Above, $L$ is the smoothness constant, $\mu $ the strong convexity constant, $B$ the Lipschitz constant and $D$ the distance to optimum at initialization\relax }}{58}{table.caption.12}\protected@file@percent }
\newlabel{tab:optimization-algo-rate}{{1}{58}{Above, $L$ is the smoothness constant, $\mu $ the strong convexity constant, $B$ the Lipschitz constant and $D$ the distance to optimum at initialization\relax }{table.caption.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1}Optimization in Machine Learning}{58}{subsection.8.1}\protected@file@percent }
\newlabel{sub:optimization_in_machine_learning}{{8.1}{58}{Optimization in Machine Learning}{subsection.8.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2}Gradient Descent on Smooth Problems}{58}{subsection.8.2}\protected@file@percent }
\newlabel{sub:gradient_descent_on_smooth_problems}{{8.2}{58}{Gradient Descent on Smooth Problems}{subsection.8.2}{}}
\newlabel{eq:gradient-descent}{{8.2}{59}{Gradient Descent on Smooth Problems}{equation.8.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.2.1}Analysis of GD for ordinary least squares}{59}{subsubsection.8.2.1}\protected@file@percent }
\newlabel{ssub:analysis_of_gd_for_ordinary_least_squares}{{8.2.1}{59}{Analysis of GD for ordinary least squares}{subsubsection.8.2.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.2.2}Analysis of GD for strongly and smooth functions}{60}{subsubsection.8.2.2}\protected@file@percent }
\newlabel{ssub:analysis_of_gd_for_strongly_and_smooth_functions}{{8.2.2}{60}{Analysis of GD for strongly and smooth functions}{subsubsection.8.2.2}{}}
\newlabel{eq:convexity}{{8.4}{60}{Convex function}{equation.8.4}{}}
\newlabel{eq:strong-convexity}{{8.5}{60}{Strong convexity}{equation.8.5}{}}
\newlabel{lm:lojasiewics-inequality}{{8.1}{60}{Lojasiewics inequality}{theorem.8.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Strong convexity\relax }}{61}{figure.caption.13}\protected@file@percent }
\newlabel{fig:strongly-convex}{{9}{61}{Strong convexity\relax }{figure.caption.13}{}}
\newlabel{eq:smoothness}{{8.6}{61}{Smoothness}{equation.8.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces $L$-smoothness\relax }}{62}{figure.caption.14}\protected@file@percent }
\newlabel{fig:smoothness}{{10}{62}{$L$-smoothness\relax }{figure.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces When a function is both smooth and strongly convex, we denote by $\kappa = L/\mu \geq 1$ its condition number, and the condition number impacts the shapes of the level sets\relax }}{62}{figure.caption.15}\protected@file@percent }
\newlabel{fig:condition-number}{{11}{62}{When a function is both smooth and strongly convex, we denote by $\kappa = L/\mu \geq 1$ its condition number, and the condition number impacts the shapes of the level sets\relax }{figure.caption.15}{}}
\newlabel{eq:iteration-inequality}{{8.7}{62}{Analysis of GD for strongly and smooth functions}{equation.8.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces The performance of gradient descent will depend on this condition number (see the steepest descent, i.e., gradient descent with exact line search): with small condition number (left), we get fast convergence, while for a large condition number (right), we get oscillations\relax }}{63}{figure.caption.16}\protected@file@percent }
\newlabel{fig:condition-number-steepest}{{12}{63}{The performance of gradient descent will depend on this condition number (see the steepest descent, i.e., gradient descent with exact line search): with small condition number (left), we get fast convergence, while for a large condition number (right), we get oscillations\relax }{figure.caption.16}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.2.3}Analysis of GD for convex and smooth functions}{63}{subsubsection.8.2.3}\protected@file@percent }
\newlabel{ssub:analysis_of_gd_for_convex_and_smooth_functions}{{8.2.3}{63}{Analysis of GD for convex and smooth functions}{subsubsection.8.2.3}{}}
\abx@aux@cite{0}{bach2021learning}
\abx@aux@segm{0}{0}{bach2021learning}
\abx@aux@cite{0}{nesterov2018lectures}
\abx@aux@segm{0}{0}{nesterov2018lectures}
\abx@aux@page{11}{64}
\abx@aux@cite{0}{boyd2004convex}
\abx@aux@segm{0}{0}{boyd2004convex}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.2.4}Beyond Gradient Descent}{65}{subsubsection.8.2.4}\protected@file@percent }
\newlabel{ssub:beyond_gradient_descent}{{8.2.4}{65}{Beyond Gradient Descent}{subsubsection.8.2.4}{}}
\abx@aux@page{12}{65}
\abx@aux@page{13}{65}
\abx@aux@cite{0}{nesterov2013gradient}
\abx@aux@segm{0}{0}{nesterov2013gradient}
\abx@aux@page{14}{66}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3}Gradient Methods on Non-smooth Problems}{66}{subsection.8.3}\protected@file@percent }
\newlabel{sub:gradient_methods_on_non_smooth_problems}{{8.3}{66}{Gradient Methods on Non-smooth Problems}{subsection.8.3}{}}
\abx@aux@cite{0}{bach2021learning}
\abx@aux@segm{0}{0}{bach2021learning}
\abx@aux@page{15}{68}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.4}Stochastic Gradient Descent}{68}{subsection.8.4}\protected@file@percent }
\newlabel{sub:stochastic_gradient_descent}{{8.4}{68}{Stochastic Gradient Descent}{subsection.8.4}{}}
\abx@aux@cite{0}{bach2021learning}
\abx@aux@segm{0}{0}{bach2021learning}
\@writefile{toc}{\contentsline {section}{\numberline {9}Kernel Methods}{69}{section.9}\protected@file@percent }
\newlabel{sec:kernel_methods}{{9}{69}{Kernel Methods}{section.9}{}}
\abx@aux@page{16}{69}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1}Motivating Example to Kernel Function}{69}{subsection.9.1}\protected@file@percent }
\newlabel{sub:motivating_example_to_kernel_function}{{9.1}{69}{Motivating Example to Kernel Function}{subsection.9.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces XOR classification problem\relax }}{69}{figure.caption.17}\protected@file@percent }
\newlabel{fig:XOR}{{13}{69}{XOR classification problem\relax }{figure.caption.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Separating hyperplane\relax }}{70}{figure.caption.18}\protected@file@percent }
\newlabel{fig:SVM}{{14}{70}{Separating hyperplane\relax }{figure.caption.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Inner product in feature space\relax }}{71}{figure.caption.19}\protected@file@percent }
\newlabel{fig:SVM-feature-space}{{15}{71}{Inner product in feature space\relax }{figure.caption.19}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2}Reproducing Kernel Hilbert Space}{72}{subsection.9.2}\protected@file@percent }
\newlabel{sub:reproducing_kernel_hilbert_space}{{9.2}{72}{Reproducing Kernel Hilbert Space}{subsection.9.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.2.1}Hilbert Space}{72}{subsubsection.9.2.1}\protected@file@percent }
\newlabel{ssub:hilbert_space}{{9.2.1}{72}{Hilbert Space}{subsubsection.9.2.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.2.2}Positive Semidefinite Kernel Functions}{73}{subsubsection.9.2.2}\protected@file@percent }
\newlabel{ssub:positive_semidefinite_kernel_functions}{{9.2.2}{73}{Positive Semidefinite Kernel Functions}{subsubsection.9.2.2}{}}
\newlabel{eq:feature-map}{{9.3}{74}{Polynomial Kernels}{equation.9.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.2.3}Constructing an RKHS from a Kernel}{74}{subsubsection.9.2.3}\protected@file@percent }
\newlabel{ssub:constructing_an_rkhs_from_a_kernel}{{9.2.3}{74}{Constructing an RKHS from a Kernel}{subsubsection.9.2.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.2.4}Alternative Way to Construct RKHS}{74}{subsubsection.9.2.4}\protected@file@percent }
\newlabel{ssub:alternative_way_to_construct_rkhs}{{9.2.4}{74}{Alternative Way to Construct RKHS}{subsubsection.9.2.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3}Algorithms}{74}{subsection.9.3}\protected@file@percent }
\newlabel{sub:algorithms}{{9.3}{74}{Algorithms}{subsection.9.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {10}Local Averaging Methods}{75}{section.10}\protected@file@percent }
\newlabel{sec:local_averaging_methods}{{10}{75}{Local Averaging Methods}{section.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1}Quick Review}{75}{subsection.10.1}\protected@file@percent }
\newlabel{sub:quick_review}{{10.1}{75}{Quick Review}{subsection.10.1}{}}
\newlabel{eq:expected-excess-risk-regression}{{10.2}{76}{Quick Review}{equation.10.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.2}Local Averaging Methods}{76}{subsection.10.2}\protected@file@percent }
\newlabel{sub:local_averaging_methods}{{10.2}{76}{Local Averaging Methods}{subsection.10.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.3}Linear Estimators}{77}{subsection.10.3}\protected@file@percent }
\newlabel{sub:linear_estimators}{{10.3}{77}{Linear Estimators}{subsection.10.3}{}}
\newlabel{eq:regression-predictor-LAM}{{10.3}{78}{Linear Estimators}{equation.10.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Weights of linear estimators in $d = 1$ estimation for three types of local averaging estimators. The $n = 8$ weight functions $\hat  {w}_i(x)$ are plotted with the observations in black.\relax }}{78}{figure.caption.20}\protected@file@percent }
\newlabel{fig:weight-functions}{{16}{78}{Weights of linear estimators in $d = 1$ estimation for three types of local averaging estimators. The $n = 8$ weight functions $\hat {w}_i(x)$ are plotted with the observations in black.\relax }{figure.caption.20}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.3.1}Partition Estimators}{78}{subsubsection.10.3.1}\protected@file@percent }
\newlabel{ssub:partition_estimators}{{10.3.1}{78}{Partition Estimators}{subsubsection.10.3.1}{}}
\newlabel{eq:partition-weight}{{10.4}{78}{Partition Estimators}{equation.10.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Regressograms in $d=1$ dimension, with three different values of $|J|$. We can see both underfitting, or overfitting in this example. Note that the target function $f^*$ is piecewise affine, and that on the affine parts, the estimator is far from linear, namely, the estimator cannot take advantage of extra-regularity.\relax }}{79}{figure.caption.21}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.3.2}Nearest-Neighbors}{80}{subsubsection.10.3.2}\protected@file@percent }
\newlabel{ssub:nearest_neighbors}{{10.3.2}{80}{Nearest-Neighbors}{subsubsection.10.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces $k$-nearest neighbor regression in $d = 1$ dimension, with three values of $k$ (the number of neighbors). We can see both underfitting ($k$ too large), and overfitting ($k$ too small).\relax }}{80}{figure.caption.22}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.3.3}Nadaraya-Watson Estimator (Kernel Regression)}{80}{subsubsection.10.3.3}\protected@file@percent }
\newlabel{ssub:nadaraya_watson_estimator_kernel_regression_}{{10.3.3}{80}{Nadaraya-Watson Estimator (Kernel Regression)}{subsubsection.10.3.3}{}}
\abx@aux@cite{0}{bach2021learning}
\abx@aux@segm{0}{0}{bach2021learning}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces Nadaraya-Watson regression in $d=1$ dimension, with three values of bandwidth $h$ for the Gaussian kernel.\relax }}{81}{figure.caption.24}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.4}Generic Consistency Analysis}{81}{subsection.10.4}\protected@file@percent }
\newlabel{sub:generic_consistency_analysis}{{10.4}{81}{Generic Consistency Analysis}{subsection.10.4}{}}
\abx@aux@page{17}{81}
\newlabel{eq:empirical-excess-risk-decomposition}{{10.5}{82}{Generic Consistency Analysis}{equation.10.5}{}}
\newlabel{eq:expected-excess-risk-decomosition}{{10.6}{83}{Generic Consistency Analysis}{equation.10.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.4.1}Fixed Partition}{83}{subsubsection.10.4.1}\protected@file@percent }
\newlabel{ssub:fixed_partition}{{10.4.1}{83}{Fixed Partition}{subsubsection.10.4.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.4.2}K-nearest Neighbors}{85}{subsubsection.10.4.2}\protected@file@percent }
\newlabel{ssub:k_nearest_neighbors}{{10.4.2}{85}{K-nearest Neighbors}{subsubsection.10.4.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces Distance to nearest neighbors\relax }}{86}{figure.caption.25}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.4.3}Kernel Regression}{87}{subsubsection.10.4.3}\protected@file@percent }
\newlabel{ssub:kernel_regression}{{10.4.3}{87}{Kernel Regression}{subsubsection.10.4.3}{}}
\abx@aux@cite{0}{ambrosio2013density}
\abx@aux@segm{0}{0}{ambrosio2013density}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.5}Universal Consistency}{89}{subsection.10.5}\protected@file@percent }
\newlabel{sub:universal_consistency}{{10.5}{89}{Universal Consistency}{subsection.10.5}{}}
\newlabel{eq:technical-assumption-LAM}{{10.11}{89}{Universal Consistency}{equation.10.11}{}}
\abx@aux@page{18}{90}
\@writefile{toc}{\contentsline {section}{\numberline {11}Sparse Methods}{92}{section.11}\protected@file@percent }
\newlabel{sec:sparse_methods}{{11}{92}{Sparse Methods}{section.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {12}Neural Networks}{93}{section.12}\protected@file@percent }
\newlabel{sec:neural_networks}{{12}{93}{Neural Networks}{section.12}{}}
\abx@aux@page{19}{94}
\abx@aux@page{20}{94}
\abx@aux@page{21}{94}
\abx@aux@page{22}{94}
\abx@aux@page{23}{94}
\abx@aux@page{24}{94}
\abx@aux@page{25}{94}
\abx@aux@page{26}{94}
\abx@aux@page{27}{94}
\abx@aux@page{28}{94}
\abx@aux@page{29}{94}
\@writefile{toc}{\contentsline {section}{Appendices}{95}{section*.28}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {A}Norms}{95}{appendix.1.A}\protected@file@percent }
\newlabel{sec:norms}{{A}{95}{Norms}{appendix.1.A}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1}Norms}{95}{subsection.1.A.1}\protected@file@percent }
\newlabel{sub:norms}{{A.1}{95}{Norms}{subsection.1.A.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2}Examples of Norm}{95}{subsection.1.A.2}\protected@file@percent }
\newlabel{sub:examples_of_norm}{{A.2}{95}{Examples of Norm}{subsection.1.A.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.3}Equivalence of Norms}{96}{subsection.1.A.3}\protected@file@percent }
\newlabel{sub:equivalence_of_norms}{{A.3}{96}{Equivalence of Norms}{subsection.1.A.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.4}Operator Norms}{96}{subsection.1.A.4}\protected@file@percent }
\newlabel{sub:operator_norms}{{A.4}{96}{Operator Norms}{subsection.1.A.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B}Probability Theory}{97}{appendix.1.B}\protected@file@percent }
\newlabel{sec:probability_theory}{{B}{97}{Probability Theory}{appendix.1.B}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.1}Independence}{97}{subsection.1.B.1}\protected@file@percent }
\newlabel{sub:independence}{{B.1}{97}{Independence}{subsection.1.B.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.2}Expectations}{99}{subsection.1.B.2}\protected@file@percent }
\newlabel{sub:expectations}{{B.2}{99}{Expectations}{subsection.1.B.2}{}}
\newlabel{def:cond-expectation}{{B.4}{99}{}{definition.1.B.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.3}Convergences}{102}{subsection.1.B.3}\protected@file@percent }
\newlabel{sub:convergences}{{B.3}{102}{Convergences}{subsection.1.B.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {C}Concentration of Measure}{107}{appendix.1.C}\protected@file@percent }
\newlabel{sec:concentration_of_measure}{{C}{107}{Concentration of Measure}{appendix.1.C}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.1}Markov Inequality}{107}{subsection.1.C.1}\protected@file@percent }
\newlabel{sub:markov_inequality}{{C.1}{107}{Markov Inequality}{subsection.1.C.1}{}}
\newlabel{eq:Markov}{{C.1}{107}{Markov Inequality}{equation.1.C.1}{}}
\newlabel{eq:Markov-higher-moment}{{C.2}{107}{Markov Inequality}{equation.1.C.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.2}Chebyshev Inequality}{107}{subsection.1.C.2}\protected@file@percent }
\newlabel{sub:chebyshev_inequality}{{C.2}{107}{Chebyshev Inequality}{subsection.1.C.2}{}}
\newlabel{eq:Chebyshev}{{C.3}{107}{Chebyshev Inequality}{equation.1.C.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.3}Chernoff's Methods}{108}{subsection.1.C.3}\protected@file@percent }
\newlabel{sub:chernoff_s_methods}{{C.3}{108}{Chernoff's Methods}{subsection.1.C.3}{}}
\newlabel{eq:Chernoff}{{C.4}{108}{Chernoff Bound}{equation.1.C.4}{}}
\newlabel{eq:Rademacher-tail}{{C.5}{108}{MGF of Rademacher Variables}{equation.1.C.5}{}}
\newlabel{eq:Bounded-tail}{{C.6}{109}{MGF of Bounded Variables}{equation.1.C.6}{}}
\newlabel{eq:Gaussian-tail-bound}{{C.7}{109}{Gaussian Tail Bound}{equation.1.C.7}{}}
\newlabel{eq:Gaussian-tail}{{C.8}{110}{Chernoff's Methods}{equation.1.C.8}{}}
\newlabel{eq:sub-Gaussian-tail}{{C.9}{110}{Sub-Gaussian Tail Bound}{equation.1.C.9}{}}
\newlabel{eq:Sub-Gaussian-tail-bound}{{C.10}{111}{Sub-Gaussian Tail Bound}{equation.1.C.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.4}Hoeffding's Inequality}{111}{subsection.1.C.4}\protected@file@percent }
\newlabel{sub:hoeffding_s_inequality}{{C.4}{111}{Hoeffding's Inequality}{subsection.1.C.4}{}}
\newlabel{eq:Hoeffding}{{C.12}{111}{Hoeffding's Inequality}{equation.1.C.12}{}}
\newlabel{eq:Hoeffding-lemma}{{C.14}{112}{Hoeffding's Inequality}{equation.1.C.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.5}Bernstein's Inequality}{113}{subsection.1.C.5}\protected@file@percent }
\newlabel{sub:bernstein_s_inequality}{{C.5}{113}{Bernstein's Inequality}{subsection.1.C.5}{}}
\newlabel{eq:Bernstein}{{C.15}{113}{Berstein's Inequality}{equation.1.C.15}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.6}McDiarmid's Inequality}{114}{subsection.1.C.6}\protected@file@percent }
\newlabel{sub:mcdiarmid_s_inequality}{{C.6}{114}{McDiarmid's Inequality}{subsection.1.C.6}{}}
\newlabel{eq:McDiarmid}{{C.16}{114}{McDiarmid's Inequality}{equation.1.C.16}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.7}Expectation of the Maximum}{115}{subsection.1.C.7}\protected@file@percent }
\newlabel{sub:expectation_of_the_maximum}{{C.7}{115}{Expectation of the Maximum}{subsection.1.C.7}{}}
\newlabel{thm:expectation-of-maximum}{{C.13}{115}{Expectation of the Maximum}{theorem.1.C.13}{}}
\@writefile{toc}{\contentsline {section}{\numberline {D}Concentration for Matrices}{116}{appendix.1.D}\protected@file@percent }
\newlabel{sec:concentration_for_matrices}{{D}{116}{Concentration for Matrices}{appendix.1.D}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {D.1}Matrix Analysis}{116}{subsection.1.D.1}\protected@file@percent }
\newlabel{sub:matrix_analysis}{{D.1}{116}{Matrix Analysis}{subsection.1.D.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {D.1.1}Matrix Functions}{116}{subsubsection.1.D.1.1}\protected@file@percent }
\newlabel{ssub:matrix_functions}{{D.1.1}{116}{Matrix Functions}{subsubsection.1.D.1.1}{}}
\newlabel{eq:matrix-function}{{D.1}{116}{Matrix Functions}{equation.1.D.1}{}}
\newlabel{eq:transfer-rule}{{D.2}{116}{Matrix Functions}{equation.1.D.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {D.1.2}Matrix Exponential}{116}{subsubsection.1.D.1.2}\protected@file@percent }
\newlabel{ssub:matrix_exponential}{{D.1.2}{116}{Matrix Exponential}{subsubsection.1.D.1.2}{}}
\newlabel{eq:matrix-exponential}{{D.4}{116}{Matrix Exponential}{equation.1.D.4}{}}
\newlabel{eq:monotone-trace-exponential}{{D.5}{116}{Matrix Exponential}{equation.1.D.5}{}}
\newlabel{eq:Golden-Thompson}{{D.6}{117}{Matrix Exponential}{equation.1.D.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {D.1.3}Matrix Logarithm}{117}{subsubsection.1.D.1.3}\protected@file@percent }
\newlabel{ssub:matrix_logarithm}{{D.1.3}{117}{Matrix Logarithm}{subsubsection.1.D.1.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {D.1.4}Expectation and the Semidefinite Order}{117}{subsubsection.1.D.1.4}\protected@file@percent }
\newlabel{ssub:expectation_and_the_semidefinite_order}{{D.1.4}{117}{Expectation and the Semidefinite Order}{subsubsection.1.D.1.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {D.1.5}Matrix Martingales}{117}{subsubsection.1.D.1.5}\protected@file@percent }
\newlabel{ssub:matrix_martingales}{{D.1.5}{117}{Matrix Martingales}{subsubsection.1.D.1.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {D.2}Tail Bounds via the Matrix Laplace Transform Method}{118}{subsection.1.D.2}\protected@file@percent }
\newlabel{sub:tail_bounds_via_the_matrix_laplace_transform_method}{{D.2}{118}{Tail Bounds via the Matrix Laplace Transform Method}{subsection.1.D.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {D.2.1}Matrix Moments and Cumulants}{118}{subsubsection.1.D.2.1}\protected@file@percent }
\newlabel{ssub:matrix_moments_and_cumulants}{{D.2.1}{118}{Matrix Moments and Cumulants}{subsubsection.1.D.2.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {D.2.2}Laplace Transform Method}{118}{subsubsection.1.D.2.2}\protected@file@percent }
\newlabel{ssub:laplace_transform_method}{{D.2.2}{118}{Laplace Transform Method}{subsubsection.1.D.2.2}{}}
\newlabel{prop:Laplace-transform}{{D.1}{118}{The Laplace Transform Method}{equation.1.D.12}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {D.2.3}Failure of the Matrix MGF}{119}{subsubsection.1.D.2.3}\protected@file@percent }
\newlabel{ssub:failure_of_the_matrix_mgf}{{D.2.3}{119}{Failure of the Matrix MGF}{subsubsection.1.D.2.3}{}}
\newlabel{eq:scalar-mgf-multiplication}{{D.13}{119}{Failure of the Matrix MGF}{equation.1.D.13}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {D.2.4}A Concave Trace Function}{119}{subsubsection.1.D.2.4}\protected@file@percent }
\newlabel{ssub:a_concave_trace_function}{{D.2.4}{119}{A Concave Trace Function}{subsubsection.1.D.2.4}{}}
\newlabel{col:concave-trace-exponential}{{D.2}{119}{}{theorem.1.D.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {D.2.5}Subadditivity of the Matrix CGF}{119}{subsubsection.1.D.2.5}\protected@file@percent }
\newlabel{ssub:subadditivity_of_the_matrix_cgf}{{D.2.5}{119}{Subadditivity of the Matrix CGF}{subsubsection.1.D.2.5}{}}
\newlabel{eq:scalar-cgf-multiplication}{{D.14}{120}{Subadditivity of the Matrix CGF}{equation.1.D.14}{}}
\newlabel{lm:subadditivity-matrix-cgf}{{D.3}{120}{Subadditiveity of Matrix CGF's}{equation.1.D.15}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {D.2.6}Tail Bounds of Independent Sums}{120}{subsubsection.1.D.2.6}\protected@file@percent }
\newlabel{ssub:tail_bounds_of_independent_sums}{{D.2.6}{120}{Tail Bounds of Independent Sums}{subsubsection.1.D.2.6}{}}
\newlabel{eq:master-tail-bound-independent-sum}{{D.17}{121}{Master Tail Bound for Independence Sums}{equation.1.D.17}{}}
\newlabel{col:tail-bound-independent-sum}{{D.5}{121}{}{equation.1.D.19}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {D.3}Matrix Gaussian and Rademacher}{122}{subsection.1.D.3}\protected@file@percent }
\newlabel{sub:matrix_gaussian_and_rademacher}{{D.3}{122}{Matrix Gaussian and Rademacher}{subsection.1.D.3}{}}
\newlabel{lm:Rademacher-Gaussian-MGF}{{D.6}{123}{Rademacher and Gaussian MGF's}{equation.1.D.21}{}}
\newlabel{eq:matrix-Gaussian-Rademacher-bound}{{D.22}{123}{Matrix Gaussian and Rademacher Series}{equation.1.D.22}{}}
\newlabel{eq:matrix-Gaussian-Rademacher-norm-bound}{{D.23}{123}{Matrix Gaussian and Rademacher Series}{equation.1.D.23}{}}
\newlabel{thm:matrix-Gaussian-Rademacher-tail}{{D.7}{123}{Matrix Gaussian and Rademacher Series}{equation.1.D.23}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {D.4}Matrix Bennett and Bernstein Bounds}{124}{subsection.1.D.4}\protected@file@percent }
\newlabel{sub:matrix_bennett_and_bernstein_bounds}{{D.4}{124}{Matrix Bennett and Bernstein Bounds}{subsection.1.D.4}{}}
\newlabel{lm:bounded-Bernstein-MGF}{{D.8}{124}{Bounded Bernstein MGF}{theorem.1.D.8}{}}
\newlabel{thm:matrix-Bernstein-bounded}{{D.9}{125}{Matrix Bernstein - Bounded Case}{equation.1.D.25}{}}
\newlabel{thm:matrix-Bernstein-subexponential}{{D.10}{126}{Matrix Bernstein - Subexponential Case}{equation.1.D.26}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {D.5}Matrix Hoeffding and Azuma and McDiarmid}{126}{subsection.1.D.5}\protected@file@percent }
\newlabel{sub:matrix_hoeffding_and_azuma_and_mcdiarmid}{{D.5}{126}{Matrix Hoeffding and Azuma and McDiarmid}{subsection.1.D.5}{}}
\newlabel{lm:symmetrization}{{D.11}{126}{Symmetrization}{equation.1.D.27}{}}
\newlabel{lm:Azuma-CGF}{{D.12}{127}{Azuma CGF}{theorem.1.D.12}{}}
\newlabel{thm:matrix-Azuma}{{D.13}{127}{Matrix Azuma}{equation.1.D.28}{}}
\abx@aux@read@bbl@mdfivesum{AD83D65B83D8A9887CB7E48E8938FDEA}
\abx@aux@defaultrefcontext{0}{ambrosio2013density}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{bach2021learning}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{bartlett2006convexity}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{boyd2004convex}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{mohri2018foundations}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{nesterov2013gradient}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{nesterov2018lectures}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{shalev2014understanding}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{sridharan2008fast}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{tropp2012user}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{wainwright2019high}{nyt/global//global/global}
\gdef \@abspage@last{128}
