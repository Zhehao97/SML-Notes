\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand*\HyPL@Entry[1]{}
\abx@aux@refcontext{nyt/global//global/global}
\HyPL@Entry{0<</S/D>>}
\abx@aux@cite{0}{bach2021learning}
\abx@aux@segm{0}{0}{bach2021learning}
\abx@aux@cite{0}{mohri2018foundations}
\abx@aux@segm{0}{0}{mohri2018foundations}
\abx@aux@cite{0}{shalev2014understanding}
\abx@aux@segm{0}{0}{shalev2014understanding}
\abx@aux@cite{0}{tropp2012user}
\abx@aux@segm{0}{0}{tropp2012user}
\abx@aux@cite{0}{wainwright2019high}
\abx@aux@segm{0}{0}{wainwright2019high}
\abx@aux@page{1}{4}
\abx@aux@page{2}{4}
\abx@aux@page{3}{4}
\abx@aux@page{4}{4}
\abx@aux@page{5}{4}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction to Supervised Learning}{5}{section.1}\protected@file@percent }
\newlabel{sec:introduction_to_supervised_learning}{{1}{5}{Introduction to Supervised Learning}{section.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Decision Theory}{5}{subsection.1.1}\protected@file@percent }
\newlabel{sub:decision_theory}{{1.1}{5}{Decision Theory}{subsection.1.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.1}Loss Functions}{5}{subsubsection.1.1.1}\protected@file@percent }
\newlabel{ssub:loss_functions}{{1.1.1}{5}{Loss Functions}{subsubsection.1.1.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.2}Risks}{5}{subsubsection.1.1.2}\protected@file@percent }
\newlabel{ssub:risks}{{1.1.2}{5}{Risks}{subsubsection.1.1.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.3}Bayes Risk}{6}{subsubsection.1.1.3}\protected@file@percent }
\newlabel{ssub:bayes_risk}{{1.1.3}{6}{Bayes Risk}{subsubsection.1.1.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Learning from Data}{9}{subsection.1.2}\protected@file@percent }
\newlabel{sub:learning_from_data}{{1.2}{9}{Learning from Data}{subsection.1.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.1}Local Averaging Methods}{10}{subsubsection.1.2.1}\protected@file@percent }
\newlabel{ssub:local_averaging_methods}{{1.2.1}{10}{Local Averaging Methods}{subsubsection.1.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces $k$-nearest-neighbor\relax }}{10}{figure.caption.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.2}Empirical Risk Minimiation}{10}{subsubsection.1.2.2}\protected@file@percent }
\newlabel{ssub:empirical_risk_minimiation}{{1.2.2}{10}{Empirical Risk Minimiation}{subsubsection.1.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces empirical risk\relax }}{11}{figure.caption.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Statistical Learning Theory}{12}{subsection.1.3}\protected@file@percent }
\newlabel{sub:statistical_learning_theory}{{1.3}{12}{Statistical Learning Theory}{subsection.1.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.1}Measures of Performance}{12}{subsubsection.1.3.1}\protected@file@percent }
\newlabel{ssub:measures_of_performance}{{1.3.1}{12}{Measures of Performance}{subsubsection.1.3.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.2}Some Notions in Learning Problems}{13}{subsubsection.1.3.2}\protected@file@percent }
\newlabel{ssub:some_notions_in_learning_problems}{{1.3.2}{13}{Some Notions in Learning Problems}{subsubsection.1.3.2}{}}
\abx@aux@cite{0}{bach2021learning}
\abx@aux@segm{0}{0}{bach2021learning}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.3}No Free Lunch Theorems}{14}{subsubsection.1.3.3}\protected@file@percent }
\newlabel{ssub:no_free_lunch_theorems}{{1.3.3}{14}{No Free Lunch Theorems}{subsubsection.1.3.3}{}}
\abx@aux@page{6}{14}
\@writefile{toc}{\contentsline {section}{\numberline {2}Convexification of the Risk}{15}{section.2}\protected@file@percent }
\newlabel{sec:convexification_of_the_risk}{{2}{15}{Convexification of the Risk}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Convex Surrogates}{15}{subsection.2.1}\protected@file@percent }
\newlabel{sub:convex_surrogates}{{2.1}{15}{Convex Surrogates}{subsection.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Geometric Interpretation of the Support Vector Machine}{16}{subsection.2.2}\protected@file@percent }
\newlabel{sub:geometric_interpretation_of_the_support_vector_machine}{{2.2}{16}{Geometric Interpretation of the Support Vector Machine}{subsection.2.2}{}}
\newlabel{eq:svc-problem}{{2.2}{17}{Geometric Interpretation of the Support Vector Machine}{equation.2.2}{}}
\abx@aux@cite{0}{bartlett2006convexity}
\abx@aux@segm{0}{0}{bartlett2006convexity}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Conditional Surrogate Risk and Classification Calibration}{19}{subsection.2.3}\protected@file@percent }
\newlabel{sub:conditional_surrogate_risk_and_classification_calibration}{{2.3}{19}{Conditional Surrogate Risk and Classification Calibration}{subsection.2.3}{}}
\newlabel{eq:classification-calibrated}{{2.5}{19}{Conditional Surrogate Risk and Classification Calibration}{equation.2.5}{}}
\abx@aux@page{7}{19}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Classification calibration\relax }}{20}{figure.caption.6}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:classfication-calibration}{{3}{20}{Classification calibration\relax }{figure.caption.6}{}}
\newlabel{eq:calibrated-differentiate}{{2.6}{20}{Conditional Surrogate Risk and Classification Calibration}{equation.2.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Relationship between Risk and Surrogate Risk}{20}{subsection.2.4}\protected@file@percent }
\newlabel{sub:relationship_between_risk_and_surrogate_risk}{{2.4}{20}{Relationship between Risk and Surrogate Risk}{subsection.2.4}{}}
\abx@aux@cite{0}{bartlett2006convexity}
\abx@aux@segm{0}{0}{bartlett2006convexity}
\newlabel{lm:excess-risk-bound}{{1}{21}{}{lemma.1}{}}
\abx@aux@page{8}{21}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The excess risk of hinge loss and $0$-$1$ loss\relax }}{22}{figure.caption.7}\protected@file@percent }
\newlabel{fig:excess-risk-hinge-loss}{{4}{22}{The excess risk of hinge loss and $0$-$1$ loss\relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Impact on Approximation Errors}{23}{subsection.2.5}\protected@file@percent }
\newlabel{sub:impact_on_approximation_errors}{{2.5}{23}{Impact on Approximation Errors}{subsection.2.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Empirical Risk Minimization}{24}{section.3}\protected@file@percent }
\newlabel{sec:empirical_risk_minimization}{{3}{24}{Empirical Risk Minimization}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Risk Minimization Decomposition}{24}{subsection.3.1}\protected@file@percent }
\newlabel{sub:risk_minimization_decomposition}{{3.1}{24}{Risk Minimization Decomposition}{subsection.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Approximation Error}{24}{subsection.3.2}\protected@file@percent }
\newlabel{sub:approximation_error}{{3.2}{24}{Approximation Error}{subsection.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces The distance between minimizer $\theta _{*}$ and set $\Theta $ on $\mathbb  {R}$\relax }}{25}{figure.caption.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Estimation Error}{25}{subsection.3.3}\protected@file@percent }
\newlabel{sub:estimation_error}{{3.3}{25}{Estimation Error}{subsection.3.3}{}}
\newlabel{eq:estimation-error}{{3.3}{25}{Estimation Error}{equation.3.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.1}Uniform Deviation from Expectation}{26}{subsubsection.3.3.1}\protected@file@percent }
\newlabel{ssub:uniform_deviation_from_expectation}{{3.3.1}{26}{Uniform Deviation from Expectation}{subsubsection.3.3.1}{}}
\newlabel{eq:expected-unifrom-deviation}{{3.4}{26}{Uniform Deviation from Expectation}{equation.3.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.2}Linear Hypothesis Space}{26}{subsubsection.3.3.2}\protected@file@percent }
\newlabel{ssub:linear_hypothesis_space}{{3.3.2}{26}{Linear Hypothesis Space}{subsubsection.3.3.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.3}Finite Hypothesis Space}{28}{subsubsection.3.3.3}\protected@file@percent }
\newlabel{ssub:finite_hypothesis_space}{{3.3.3}{28}{Finite Hypothesis Space}{subsubsection.3.3.3}{}}
\newlabel{eq:finite-expectation-bounds}{{3.7}{29}{Finite Hypothesis Space}{equation.3.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.4}Beyond the Finite Hypothesis Space}{30}{subsubsection.3.3.4}\protected@file@percent }
\newlabel{ssub:beyond_the_finite_hypothesis_space}{{3.3.4}{30}{Beyond the Finite Hypothesis Space}{subsubsection.3.3.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces The left picture is an example in two dimensions of a covering with Euclidean balls; The right is an example of $l_{\infty }$-balls\relax }}{30}{figure.caption.9}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}PAC Learning and Uniform Convergence}{32}{section.4}\protected@file@percent }
\newlabel{sec:pac_learn_and_uniform_convergence}{{4}{32}{PAC Learning and Uniform Convergence}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}PAC Learning}{32}{subsection.4.1}\protected@file@percent }
\newlabel{sub:pac_learning}{{4.1}{32}{PAC Learning}{subsection.4.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Agnostic PAC Learning}{32}{subsection.4.2}\protected@file@percent }
\newlabel{sub:agnostic_pac_learning}{{4.2}{32}{Agnostic PAC Learning}{subsection.4.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Uniform Convergence}{33}{subsection.4.3}\protected@file@percent }
\newlabel{sub:uniform_convergence}{{4.3}{33}{Uniform Convergence}{subsection.4.3}{}}
\newlabel{eq:estimation-error-bound}{{4.1}{33}{Uniform Convergence}{equation.4.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Rademacher Complexity}{35}{section.5}\protected@file@percent }
\newlabel{sec:rademacher_complexity}{{5}{35}{Rademacher Complexity}{section.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Motivation for Rademacher Complexity}{35}{subsection.5.1}\protected@file@percent }
\newlabel{sub:motivation_for_rademacher_complexity}{{5.1}{35}{Motivation for Rademacher Complexity}{subsection.5.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Rademacher Complexity}{36}{subsection.5.2}\protected@file@percent }
\newlabel{sub:rademacher_complexity}{{5.2}{36}{Rademacher Complexity}{subsection.5.2}{}}
\newlabel{eq:empirical-Rademacher-complexity}{{5.4}{36}{Empirical Rademacher Complexity}{equation.5.4}{}}
\newlabel{eq:Rademacher-complexity}{{5.5}{36}{Rademacher Complexity}{equation.5.5}{}}
\newlabel{thm:symmetrization-Rademacher-complexity}{{5.1}{37}{Symmetrization}{equation.5.7}{}}
\newlabel{eq:empirical-process-bound-Rademacher-complexity}{{5.8}{37}{Empirical Process bonud via Rademacher Complexity}{equation.5.8}{}}
\newlabel{thm:empirical-process-bound-Radeamcher-complexity}{{5.2}{37}{Empirical Process bonud via Rademacher Complexity}{equation.5.8}{}}
\newlabel{eq:generlization-bound-Rademacher-complexity}{{5.9}{40}{Generalization Bonud via Rademacher Complexity}{equation.5.9}{}}
\newlabel{thm:generalization-bound-Radeamcher-complexity}{{5.3}{40}{Generalization Bonud via Rademacher Complexity}{equation.5.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Uniform Deviation Bounds for Linear Regression}{40}{subsection.5.3}\protected@file@percent }
\newlabel{sub:uniform_deviation_bounds_for_linear_regression}{{5.3}{40}{Uniform Deviation Bounds for Linear Regression}{subsection.5.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.1}Lipschitz-continuous Losses}{40}{subsubsection.5.3.1}\protected@file@percent }
\newlabel{ssub:lipschitz_continuous_losses}{{5.3.1}{40}{Lipschitz-continuous Losses}{subsubsection.5.3.1}{}}
\newlabel{eq:Rademacher-complexity-of-hypothesis-class}{{5.12}{41}{Lipschitz-continuous Losses}{equation.5.12}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.2}Ball-constrained Linear Predictions}{41}{subsubsection.5.3.2}\protected@file@percent }
\newlabel{ssub:ball_constrained_linear_predictions}{{5.3.2}{41}{Ball-constrained Linear Predictions}{subsubsection.5.3.2}{}}
\newlabel{eq:Rademacher-complexity-of-function-class}{{5.14}{42}{Ball-constrained Linear Predictions}{equation.5.14}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.3}Putting Things Together}{42}{subsubsection.5.3.3}\protected@file@percent }
\newlabel{ssub:putting_things_together}{{5.3.3}{42}{Putting Things Together}{subsubsection.5.3.3}{}}
\abx@aux@cite{0}{sridharan2008fast}
\abx@aux@segm{0}{0}{sridharan2008fast}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.4}From Constrained to Regularized Estimation}{43}{subsubsection.5.3.4}\protected@file@percent }
\newlabel{ssub:from_constrained_to_regularized_estimation}{{5.3.4}{43}{From Constrained to Regularized Estimation}{subsubsection.5.3.4}{}}
\newlabel{eq:regularized-empirical-risk}{{5.16}{43}{From Constrained to Regularized Estimation}{equation.5.16}{}}
\abx@aux@page{9}{43}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Generalization Bounds for SVM}{44}{subsection.5.4}\protected@file@percent }
\newlabel{sub:generalization_bounds_for_svm}{{5.4}{44}{Generalization Bounds for SVM}{subsection.5.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Growth Function and VC-Dimension}{45}{section.6}\protected@file@percent }
\newlabel{sec:growth_function_and_vc_dimension}{{6}{45}{Growth Function and VC-Dimension}{section.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Growth Function}{45}{subsection.6.1}\protected@file@percent }
\newlabel{sub:growth_function}{{6.1}{45}{Growth Function}{subsection.6.1}{}}
\newlabel{eq:growth-function}{{6.1}{45}{Growth Function}{equation.6.1}{}}
\newlabel{thm:Massart-lemma}{{6.1}{45}{Massart's Lemma}{equation.6.2}{}}
\newlabel{eq:generalization-bound-growth-function}{{6.5}{47}{Generalization Bound via Growth Function}{equation.6.5}{}}
\newlabel{thm:generalization-bound-growth-function}{{6.3}{47}{Generalization Bound via Growth Function}{equation.6.5}{}}
\newlabel{eq:direct-generalization-bound-growth-function}{{6.6}{47}{Growth Function}{equation.6.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}VC-dimension}{47}{subsection.6.2}\protected@file@percent }
\newlabel{sub:vc_dimension}{{6.2}{47}{VC-dimension}{subsection.6.2}{}}
\newlabel{eq:VC-dimension}{{6.7}{47}{VC-dimension}{equation.6.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces An example of a sine function used for classification\relax }}{49}{figure.caption.10}\protected@file@percent }
\newlabel{fig:sine-function}{{7}{49}{An example of a sine function used for classification\relax }{figure.caption.10}{}}
\newlabel{thm:Radon-thm}{{6.4}{49}{Radon's theorem}{theorem.6.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Link Growth Function and VC-dimension}{50}{subsection.6.3}\protected@file@percent }
\newlabel{sub:link_growth_function_and_vc_dimension}{{6.3}{50}{Link Growth Function and VC-dimension}{subsection.6.3}{}}
\newlabel{eq:generalization-bound-VC-dimension}{{6.11}{50}{Generalization Bounds via VC-dimension}{equation.6.11}{}}
\abx@aux@cite{0}{mohri2018foundations}
\abx@aux@segm{0}{0}{mohri2018foundations}
\newlabel{thm:generalization-bound-VC-dimension}{{6.7}{51}{Generalization Bounds via VC-dimension}{equation.6.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4}Lower Bounds}{51}{subsection.6.4}\protected@file@percent }
\newlabel{sub:lower_bounds}{{6.4}{51}{Lower Bounds}{subsection.6.4}{}}
\abx@aux@page{10}{51}
\@writefile{toc}{\contentsline {section}{\numberline {7}Covering Number and Chaining}{52}{section.7}\protected@file@percent }
\newlabel{sec:covering_number_and_chaining}{{7}{52}{Covering Number and Chaining}{section.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Covering and Packing}{52}{subsection.7.1}\protected@file@percent }
\newlabel{sub:covering_and_packing}{{7.1}{52}{Covering and Packing}{subsection.7.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Covering and packing sets\relax }}{53}{figure.caption.11}\protected@file@percent }
\newlabel{fig:covering-packing}{{8}{53}{Covering and packing sets\relax }{figure.caption.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Bound Rademacher Complexity via Covering Number}{54}{subsection.7.2}\protected@file@percent }
\newlabel{sub:bound_rademacher_complexity_via_covering_number}{{7.2}{54}{Bound Rademacher Complexity via Covering Number}{subsection.7.2}{}}
\newlabel{thm:covering-bound-Rademacher-complexity}{{7.1}{55}{Bounding Rademacher Complexity via Covering Number}{equation.7.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3}Chaining}{56}{subsection.7.3}\protected@file@percent }
\newlabel{sub:chaining}{{7.3}{56}{Chaining}{subsection.7.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Optimization for Machine Learning}{59}{section.8}\protected@file@percent }
\newlabel{sec:optimization_for_machine_learning}{{8}{59}{Optimization for Machine Learning}{section.8}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Above, $L$ is the smoothness constant, $\mu $ the strong convexity constant, $B$ the Lipschitz constant and $D$ the distance to optimum at initialization\relax }}{59}{table.caption.12}\protected@file@percent }
\newlabel{tab:optimization-algo-rate}{{1}{59}{Above, $L$ is the smoothness constant, $\mu $ the strong convexity constant, $B$ the Lipschitz constant and $D$ the distance to optimum at initialization\relax }{table.caption.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1}Optimization in Machine Learning}{59}{subsection.8.1}\protected@file@percent }
\newlabel{sub:optimization_in_machine_learning}{{8.1}{59}{Optimization in Machine Learning}{subsection.8.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2}Gradient Descent on Smooth Problems}{59}{subsection.8.2}\protected@file@percent }
\newlabel{sub:gradient_descent_on_smooth_problems}{{8.2}{59}{Gradient Descent on Smooth Problems}{subsection.8.2}{}}
\newlabel{eq:gradient-descent}{{8.2}{60}{Gradient Descent on Smooth Problems}{equation.8.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.2.1}Analysis of GD for ordinary least squares}{60}{subsubsection.8.2.1}\protected@file@percent }
\newlabel{ssub:analysis_of_gd_for_ordinary_least_squares}{{8.2.1}{60}{Analysis of GD for ordinary least squares}{subsubsection.8.2.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.2.2}Analysis of GD for strongly and smooth functions}{61}{subsubsection.8.2.2}\protected@file@percent }
\newlabel{ssub:analysis_of_gd_for_strongly_and_smooth_functions}{{8.2.2}{61}{Analysis of GD for strongly and smooth functions}{subsubsection.8.2.2}{}}
\newlabel{eq:convexity}{{8.4}{61}{Convex function}{equation.8.4}{}}
\newlabel{eq:strong-convexity}{{8.5}{61}{Strong convexity}{equation.8.5}{}}
\newlabel{lm:lojasiewics-inequality}{{6}{61}{Lojasiewics inequality}{lemma.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Strong convexity\relax }}{62}{figure.caption.13}\protected@file@percent }
\newlabel{fig:strongly-convex}{{9}{62}{Strong convexity\relax }{figure.caption.13}{}}
\newlabel{eq:smoothness}{{8.6}{62}{Smoothness}{equation.8.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces $L$-smoothness\relax }}{63}{figure.caption.14}\protected@file@percent }
\newlabel{fig:smoothness}{{10}{63}{$L$-smoothness\relax }{figure.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces When a function is both smooth and strongly convex, we denote by $\kappa = L/\mu \geq 1$ its condition number, and the condition number impacts the shapes of the level sets\relax }}{63}{figure.caption.15}\protected@file@percent }
\newlabel{fig:condition-number}{{11}{63}{When a function is both smooth and strongly convex, we denote by $\kappa = L/\mu \geq 1$ its condition number, and the condition number impacts the shapes of the level sets\relax }{figure.caption.15}{}}
\newlabel{eq:iteration-inequality}{{8.7}{63}{Analysis of GD for strongly and smooth functions}{equation.8.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces The performance of gradient descent will depend on this condition number (see the steepest descent, i.e., gradient descent with exact line search): with small condition number (left), we get fast convergence, while for a large condition number (right), we get oscillations\relax }}{64}{figure.caption.16}\protected@file@percent }
\newlabel{fig:condition-number-steepest}{{12}{64}{The performance of gradient descent will depend on this condition number (see the steepest descent, i.e., gradient descent with exact line search): with small condition number (left), we get fast convergence, while for a large condition number (right), we get oscillations\relax }{figure.caption.16}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.2.3}Analysis of GD for convex and smooth functions}{64}{subsubsection.8.2.3}\protected@file@percent }
\newlabel{ssub:analysis_of_gd_for_convex_and_smooth_functions}{{8.2.3}{64}{Analysis of GD for convex and smooth functions}{subsubsection.8.2.3}{}}
\abx@aux@cite{0}{bach2021learning}
\abx@aux@segm{0}{0}{bach2021learning}
\abx@aux@cite{0}{nesterov2018lectures}
\abx@aux@segm{0}{0}{nesterov2018lectures}
\abx@aux@page{11}{65}
\abx@aux@cite{0}{boyd2004convex}
\abx@aux@segm{0}{0}{boyd2004convex}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.2.4}Beyond Gradient Descent}{66}{subsubsection.8.2.4}\protected@file@percent }
\newlabel{ssub:beyond_gradient_descent}{{8.2.4}{66}{Beyond Gradient Descent}{subsubsection.8.2.4}{}}
\abx@aux@page{12}{66}
\abx@aux@page{13}{66}
\abx@aux@cite{0}{nesterov2013gradient}
\abx@aux@segm{0}{0}{nesterov2013gradient}
\abx@aux@page{14}{67}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3}Gradient Methods on Non-smooth Problems}{67}{subsection.8.3}\protected@file@percent }
\newlabel{sub:gradient_methods_on_non_smooth_problems}{{8.3}{67}{Gradient Methods on Non-smooth Problems}{subsection.8.3}{}}
\newlabel{thm:convergence-GD-non-smooth}{{8.3}{68}{Convergence of the Subgradient Method}{theorem.8.3}{}}
\newlabel{eq:gradient-methods-non-smooth}{{8.10}{68}{Gradient Methods on Non-smooth Problems}{equation.8.10}{}}
\abx@aux@cite{0}{bach2021learning}
\abx@aux@segm{0}{0}{bach2021learning}
\abx@aux@page{15}{69}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.4}Stochastic Gradient Descent}{69}{subsection.8.4}\protected@file@percent }
\newlabel{sub:stochastic_gradient_descent}{{8.4}{69}{Stochastic Gradient Descent}{subsection.8.4}{}}
\newlabel{eq:stochasitc-gradient-descent}{{8.11}{69}{Stochastic Gradient Descent}{equation.8.11}{}}
\newlabel{thm:convergence-SGD-non-smooth}{{8.4}{70}{Convergence of SGD}{theorem.8.4}{}}
\newlabel{eq:stochastic-gradient-methods-non-smooth}{{8.12}{71}{Stochastic Gradient Descent}{equation.8.12}{}}
\abx@aux@cite{0}{bach2021learning}
\abx@aux@segm{0}{0}{bach2021learning}
\@writefile{toc}{\contentsline {section}{\numberline {9}Kernel Methods}{72}{section.9}\protected@file@percent }
\newlabel{sec:kernel_methods}{{9}{72}{Kernel Methods}{section.9}{}}
\abx@aux@page{16}{72}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1}Motivating Example to Kernel Function}{72}{subsection.9.1}\protected@file@percent }
\newlabel{sub:motivating_example_to_kernel_function}{{9.1}{72}{Motivating Example to Kernel Function}{subsection.9.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces XOR classification problem\relax }}{72}{figure.caption.17}\protected@file@percent }
\newlabel{fig:XOR}{{13}{72}{XOR classification problem\relax }{figure.caption.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Separating hyperplane\relax }}{73}{figure.caption.18}\protected@file@percent }
\newlabel{fig:SVM}{{14}{73}{Separating hyperplane\relax }{figure.caption.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Inner product in feature space\relax }}{74}{figure.caption.19}\protected@file@percent }
\newlabel{fig:SVM-feature-space}{{15}{74}{Inner product in feature space\relax }{figure.caption.19}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2}Reproducing Kernel Hilbert Space}{75}{subsection.9.2}\protected@file@percent }
\newlabel{sub:reproducing_kernel_hilbert_space}{{9.2}{75}{Reproducing Kernel Hilbert Space}{subsection.9.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.2.1}Hilbert Space}{75}{subsubsection.9.2.1}\protected@file@percent }
\newlabel{ssub:hilbert_space}{{9.2.1}{75}{Hilbert Space}{subsubsection.9.2.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.2.2}Positive Semidefinite Kernel Functions}{76}{subsubsection.9.2.2}\protected@file@percent }
\newlabel{ssub:positive_semidefinite_kernel_functions}{{9.2.2}{76}{Positive Semidefinite Kernel Functions}{subsubsection.9.2.2}{}}
\newlabel{eq:feature-map}{{9.3}{77}{Polynomial Kernels}{equation.9.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.2.3}Constructing an RKHS from a Kernel}{77}{subsubsection.9.2.3}\protected@file@percent }
\newlabel{ssub:constructing_an_rkhs_from_a_kernel}{{9.2.3}{77}{Constructing an RKHS from a Kernel}{subsubsection.9.2.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.2.4}Alternative Way to Construct RKHS}{77}{subsubsection.9.2.4}\protected@file@percent }
\newlabel{ssub:alternative_way_to_construct_rkhs}{{9.2.4}{77}{Alternative Way to Construct RKHS}{subsubsection.9.2.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3}Algorithms}{77}{subsection.9.3}\protected@file@percent }
\newlabel{sub:algorithms}{{9.3}{77}{Algorithms}{subsection.9.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {10}Local Averaging Methods}{78}{section.10}\protected@file@percent }
\newlabel{sec:local_averaging_methods}{{10}{78}{Local Averaging Methods}{section.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1}Quick Review}{78}{subsection.10.1}\protected@file@percent }
\newlabel{sub:quick_review}{{10.1}{78}{Quick Review}{subsection.10.1}{}}
\newlabel{eq:expected-excess-risk-regression}{{10.2}{79}{Quick Review}{equation.10.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.2}Local Averaging Methods}{79}{subsection.10.2}\protected@file@percent }
\newlabel{sub:local_averaging_methods}{{10.2}{79}{Local Averaging Methods}{subsection.10.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.3}Linear Estimators}{80}{subsection.10.3}\protected@file@percent }
\newlabel{sub:linear_estimators}{{10.3}{80}{Linear Estimators}{subsection.10.3}{}}
\newlabel{eq:regression-predictor-LAM}{{10.3}{81}{Linear Estimators}{equation.10.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Weights of linear estimators in $d = 1$ estimation for three types of local averaging estimators. The $n = 8$ weight functions $\hat  {w}_i(x)$ are plotted with the observations in black.\relax }}{81}{figure.caption.20}\protected@file@percent }
\newlabel{fig:weight-functions}{{16}{81}{Weights of linear estimators in $d = 1$ estimation for three types of local averaging estimators. The $n = 8$ weight functions $\hat {w}_i(x)$ are plotted with the observations in black.\relax }{figure.caption.20}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.3.1}Partition Estimators}{81}{subsubsection.10.3.1}\protected@file@percent }
\newlabel{ssub:partition_estimators}{{10.3.1}{81}{Partition Estimators}{subsubsection.10.3.1}{}}
\newlabel{eq:partition-weight}{{10.4}{81}{Partition Estimators}{equation.10.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Regressograms in $d=1$ dimension, with three different values of $|J|$. We can see both underfitting, or overfitting in this example. Note that the target function $f^*$ is piecewise affine, and that on the affine parts, the estimator is far from linear, namely, the estimator cannot take advantage of extra-regularity.\relax }}{82}{figure.caption.21}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.3.2}Nearest-Neighbors}{83}{subsubsection.10.3.2}\protected@file@percent }
\newlabel{ssub:nearest_neighbors}{{10.3.2}{83}{Nearest-Neighbors}{subsubsection.10.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces $k$-nearest neighbor regression in $d = 1$ dimension, with three values of $k$ (the number of neighbors). We can see both underfitting ($k$ too large), and overfitting ($k$ too small).\relax }}{83}{figure.caption.22}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.3.3}Nadaraya-Watson Estimator (Kernel Regression)}{83}{subsubsection.10.3.3}\protected@file@percent }
\newlabel{ssub:nadaraya_watson_estimator_kernel_regression_}{{10.3.3}{83}{Nadaraya-Watson Estimator (Kernel Regression)}{subsubsection.10.3.3}{}}
\abx@aux@cite{0}{bach2021learning}
\abx@aux@segm{0}{0}{bach2021learning}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces Nadaraya-Watson regression in $d=1$ dimension, with three values of bandwidth $h$ for the Gaussian kernel.\relax }}{84}{figure.caption.24}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.4}Generic Consistency Analysis}{84}{subsection.10.4}\protected@file@percent }
\newlabel{sub:generic_consistency_analysis}{{10.4}{84}{Generic Consistency Analysis}{subsection.10.4}{}}
\abx@aux@page{17}{84}
\newlabel{eq:empirical-excess-risk-decomposition}{{10.5}{85}{Generic Consistency Analysis}{equation.10.5}{}}
\newlabel{eq:expected-excess-risk-decomosition}{{10.6}{86}{Generic Consistency Analysis}{equation.10.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.4.1}Fixed Partition}{86}{subsubsection.10.4.1}\protected@file@percent }
\newlabel{ssub:fixed_partition}{{10.4.1}{86}{Fixed Partition}{subsubsection.10.4.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.4.2}K-nearest Neighbors}{88}{subsubsection.10.4.2}\protected@file@percent }
\newlabel{ssub:k_nearest_neighbors}{{10.4.2}{88}{K-nearest Neighbors}{subsubsection.10.4.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces Distance to nearest neighbors\relax }}{89}{figure.caption.25}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.4.3}Kernel Regression}{90}{subsubsection.10.4.3}\protected@file@percent }
\newlabel{ssub:kernel_regression}{{10.4.3}{90}{Kernel Regression}{subsubsection.10.4.3}{}}
\abx@aux@cite{0}{ambrosio2013density}
\abx@aux@segm{0}{0}{ambrosio2013density}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.5}Universal Consistency}{92}{subsection.10.5}\protected@file@percent }
\newlabel{sub:universal_consistency}{{10.5}{92}{Universal Consistency}{subsection.10.5}{}}
\newlabel{eq:technical-assumption-LAM}{{10.11}{92}{Universal Consistency}{equation.10.11}{}}
\abx@aux@page{18}{93}
\@writefile{toc}{\contentsline {section}{\numberline {11}Sparse Methods}{95}{section.11}\protected@file@percent }
\newlabel{sec:sparse_methods}{{11}{95}{Sparse Methods}{section.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {12}Neural Networks}{96}{section.12}\protected@file@percent }
\newlabel{sec:neural_networks}{{12}{96}{Neural Networks}{section.12}{}}
\abx@aux@page{19}{97}
\abx@aux@page{20}{97}
\abx@aux@page{21}{97}
\abx@aux@page{22}{97}
\abx@aux@page{23}{97}
\abx@aux@page{24}{97}
\abx@aux@page{25}{97}
\abx@aux@page{26}{97}
\abx@aux@page{27}{97}
\abx@aux@page{28}{97}
\abx@aux@page{29}{97}
\@writefile{toc}{\contentsline {section}{Appendices}{98}{section*.28}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {A}Norms}{98}{appendix.1.A}\protected@file@percent }
\newlabel{sec:norms}{{A}{98}{Norms}{appendix.1.A}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1}Norms}{98}{subsection.1.A.1}\protected@file@percent }
\newlabel{sub:norms}{{A.1}{98}{Norms}{subsection.1.A.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2}Examples of Norm}{98}{subsection.1.A.2}\protected@file@percent }
\newlabel{sub:examples_of_norm}{{A.2}{98}{Examples of Norm}{subsection.1.A.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.3}Equivalence of Norms}{99}{subsection.1.A.3}\protected@file@percent }
\newlabel{sub:equivalence_of_norms}{{A.3}{99}{Equivalence of Norms}{subsection.1.A.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.4}Operator Norms}{99}{subsection.1.A.4}\protected@file@percent }
\newlabel{sub:operator_norms}{{A.4}{99}{Operator Norms}{subsection.1.A.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B}Probability Theory}{100}{appendix.1.B}\protected@file@percent }
\newlabel{sec:probability_theory}{{B}{100}{Probability Theory}{appendix.1.B}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.1}Independence}{100}{subsection.1.B.1}\protected@file@percent }
\newlabel{sub:independence}{{B.1}{100}{Independence}{subsection.1.B.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.2}Expectations}{102}{subsection.1.B.2}\protected@file@percent }
\newlabel{sub:expectations}{{B.2}{102}{Expectations}{subsection.1.B.2}{}}
\newlabel{def:cond-expectation}{{B.4}{102}{}{definition.1.B.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.3}Convergences}{105}{subsection.1.B.3}\protected@file@percent }
\newlabel{sub:convergences}{{B.3}{105}{Convergences}{subsection.1.B.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {C}Concentration of Measure}{110}{appendix.1.C}\protected@file@percent }
\newlabel{sec:concentration_of_measure}{{C}{110}{Concentration of Measure}{appendix.1.C}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.1}Markov Inequality}{110}{subsection.1.C.1}\protected@file@percent }
\newlabel{sub:markov_inequality}{{C.1}{110}{Markov Inequality}{subsection.1.C.1}{}}
\newlabel{eq:Markov}{{C.1}{110}{Markov Inequality}{equation.1.C.1}{}}
\newlabel{eq:Markov-higher-moment}{{C.2}{110}{Markov Inequality}{equation.1.C.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.2}Chebyshev Inequality}{110}{subsection.1.C.2}\protected@file@percent }
\newlabel{sub:chebyshev_inequality}{{C.2}{110}{Chebyshev Inequality}{subsection.1.C.2}{}}
\newlabel{eq:Chebyshev}{{C.3}{110}{Chebyshev Inequality}{equation.1.C.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.3}Chernoff's Methods}{111}{subsection.1.C.3}\protected@file@percent }
\newlabel{sub:chernoff_s_methods}{{C.3}{111}{Chernoff's Methods}{subsection.1.C.3}{}}
\newlabel{eq:Chernoff}{{C.4}{111}{Chernoff Bound}{equation.1.C.4}{}}
\newlabel{eq:Rademacher-tail}{{C.5}{111}{MGF of Rademacher Variables}{equation.1.C.5}{}}
\newlabel{eq:Bounded-tail}{{C.6}{112}{MGF of Bounded Variables}{equation.1.C.6}{}}
\newlabel{eq:Gaussian-tail-bound}{{C.7}{112}{Gaussian Tail Bound}{equation.1.C.7}{}}
\newlabel{eq:Gaussian-tail}{{C.8}{113}{Chernoff's Methods}{equation.1.C.8}{}}
\newlabel{eq:sub-Gaussian-tail}{{C.9}{113}{Sub-Gaussian Tail Bound}{equation.1.C.9}{}}
\newlabel{eq:Sub-Gaussian-tail-bound}{{C.10}{114}{Sub-Gaussian Tail Bound}{equation.1.C.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.4}Hoeffding's Inequality}{114}{subsection.1.C.4}\protected@file@percent }
\newlabel{sub:hoeffding_s_inequality}{{C.4}{114}{Hoeffding's Inequality}{subsection.1.C.4}{}}
\newlabel{eq:Hoeffding}{{C.12}{114}{Hoeffding's Inequality}{equation.1.C.12}{}}
\newlabel{eq:Hoeffding-lemma}{{C.14}{115}{Hoeffding's Inequality}{equation.1.C.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.5}Bernstein's Inequality}{116}{subsection.1.C.5}\protected@file@percent }
\newlabel{sub:bernstein_s_inequality}{{C.5}{116}{Bernstein's Inequality}{subsection.1.C.5}{}}
\newlabel{eq:Bernstein}{{C.15}{116}{Berstein's Inequality}{equation.1.C.15}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.6}McDiarmid's Inequality}{117}{subsection.1.C.6}\protected@file@percent }
\newlabel{sub:mcdiarmid_s_inequality}{{C.6}{117}{McDiarmid's Inequality}{subsection.1.C.6}{}}
\newlabel{eq:McDiarmid}{{C.16}{117}{McDiarmid's Inequality}{equation.1.C.16}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.7}Expectation of the Maximum}{118}{subsection.1.C.7}\protected@file@percent }
\newlabel{sub:expectation_of_the_maximum}{{C.7}{118}{Expectation of the Maximum}{subsection.1.C.7}{}}
\newlabel{thm:expectation-of-maximum}{{C.10}{118}{Expectation of the Maximum}{theorem.1.C.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {D}Concentration for Matrices}{119}{appendix.1.D}\protected@file@percent }
\newlabel{sec:concentration_for_matrices}{{D}{119}{Concentration for Matrices}{appendix.1.D}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {D.1}Matrix Analysis}{119}{subsection.1.D.1}\protected@file@percent }
\newlabel{sub:matrix_analysis}{{D.1}{119}{Matrix Analysis}{subsection.1.D.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {D.1.1}Matrix Functions}{119}{subsubsection.1.D.1.1}\protected@file@percent }
\newlabel{ssub:matrix_functions}{{D.1.1}{119}{Matrix Functions}{subsubsection.1.D.1.1}{}}
\newlabel{eq:matrix-function}{{D.1}{119}{Matrix Functions}{equation.1.D.1}{}}
\newlabel{eq:transfer-rule}{{D.2}{119}{Matrix Functions}{equation.1.D.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {D.1.2}Matrix Exponential}{119}{subsubsection.1.D.1.2}\protected@file@percent }
\newlabel{ssub:matrix_exponential}{{D.1.2}{119}{Matrix Exponential}{subsubsection.1.D.1.2}{}}
\newlabel{eq:matrix-exponential}{{D.4}{119}{Matrix Exponential}{equation.1.D.4}{}}
\newlabel{eq:monotone-trace-exponential}{{D.5}{119}{Matrix Exponential}{equation.1.D.5}{}}
\newlabel{eq:Golden-Thompson}{{D.6}{120}{Matrix Exponential}{equation.1.D.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {D.1.3}Matrix Logarithm}{120}{subsubsection.1.D.1.3}\protected@file@percent }
\newlabel{ssub:matrix_logarithm}{{D.1.3}{120}{Matrix Logarithm}{subsubsection.1.D.1.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {D.1.4}Expectation and the Semidefinite Order}{120}{subsubsection.1.D.1.4}\protected@file@percent }
\newlabel{ssub:expectation_and_the_semidefinite_order}{{D.1.4}{120}{Expectation and the Semidefinite Order}{subsubsection.1.D.1.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {D.1.5}Matrix Martingales}{120}{subsubsection.1.D.1.5}\protected@file@percent }
\newlabel{ssub:matrix_martingales}{{D.1.5}{120}{Matrix Martingales}{subsubsection.1.D.1.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {D.2}Tail Bounds via the Matrix Laplace Transform Method}{121}{subsection.1.D.2}\protected@file@percent }
\newlabel{sub:tail_bounds_via_the_matrix_laplace_transform_method}{{D.2}{121}{Tail Bounds via the Matrix Laplace Transform Method}{subsection.1.D.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {D.2.1}Matrix Moments and Cumulants}{121}{subsubsection.1.D.2.1}\protected@file@percent }
\newlabel{ssub:matrix_moments_and_cumulants}{{D.2.1}{121}{Matrix Moments and Cumulants}{subsubsection.1.D.2.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {D.2.2}Laplace Transform Method}{121}{subsubsection.1.D.2.2}\protected@file@percent }
\newlabel{ssub:laplace_transform_method}{{D.2.2}{121}{Laplace Transform Method}{subsubsection.1.D.2.2}{}}
\newlabel{prop:Laplace-transform}{{D.1}{121}{The Laplace Transform Method}{equation.1.D.12}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {D.2.3}Failure of the Matrix MGF}{122}{subsubsection.1.D.2.3}\protected@file@percent }
\newlabel{ssub:failure_of_the_matrix_mgf}{{D.2.3}{122}{Failure of the Matrix MGF}{subsubsection.1.D.2.3}{}}
\newlabel{eq:scalar-mgf-multiplication}{{D.13}{122}{Failure of the Matrix MGF}{equation.1.D.13}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {D.2.4}A Concave Trace Function}{122}{subsubsection.1.D.2.4}\protected@file@percent }
\newlabel{ssub:a_concave_trace_function}{{D.2.4}{122}{A Concave Trace Function}{subsubsection.1.D.2.4}{}}
\newlabel{col:concave-trace-exponential}{{D.2}{122}{}{theorem.1.D.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {D.2.5}Subadditivity of the Matrix CGF}{122}{subsubsection.1.D.2.5}\protected@file@percent }
\newlabel{ssub:subadditivity_of_the_matrix_cgf}{{D.2.5}{122}{Subadditivity of the Matrix CGF}{subsubsection.1.D.2.5}{}}
\newlabel{eq:scalar-cgf-multiplication}{{D.14}{123}{Subadditivity of the Matrix CGF}{equation.1.D.14}{}}
\newlabel{lm:subadditivity-matrix-cgf}{{12}{123}{Subadditiveity of Matrix CGF's}{equation.1.D.15}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {D.2.6}Tail Bounds of Independent Sums}{123}{subsubsection.1.D.2.6}\protected@file@percent }
\newlabel{ssub:tail_bounds_of_independent_sums}{{D.2.6}{123}{Tail Bounds of Independent Sums}{subsubsection.1.D.2.6}{}}
\newlabel{eq:master-tail-bound-independent-sum}{{D.17}{124}{Master Tail Bound for Independence Sums}{equation.1.D.17}{}}
\newlabel{col:tail-bound-independent-sum}{{D.4}{124}{}{equation.1.D.19}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {D.3}Matrix Gaussian and Rademacher}{125}{subsection.1.D.3}\protected@file@percent }
\newlabel{sub:matrix_gaussian_and_rademacher}{{D.3}{125}{Matrix Gaussian and Rademacher}{subsection.1.D.3}{}}
\newlabel{lm:Rademacher-Gaussian-MGF}{{13}{126}{Rademacher and Gaussian MGF's}{equation.1.D.21}{}}
\newlabel{eq:matrix-Gaussian-Rademacher-bound}{{D.22}{126}{Matrix Gaussian and Rademacher Series}{equation.1.D.22}{}}
\newlabel{eq:matrix-Gaussian-Rademacher-norm-bound}{{D.23}{126}{Matrix Gaussian and Rademacher Series}{equation.1.D.23}{}}
\newlabel{thm:matrix-Gaussian-Rademacher-tail}{{D.5}{126}{Matrix Gaussian and Rademacher Series}{equation.1.D.23}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {D.4}Matrix Bennett and Bernstein Bounds}{127}{subsection.1.D.4}\protected@file@percent }
\newlabel{sub:matrix_bennett_and_bernstein_bounds}{{D.4}{127}{Matrix Bennett and Bernstein Bounds}{subsection.1.D.4}{}}
\newlabel{lm:bounded-Bernstein-MGF}{{14}{127}{Bounded Bernstein MGF}{lemma.14}{}}
\newlabel{thm:matrix-Bernstein-bounded}{{D.6}{128}{Matrix Bernstein - Bounded Case}{equation.1.D.25}{}}
\newlabel{thm:matrix-Bernstein-subexponential}{{D.7}{129}{Matrix Bernstein - Subexponential Case}{equation.1.D.26}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {D.5}Matrix Hoeffding and Azuma and McDiarmid}{129}{subsection.1.D.5}\protected@file@percent }
\newlabel{sub:matrix_hoeffding_and_azuma_and_mcdiarmid}{{D.5}{129}{Matrix Hoeffding and Azuma and McDiarmid}{subsection.1.D.5}{}}
\newlabel{lm:symmetrization}{{15}{129}{Symmetrization}{equation.1.D.27}{}}
\newlabel{lm:Azuma-CGF}{{16}{130}{Azuma CGF}{lemma.16}{}}
\newlabel{thm:matrix-Azuma}{{D.8}{130}{Matrix Azuma}{equation.1.D.28}{}}
\abx@aux@read@bbl@mdfivesum{AD83D65B83D8A9887CB7E48E8938FDEA}
\abx@aux@defaultrefcontext{0}{ambrosio2013density}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{bach2021learning}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{bartlett2006convexity}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{boyd2004convex}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{mohri2018foundations}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{nesterov2013gradient}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{nesterov2018lectures}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{shalev2014understanding}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{sridharan2008fast}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{tropp2012user}{nyt/global//global/global}
\abx@aux@defaultrefcontext{0}{wainwright2019high}{nyt/global//global/global}
\gdef \@abspage@last{131}
