\contentsline {section}{\numberline {1}Introduction to Supervised Learning}{5}{section.1}%
\contentsline {subsection}{\numberline {1.1}Decision Theory}{5}{subsection.1.1}%
\contentsline {subsubsection}{\numberline {1.1.1}Loss Functions}{5}{subsubsection.1.1.1}%
\contentsline {subsubsection}{\numberline {1.1.2}Risks}{5}{subsubsection.1.1.2}%
\contentsline {subsubsection}{\numberline {1.1.3}Bayes Risk}{6}{subsubsection.1.1.3}%
\contentsline {subsection}{\numberline {1.2}Learning from Data}{9}{subsection.1.2}%
\contentsline {subsubsection}{\numberline {1.2.1}Local Averaging Methods}{10}{subsubsection.1.2.1}%
\contentsline {subsubsection}{\numberline {1.2.2}Empirical Risk Minimiation}{10}{subsubsection.1.2.2}%
\contentsline {subsection}{\numberline {1.3}Statistical Learning Theory}{12}{subsection.1.3}%
\contentsline {subsubsection}{\numberline {1.3.1}Measures of Performance}{12}{subsubsection.1.3.1}%
\contentsline {subsubsection}{\numberline {1.3.2}Some Notions in Learning Problems}{13}{subsubsection.1.3.2}%
\contentsline {subsubsection}{\numberline {1.3.3}No Free Lunch Theorems}{14}{subsubsection.1.3.3}%
\contentsline {section}{\numberline {2}Convexification of the Risk}{15}{section.2}%
\contentsline {subsection}{\numberline {2.1}Convex Surrogates}{15}{subsection.2.1}%
\contentsline {subsection}{\numberline {2.2}Geometric Interpretation of the Support Vector Machine}{16}{subsection.2.2}%
\contentsline {subsection}{\numberline {2.3}Conditional Surrogate Risk and Classification Calibration}{19}{subsection.2.3}%
\contentsline {subsection}{\numberline {2.4}Relationship between Risk and Surrogate Risk}{20}{subsection.2.4}%
\contentsline {subsection}{\numberline {2.5}Impact on Approximation Errors}{23}{subsection.2.5}%
\contentsline {section}{\numberline {3}Empirical Risk Minimization}{24}{section.3}%
\contentsline {subsection}{\numberline {3.1}Risk Minimization Decomposition}{24}{subsection.3.1}%
\contentsline {subsection}{\numberline {3.2}Approximation Error}{24}{subsection.3.2}%
\contentsline {subsection}{\numberline {3.3}Estimation Error}{25}{subsection.3.3}%
\contentsline {subsubsection}{\numberline {3.3.1}Uniform Deviation from Expectation}{26}{subsubsection.3.3.1}%
\contentsline {subsubsection}{\numberline {3.3.2}Linear Hypothesis Space}{26}{subsubsection.3.3.2}%
\contentsline {subsubsection}{\numberline {3.3.3}Finite Hypothesis Space}{28}{subsubsection.3.3.3}%
\contentsline {subsubsection}{\numberline {3.3.4}Beyond the Finite Hypothesis Space}{30}{subsubsection.3.3.4}%
\contentsline {section}{\numberline {4}PAC Learning and Uniform Convergence}{32}{section.4}%
\contentsline {subsection}{\numberline {4.1}PAC Learning}{32}{subsection.4.1}%
\contentsline {subsection}{\numberline {4.2}Agnostic PAC Learning}{32}{subsection.4.2}%
\contentsline {subsection}{\numberline {4.3}Uniform Convergence}{33}{subsection.4.3}%
\contentsline {section}{\numberline {5}Rademacher Complexity}{35}{section.5}%
\contentsline {subsection}{\numberline {5.1}Motivation for Rademacher Complexity}{35}{subsection.5.1}%
\contentsline {subsection}{\numberline {5.2}Rademacher Complexity}{36}{subsection.5.2}%
\contentsline {subsection}{\numberline {5.3}Uniform Deviation Bounds for Linear Regression}{40}{subsection.5.3}%
\contentsline {subsubsection}{\numberline {5.3.1}Lipschitz-continuous Losses}{40}{subsubsection.5.3.1}%
\contentsline {subsubsection}{\numberline {5.3.2}Ball-constrained Linear Predictions}{41}{subsubsection.5.3.2}%
\contentsline {subsubsection}{\numberline {5.3.3}Putting Things Together}{42}{subsubsection.5.3.3}%
\contentsline {subsubsection}{\numberline {5.3.4}From Constrained to Regularized Estimation}{43}{subsubsection.5.3.4}%
\contentsline {subsection}{\numberline {5.4}Generalization Bounds for SVM}{44}{subsection.5.4}%
\contentsline {section}{\numberline {6}Growth Function and VC-Dimension}{45}{section.6}%
\contentsline {subsection}{\numberline {6.1}Growth Function}{45}{subsection.6.1}%
\contentsline {subsection}{\numberline {6.2}VC-dimension}{47}{subsection.6.2}%
\contentsline {subsection}{\numberline {6.3}Link Growth Function and VC-dimension}{50}{subsection.6.3}%
\contentsline {subsection}{\numberline {6.4}Lower Bounds}{51}{subsection.6.4}%
\contentsline {section}{\numberline {7}Covering Number and Chaining}{52}{section.7}%
\contentsline {subsection}{\numberline {7.1}Covering and Packing}{52}{subsection.7.1}%
\contentsline {subsection}{\numberline {7.2}Bound Rademacher Complexity via Covering Number}{54}{subsection.7.2}%
\contentsline {subsection}{\numberline {7.3}Chaining}{56}{subsection.7.3}%
\contentsline {section}{\numberline {8}Optimization for Machine Learning}{59}{section.8}%
\contentsline {subsection}{\numberline {8.1}Optimization in Machine Learning}{59}{subsection.8.1}%
\contentsline {subsection}{\numberline {8.2}Gradient Descent on Smooth Problems}{59}{subsection.8.2}%
\contentsline {subsubsection}{\numberline {8.2.1}Analysis of GD for ordinary least squares}{60}{subsubsection.8.2.1}%
\contentsline {subsubsection}{\numberline {8.2.2}Analysis of GD for strongly and smooth functions}{61}{subsubsection.8.2.2}%
\contentsline {subsubsection}{\numberline {8.2.3}Analysis of GD for convex and smooth functions}{64}{subsubsection.8.2.3}%
\contentsline {subsubsection}{\numberline {8.2.4}Beyond Gradient Descent}{66}{subsubsection.8.2.4}%
\contentsline {subsection}{\numberline {8.3}Gradient Methods on Non-smooth Problems}{67}{subsection.8.3}%
\contentsline {subsection}{\numberline {8.4}Stochastic Gradient Descent}{69}{subsection.8.4}%
\contentsline {section}{\numberline {9}Kernel Methods}{72}{section.9}%
\contentsline {subsection}{\numberline {9.1}Motivating Example to Kernel Function}{72}{subsection.9.1}%
\contentsline {subsection}{\numberline {9.2}Reproducing Kernel Hilbert Space}{75}{subsection.9.2}%
\contentsline {subsubsection}{\numberline {9.2.1}Hilbert Space}{75}{subsubsection.9.2.1}%
\contentsline {subsubsection}{\numberline {9.2.2}Positive Semidefinite Kernel Functions}{76}{subsubsection.9.2.2}%
\contentsline {subsubsection}{\numberline {9.2.3}Constructing an RKHS from a Kernel}{77}{subsubsection.9.2.3}%
\contentsline {subsubsection}{\numberline {9.2.4}Alternative Way to Construct RKHS}{77}{subsubsection.9.2.4}%
\contentsline {subsection}{\numberline {9.3}Algorithms}{77}{subsection.9.3}%
\contentsline {section}{\numberline {10}Local Averaging Methods}{78}{section.10}%
\contentsline {subsection}{\numberline {10.1}Quick Review}{78}{subsection.10.1}%
\contentsline {subsection}{\numberline {10.2}Local Averaging Methods}{79}{subsection.10.2}%
\contentsline {subsection}{\numberline {10.3}Linear Estimators}{80}{subsection.10.3}%
\contentsline {subsubsection}{\numberline {10.3.1}Partition Estimators}{81}{subsubsection.10.3.1}%
\contentsline {subsubsection}{\numberline {10.3.2}Nearest-Neighbors}{83}{subsubsection.10.3.2}%
\contentsline {subsubsection}{\numberline {10.3.3}Nadaraya-Watson Estimator (Kernel Regression)}{83}{subsubsection.10.3.3}%
\contentsline {subsection}{\numberline {10.4}Generic Consistency Analysis}{84}{subsection.10.4}%
\contentsline {subsubsection}{\numberline {10.4.1}Fixed Partition}{86}{subsubsection.10.4.1}%
\contentsline {subsubsection}{\numberline {10.4.2}K-nearest Neighbors}{88}{subsubsection.10.4.2}%
\contentsline {subsubsection}{\numberline {10.4.3}Kernel Regression}{90}{subsubsection.10.4.3}%
\contentsline {subsection}{\numberline {10.5}Universal Consistency}{92}{subsection.10.5}%
\contentsline {section}{\numberline {11}Sparse Methods}{95}{section.11}%
\contentsline {section}{\numberline {12}Neural Networks}{96}{section.12}%
\contentsline {section}{Appendices}{98}{section*.28}%
\contentsline {section}{\numberline {A}Norms}{98}{appendix.1.A}%
\contentsline {subsection}{\numberline {A.1}Norms}{98}{subsection.1.A.1}%
\contentsline {subsection}{\numberline {A.2}Examples of Norm}{98}{subsection.1.A.2}%
\contentsline {subsection}{\numberline {A.3}Equivalence of Norms}{99}{subsection.1.A.3}%
\contentsline {subsection}{\numberline {A.4}Operator Norms}{99}{subsection.1.A.4}%
\contentsline {section}{\numberline {B}Probability Theory}{100}{appendix.1.B}%
\contentsline {subsection}{\numberline {B.1}Independence}{100}{subsection.1.B.1}%
\contentsline {subsection}{\numberline {B.2}Expectations}{102}{subsection.1.B.2}%
\contentsline {subsection}{\numberline {B.3}Convergences}{105}{subsection.1.B.3}%
\contentsline {section}{\numberline {C}Concentration of Measure}{110}{appendix.1.C}%
\contentsline {subsection}{\numberline {C.1}Markov Inequality}{110}{subsection.1.C.1}%
\contentsline {subsection}{\numberline {C.2}Chebyshev Inequality}{110}{subsection.1.C.2}%
\contentsline {subsection}{\numberline {C.3}Chernoff's Methods}{111}{subsection.1.C.3}%
\contentsline {subsection}{\numberline {C.4}Hoeffding's Inequality}{114}{subsection.1.C.4}%
\contentsline {subsection}{\numberline {C.5}Bernstein's Inequality}{116}{subsection.1.C.5}%
\contentsline {subsection}{\numberline {C.6}McDiarmid's Inequality}{117}{subsection.1.C.6}%
\contentsline {subsection}{\numberline {C.7}Expectation of the Maximum}{118}{subsection.1.C.7}%
\contentsline {section}{\numberline {D}Concentration for Matrices}{119}{appendix.1.D}%
\contentsline {subsection}{\numberline {D.1}Matrix Analysis}{119}{subsection.1.D.1}%
\contentsline {subsubsection}{\numberline {D.1.1}Matrix Functions}{119}{subsubsection.1.D.1.1}%
\contentsline {subsubsection}{\numberline {D.1.2}Matrix Exponential}{119}{subsubsection.1.D.1.2}%
\contentsline {subsubsection}{\numberline {D.1.3}Matrix Logarithm}{120}{subsubsection.1.D.1.3}%
\contentsline {subsubsection}{\numberline {D.1.4}Expectation and the Semidefinite Order}{120}{subsubsection.1.D.1.4}%
\contentsline {subsubsection}{\numberline {D.1.5}Matrix Martingales}{120}{subsubsection.1.D.1.5}%
\contentsline {subsection}{\numberline {D.2}Tail Bounds via the Matrix Laplace Transform Method}{121}{subsection.1.D.2}%
\contentsline {subsubsection}{\numberline {D.2.1}Matrix Moments and Cumulants}{121}{subsubsection.1.D.2.1}%
\contentsline {subsubsection}{\numberline {D.2.2}Laplace Transform Method}{121}{subsubsection.1.D.2.2}%
\contentsline {subsubsection}{\numberline {D.2.3}Failure of the Matrix MGF}{122}{subsubsection.1.D.2.3}%
\contentsline {subsubsection}{\numberline {D.2.4}A Concave Trace Function}{122}{subsubsection.1.D.2.4}%
\contentsline {subsubsection}{\numberline {D.2.5}Subadditivity of the Matrix CGF}{122}{subsubsection.1.D.2.5}%
\contentsline {subsubsection}{\numberline {D.2.6}Tail Bounds of Independent Sums}{123}{subsubsection.1.D.2.6}%
\contentsline {subsection}{\numberline {D.3}Matrix Gaussian and Rademacher}{125}{subsection.1.D.3}%
\contentsline {subsection}{\numberline {D.4}Matrix Bennett and Bernstein Bounds}{127}{subsection.1.D.4}%
\contentsline {subsection}{\numberline {D.5}Matrix Hoeffding and Azuma and McDiarmid}{129}{subsection.1.D.5}%
